{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1.mnist.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6fe3e3767e644a4b835828fd38694807": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_90cb2a28277243fabf31bf3acc155165",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c5a25a2d09604db49220bae979180dd4",
              "IPY_MODEL_19e1227d9b5f40659f011e51c3aa0386"
            ]
          }
        },
        "90cb2a28277243fabf31bf3acc155165": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c5a25a2d09604db49220bae979180dd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_eefb97e928834f1693cc2182349a983a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2332d1e88b1a4bf4adc4ab08048a53fb"
          }
        },
        "19e1227d9b5f40659f011e51c3aa0386": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d7d2c19d15314c7fb61e0148ec69feb6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 9920512/? [00:20&lt;00:00, 2504121.47it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e6d743849a3a432e8cac4223f0812c74"
          }
        },
        "eefb97e928834f1693cc2182349a983a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2332d1e88b1a4bf4adc4ab08048a53fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d7d2c19d15314c7fb61e0148ec69feb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e6d743849a3a432e8cac4223f0812c74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "26f055f63f0c448582578fa9ae4ec3f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b01bb85707f14d529e4fb7d6eb355945",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f1421288400d4980affd07279f287a9c",
              "IPY_MODEL_b9ebc85465d0442393c348135bc5d883"
            ]
          }
        },
        "b01bb85707f14d529e4fb7d6eb355945": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f1421288400d4980affd07279f287a9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5b711dd881b64fe7bcbac020fd39ab88",
            "_dom_classes": [],
            "description": "  0%",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c1f47e70dc9747a0b95c15b07b7f3aab"
          }
        },
        "b9ebc85465d0442393c348135bc5d883": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_89001cd83c6144f6bd33de7834b93bf9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/28881 [00:00&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0a2d8e1ac6d948469ca62dfa59deffdf"
          }
        },
        "5b711dd881b64fe7bcbac020fd39ab88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c1f47e70dc9747a0b95c15b07b7f3aab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "89001cd83c6144f6bd33de7834b93bf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0a2d8e1ac6d948469ca62dfa59deffdf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f0268a9111804c8a84edbe09b26c72ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c72409f3313b4faca3348bb1ecbfc260",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_dc0f90e79d5644859331f0ea16cbb0c1",
              "IPY_MODEL_f91d1b0fcb7a428a9da5378ae92bb1c3"
            ]
          }
        },
        "c72409f3313b4faca3348bb1ecbfc260": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dc0f90e79d5644859331f0ea16cbb0c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_75d4bdb2eb134273b21857b3ef33b07a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f172ddc9c56640cead15783dc80fe947"
          }
        },
        "f91d1b0fcb7a428a9da5378ae92bb1c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4a902e376acf49e5aea50492f7ee0407",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1654784/? [00:01&lt;00:00, 1471303.88it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8b1496c78a094dff94630b34eb2d8207"
          }
        },
        "75d4bdb2eb134273b21857b3ef33b07a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f172ddc9c56640cead15783dc80fe947": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4a902e376acf49e5aea50492f7ee0407": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8b1496c78a094dff94630b34eb2d8207": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "351ec23594c8486a9bfe102641ed2a4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_41ac6bcc9d1948358230074db9043e87",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a2073e5f8cf74e39990155d47d1e5252",
              "IPY_MODEL_7302106525844af29f10e0d7d49f7be7"
            ]
          }
        },
        "41ac6bcc9d1948358230074db9043e87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a2073e5f8cf74e39990155d47d1e5252": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_82b270b3d4754bd3b796969a0cbf8a4e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c320ad0736fd467786efb62d5e602e4d"
          }
        },
        "7302106525844af29f10e0d7d49f7be7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6674c29b5e9a460eb0a300a0f1c35e6a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 8192/? [00:00&lt;00:00, 10894.51it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_13f1f686420446c19dd0482132c33f4a"
          }
        },
        "82b270b3d4754bd3b796969a0cbf8a4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c320ad0736fd467786efb62d5e602e4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6674c29b5e9a460eb0a300a0f1c35e6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "13f1f686420446c19dd0482132c33f4a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYTazKuZVxaw",
        "outputId": "5dec9fb0-426e-4801-8546-692923e22ab2"
      },
      "source": [
        "!pip install pytorch-lightning"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-lightning\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/d0/84a2f072cd407f93a1e50dff059656bce305f084e63a45cbbceb2fdb67b4/pytorch_lightning-1.1.0-py3-none-any.whl (665kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 675kB 6.6MB/s \n",
            "\u001b[?25hCollecting fsspec>=0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/8b/1df260f860f17cb08698170153ef7db672c497c1840dcc8613ce26a8a005/fsspec-0.8.4-py3-none-any.whl (91kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 92kB 8.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (1.18.5)\n",
            "Collecting PyYAML>=5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 276kB 23.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (2.3.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (4.41.1)\n",
            "Collecting future>=0.17.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 829kB 37.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.3 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (1.7.0+cu101)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.34.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.12.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (50.3.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.15.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.17.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.0.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.36.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.4.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.10.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.3.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.7.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.3->pytorch-lightning) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.3->pytorch-lightning) (0.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.1.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch-lightning) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch-lightning) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch-lightning) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch-lightning) (2020.12.5)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (3.1.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (3.4.0)\n",
            "Building wheels for collected packages: PyYAML, future\n",
            "  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyYAML: filename=PyYAML-5.3.1-cp36-cp36m-linux_x86_64.whl size=44620 sha256=23090a47771c716fc136cf4ebfd23a75ca959ae3fbd36e67c673b64519cd6d54\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-cp36-none-any.whl size=491057 sha256=f47006ab26858036dee37b822445b25ffcad436b654f8511051e24d9bdc7f966\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
            "Successfully built PyYAML future\n",
            "Installing collected packages: fsspec, PyYAML, future, pytorch-lightning\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "Successfully installed PyYAML-5.3.1 fsspec-0.8.4 future-0.18.2 pytorch-lightning-1.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0KbdUK_hSxK"
      },
      "source": [
        "import os\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "from torch.utils.data import DataLoader, random_split\r\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eg6VIPfhW47"
      },
      "source": [
        "class MNISTClassifier(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(MNISTClassifier, self).__init__()\r\n",
        "\r\n",
        "        # MNIST images are of dimensions(1,28,28)(channel,width,height)\r\n",
        "        self.layer_1 = nn.Linear(in_features=28 * 28, out_features=128)\r\n",
        "        self.layer_2 = nn.Linear(in_features=128, out_features=256)\r\n",
        "        self.layer_3 = nn.Linear(in_features=256, out_features=10)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        batch_size, channels, width, height = x.size()\r\n",
        "\r\n",
        "        # (b,1,28,28)->(b,1*128*128)\r\n",
        "        x = x.view(batch_size, -1)\r\n",
        "\r\n",
        "        # layer 1\r\n",
        "        x = self.layer_1(x)\r\n",
        "        x = torch.relu(x)\r\n",
        "\r\n",
        "        # layer 2\r\n",
        "        x = self.layer_2(x)\r\n",
        "        x = torch.relu(x)\r\n",
        "\r\n",
        "        # layer 3\r\n",
        "        x = self.layer_3(x)\r\n",
        "\r\n",
        "        # probabaility distribution over labels\r\n",
        "        x = torch.log_softmax(x, dim=1)\r\n",
        "\r\n",
        "        return x"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdO6jyI0hauv"
      },
      "source": [
        "# Transforms\r\n",
        "transform = transforms.Compose(\r\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.1307), (0.3081))]\r\n",
        ")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376,
          "referenced_widgets": [
            "6fe3e3767e644a4b835828fd38694807",
            "90cb2a28277243fabf31bf3acc155165",
            "c5a25a2d09604db49220bae979180dd4",
            "19e1227d9b5f40659f011e51c3aa0386",
            "eefb97e928834f1693cc2182349a983a",
            "2332d1e88b1a4bf4adc4ab08048a53fb",
            "d7d2c19d15314c7fb61e0148ec69feb6",
            "e6d743849a3a432e8cac4223f0812c74",
            "26f055f63f0c448582578fa9ae4ec3f9",
            "b01bb85707f14d529e4fb7d6eb355945",
            "f1421288400d4980affd07279f287a9c",
            "b9ebc85465d0442393c348135bc5d883",
            "5b711dd881b64fe7bcbac020fd39ab88",
            "c1f47e70dc9747a0b95c15b07b7f3aab",
            "89001cd83c6144f6bd33de7834b93bf9",
            "0a2d8e1ac6d948469ca62dfa59deffdf",
            "f0268a9111804c8a84edbe09b26c72ce",
            "c72409f3313b4faca3348bb1ecbfc260",
            "dc0f90e79d5644859331f0ea16cbb0c1",
            "f91d1b0fcb7a428a9da5378ae92bb1c3",
            "75d4bdb2eb134273b21857b3ef33b07a",
            "f172ddc9c56640cead15783dc80fe947",
            "4a902e376acf49e5aea50492f7ee0407",
            "8b1496c78a094dff94630b34eb2d8207",
            "351ec23594c8486a9bfe102641ed2a4a",
            "41ac6bcc9d1948358230074db9043e87",
            "a2073e5f8cf74e39990155d47d1e5252",
            "7302106525844af29f10e0d7d49f7be7",
            "82b270b3d4754bd3b796969a0cbf8a4e",
            "c320ad0736fd467786efb62d5e602e4d",
            "6674c29b5e9a460eb0a300a0f1c35e6a",
            "13f1f686420446c19dd0482132c33f4a"
          ]
        },
        "id": "1Jg5XzgwheuS",
        "outputId": "73bcffab-cd8d-4395-fc1a-895a7302cf49"
      },
      "source": [
        "# Training , Validation Data\r\n",
        "mnist_train = datasets.MNIST(root=os.getcwd(), train=True, download=True,transform=transform)\r\n",
        "mnist_train, mnist_val = random_split(dataset=mnist_train, lengths=[55000, 5000])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /content/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6fe3e3767e644a4b835828fd38694807",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting /content/MNIST/raw/train-images-idx3-ubyte.gz to /content/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /content/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "26f055f63f0c448582578fa9ae4ec3f9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting /content/MNIST/raw/train-labels-idx1-ubyte.gz to /content/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /content/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f0268a9111804c8a84edbe09b26c72ce",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting /content/MNIST/raw/t10k-images-idx3-ubyte.gz to /content/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /content/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "351ec23594c8486a9bfe102641ed2a4a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting /content/MNIST/raw/t10k-labels-idx1-ubyte.gz to /content/MNIST/raw\n",
            "Processing...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:480: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
            "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUnQsCNihgp-"
      },
      "source": [
        "# Test Data\r\n",
        "mnist_test = datasets.MNIST(root=os.getcwd(), train=False, download=True,transform=transform)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-A7KZ-qGhlWy"
      },
      "source": [
        "# DataLoaders\r\n",
        "mnist_train = DataLoader(dataset=mnist_train, batch_size=64)\r\n",
        "mnist_val = DataLoader(dataset=mnist_val, batch_size=64)\r\n",
        "mnist_test = DataLoader(dataset=mnist_test, batch_size=64)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gIJlKRjhnBR"
      },
      "source": [
        "# Optimizer\r\n",
        "pytorch_model = MNISTClassifier()\r\n",
        "optimizer = torch.optim.Adam(params=pytorch_model.parameters(), lr=1e-3)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ju5S_FJwhpGd",
        "outputId": "971373ba-210e-4e77-a2a9-bfec874fe4ad"
      },
      "source": [
        "# Loss\r\n",
        "def cross_entropy_loss(logits, labels):\r\n",
        "    return nn.functional.nll_loss(logits, labels)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2c3OXufhqul",
        "outputId": "082a489a-15bf-4971-e14a-8e30ffcccf78"
      },
      "source": [
        "# Training Loop\r\n",
        "num_epochs = 10\r\n",
        "\r\n",
        "for epoch in range(num_epochs):\r\n",
        "  print(\"Epoch: \", epoch)\r\n",
        "  \r\n",
        "  # Training\r\n",
        "  for train_batch in mnist_train:\r\n",
        "      x, y = train_batch\r\n",
        "      logits = pytorch_model(x)\r\n",
        "      loss = cross_entropy_loss(logits, y)\r\n",
        "      print(\"train loss: \", loss.item())\r\n",
        "\r\n",
        "      loss.backward()\r\n",
        "      optimizer.step()\r\n",
        "      optimizer.zero_grad()\r\n",
        "\r\n",
        "  # Validation\r\n",
        "  with torch.no_grad():\r\n",
        "      val_loss = []\r\n",
        "      for val_batch in mnist_val:\r\n",
        "          x, y = val_batch\r\n",
        "          logits = pytorch_model(x)\r\n",
        "          val_loss.append(cross_entropy_loss(logits, y).item())\r\n",
        "\r\n",
        "      val_loss = torch.mean(torch.tensor(val_loss))\r\n",
        "      print(\"val_loss: \", val_loss.item())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "train loss:  0.005644440185278654\n",
            "train loss:  0.041159845888614655\n",
            "train loss:  0.13122306764125824\n",
            "train loss:  0.04380566254258156\n",
            "train loss:  0.07701154798269272\n",
            "train loss:  0.026706727221608162\n",
            "train loss:  0.04122988134622574\n",
            "train loss:  0.029274215921759605\n",
            "train loss:  0.017999833449721336\n",
            "train loss:  0.04827303811907768\n",
            "train loss:  0.006754807662218809\n",
            "train loss:  0.023744111880660057\n",
            "train loss:  0.021815655753016472\n",
            "train loss:  0.04145560786128044\n",
            "train loss:  0.07162683457136154\n",
            "train loss:  0.1398409754037857\n",
            "train loss:  0.04645596072077751\n",
            "train loss:  0.017741616815328598\n",
            "train loss:  0.05203850939869881\n",
            "train loss:  0.13359872996807098\n",
            "train loss:  0.055770572274923325\n",
            "train loss:  0.01271052099764347\n",
            "train loss:  0.031201595440506935\n",
            "train loss:  0.017982332035899162\n",
            "train loss:  0.08215659111738205\n",
            "train loss:  0.07065184414386749\n",
            "train loss:  0.09248240292072296\n",
            "train loss:  0.01038680225610733\n",
            "train loss:  0.018884999677538872\n",
            "train loss:  0.02201698161661625\n",
            "train loss:  0.04698924720287323\n",
            "train loss:  0.12000952661037445\n",
            "train loss:  0.040044497698545456\n",
            "train loss:  0.006319069303572178\n",
            "train loss:  0.14890526235103607\n",
            "train loss:  0.01963213086128235\n",
            "train loss:  0.0849219337105751\n",
            "train loss:  0.0175901111215353\n",
            "train loss:  0.05117186903953552\n",
            "train loss:  0.011150277219712734\n",
            "train loss:  0.040359046310186386\n",
            "train loss:  0.048786915838718414\n",
            "train loss:  0.10158786177635193\n",
            "train loss:  0.02145281806588173\n",
            "train loss:  0.04325130954384804\n",
            "train loss:  0.03881574794650078\n",
            "train loss:  0.009820414707064629\n",
            "train loss:  0.12962773442268372\n",
            "train loss:  0.004655144177377224\n",
            "train loss:  0.0029109271708875895\n",
            "train loss:  0.0915374904870987\n",
            "train loss:  0.01073329709470272\n",
            "train loss:  0.0962231233716011\n",
            "train loss:  0.004728696774691343\n",
            "train loss:  0.04996482655405998\n",
            "train loss:  0.03279277682304382\n",
            "train loss:  0.017200734466314316\n",
            "train loss:  0.038140565156936646\n",
            "train loss:  0.04893141984939575\n",
            "train loss:  0.03289173170924187\n",
            "train loss:  0.02974136546254158\n",
            "train loss:  0.08501888811588287\n",
            "train loss:  0.025179825723171234\n",
            "train loss:  0.05922885239124298\n",
            "train loss:  0.050037771463394165\n",
            "train loss:  0.09445784986019135\n",
            "train loss:  0.16076898574829102\n",
            "train loss:  0.006203177850693464\n",
            "train loss:  0.10183292627334595\n",
            "train loss:  0.02915259450674057\n",
            "train loss:  0.011344796977937222\n",
            "train loss:  0.051093291491270065\n",
            "train loss:  0.04377302527427673\n",
            "train loss:  0.006586703006178141\n",
            "train loss:  0.04652359336614609\n",
            "train loss:  0.13404303789138794\n",
            "train loss:  0.00482187932357192\n",
            "train loss:  0.04692111909389496\n",
            "train loss:  0.030208155512809753\n",
            "train loss:  0.0037026323843747377\n",
            "train loss:  0.06257674098014832\n",
            "train loss:  0.014296473935246468\n",
            "train loss:  0.06876319646835327\n",
            "train loss:  0.007345736026763916\n",
            "train loss:  0.02621702291071415\n",
            "train loss:  0.013701306656002998\n",
            "train loss:  0.057389996945858\n",
            "train loss:  0.014323360286653042\n",
            "train loss:  0.00762447714805603\n",
            "train loss:  0.07715494930744171\n",
            "train loss:  0.06900925934314728\n",
            "train loss:  0.06479258835315704\n",
            "train loss:  0.0016453155549243093\n",
            "train loss:  0.09286043047904968\n",
            "train loss:  0.05940074846148491\n",
            "train loss:  0.07628520578145981\n",
            "train loss:  0.038344528526067734\n",
            "train loss:  0.030756715685129166\n",
            "train loss:  0.026754025369882584\n",
            "train loss:  0.02885042503476143\n",
            "train loss:  0.014098254963755608\n",
            "train loss:  0.015868568792939186\n",
            "train loss:  0.018730634823441505\n",
            "train loss:  0.17064177989959717\n",
            "train loss:  0.017527693882584572\n",
            "train loss:  0.07425720989704132\n",
            "train loss:  0.00659985700622201\n",
            "train loss:  0.06066982075572014\n",
            "train loss:  0.03651650622487068\n",
            "train loss:  0.07857520133256912\n",
            "train loss:  0.09554380923509598\n",
            "train loss:  0.01646733656525612\n",
            "train loss:  0.09541196376085281\n",
            "train loss:  0.061332497745752335\n",
            "train loss:  0.01357839535921812\n",
            "train loss:  0.001517444965429604\n",
            "train loss:  0.04331992194056511\n",
            "train loss:  0.03381646052002907\n",
            "train loss:  0.01542305015027523\n",
            "train loss:  0.10509341210126877\n",
            "train loss:  0.012473399750888348\n",
            "train loss:  0.08690530061721802\n",
            "train loss:  0.008629550226032734\n",
            "train loss:  0.03360060229897499\n",
            "train loss:  0.01391401793807745\n",
            "train loss:  0.08327819406986237\n",
            "train loss:  0.11170011758804321\n",
            "train loss:  0.07074334472417831\n",
            "train loss:  0.01966354250907898\n",
            "train loss:  0.08643516898155212\n",
            "train loss:  0.12356393784284592\n",
            "train loss:  0.04853108897805214\n",
            "train loss:  0.08406264334917068\n",
            "train loss:  0.03491463512182236\n",
            "train loss:  0.02976076491177082\n",
            "train loss:  0.015585774555802345\n",
            "train loss:  0.008826589211821556\n",
            "train loss:  0.00557082612067461\n",
            "train loss:  0.007395241409540176\n",
            "train loss:  0.02460942044854164\n",
            "train loss:  0.010963944718241692\n",
            "train loss:  0.014642720110714436\n",
            "train loss:  0.06731579452753067\n",
            "train loss:  0.016714593395590782\n",
            "train loss:  0.010647483170032501\n",
            "train loss:  0.03691500797867775\n",
            "train loss:  0.009430546313524246\n",
            "train loss:  0.01995798759162426\n",
            "train loss:  0.015105192549526691\n",
            "train loss:  0.05968095734715462\n",
            "train loss:  0.025662537664175034\n",
            "train loss:  0.03868123143911362\n",
            "train loss:  0.03030351921916008\n",
            "train loss:  0.003684576600790024\n",
            "train loss:  0.01661342941224575\n",
            "train loss:  0.013397648930549622\n",
            "train loss:  0.03615061566233635\n",
            "train loss:  0.033976756036281586\n",
            "train loss:  0.015932945534586906\n",
            "train loss:  0.019264260306954384\n",
            "train loss:  0.009621911682188511\n",
            "train loss:  0.017505662515759468\n",
            "train loss:  0.007065769284963608\n",
            "train loss:  0.005174953490495682\n",
            "train loss:  0.040367238223552704\n",
            "train loss:  0.009827970527112484\n",
            "train loss:  0.0019360326696187258\n",
            "train loss:  0.15659229457378387\n",
            "train loss:  0.1381465196609497\n",
            "train loss:  0.011688333004713058\n",
            "train loss:  0.022523974999785423\n",
            "train loss:  0.06040613725781441\n",
            "train loss:  0.0037281799595803022\n",
            "train loss:  0.022540738806128502\n",
            "train loss:  0.020488634705543518\n",
            "train loss:  0.05290856584906578\n",
            "train loss:  0.0724303275346756\n",
            "train loss:  0.04677405580878258\n",
            "train loss:  0.08645467460155487\n",
            "train loss:  0.05020827054977417\n",
            "train loss:  0.009506739675998688\n",
            "train loss:  0.005720808170735836\n",
            "train loss:  0.08161785453557968\n",
            "train loss:  0.2773590087890625\n",
            "train loss:  0.10931183397769928\n",
            "train loss:  0.02968391589820385\n",
            "train loss:  0.021755097433924675\n",
            "train loss:  0.011440591886639595\n",
            "train loss:  0.003052355023100972\n",
            "train loss:  0.033870816230773926\n",
            "train loss:  0.055984076112508774\n",
            "train loss:  0.023472227156162262\n",
            "train loss:  0.009362442418932915\n",
            "train loss:  0.035158511251211166\n",
            "train loss:  0.03691224753856659\n",
            "train loss:  0.07326965779066086\n",
            "train loss:  0.07358165085315704\n",
            "train loss:  0.02172873727977276\n",
            "train loss:  0.04586487635970116\n",
            "train loss:  0.11807382851839066\n",
            "train loss:  0.012043113820254803\n",
            "train loss:  0.01473813597112894\n",
            "train loss:  0.02558143064379692\n",
            "train loss:  0.07434456795454025\n",
            "train loss:  0.01332621555775404\n",
            "train loss:  0.014036307111382484\n",
            "train loss:  0.06677619367837906\n",
            "train loss:  0.019557002931833267\n",
            "train loss:  0.004277972970157862\n",
            "train loss:  0.006148918066173792\n",
            "train loss:  0.1476268470287323\n",
            "train loss:  0.04606688395142555\n",
            "train loss:  0.027158433571457863\n",
            "train loss:  0.0011768728727474809\n",
            "train loss:  0.03602534160017967\n",
            "train loss:  0.08698704838752747\n",
            "train loss:  0.027074703946709633\n",
            "train loss:  0.07226363569498062\n",
            "train loss:  0.02176286093890667\n",
            "train loss:  0.039492785930633545\n",
            "train loss:  0.07570941746234894\n",
            "train loss:  0.0726780816912651\n",
            "train loss:  0.02128622867166996\n",
            "train loss:  0.0062283300794661045\n",
            "train loss:  0.011474847793579102\n",
            "train loss:  0.03504979982972145\n",
            "train loss:  0.007137557957321405\n",
            "train loss:  0.010290255770087242\n",
            "train loss:  0.022336257621645927\n",
            "train loss:  0.008695072494447231\n",
            "train loss:  0.04430464282631874\n",
            "train loss:  0.013201274909079075\n",
            "train loss:  0.1558828055858612\n",
            "train loss:  0.05428125709295273\n",
            "train loss:  0.0715932697057724\n",
            "train loss:  0.11887988448143005\n",
            "train loss:  0.08858798444271088\n",
            "train loss:  0.011011293157935143\n",
            "train loss:  0.04961421713232994\n",
            "train loss:  0.0551367849111557\n",
            "train loss:  0.009682225994765759\n",
            "train loss:  0.17820830643177032\n",
            "train loss:  0.047577764838933945\n",
            "train loss:  0.05630870535969734\n",
            "train loss:  0.004162684082984924\n",
            "train loss:  0.05025460198521614\n",
            "train loss:  0.058142028748989105\n",
            "train loss:  0.06766323745250702\n",
            "train loss:  0.046800725162029266\n",
            "train loss:  0.000916202669031918\n",
            "train loss:  0.028282267972826958\n",
            "train loss:  0.01575162075459957\n",
            "train loss:  0.014469847083091736\n",
            "train loss:  0.035659682005643845\n",
            "train loss:  0.16889183223247528\n",
            "train loss:  0.06515754759311676\n",
            "train loss:  0.037309616804122925\n",
            "train loss:  0.03158766031265259\n",
            "train loss:  0.002207702724263072\n",
            "train loss:  0.0593797005712986\n",
            "train loss:  0.06649301201105118\n",
            "train loss:  0.02286474034190178\n",
            "train loss:  0.011612804606556892\n",
            "train loss:  0.04156074672937393\n",
            "train loss:  0.08971241861581802\n",
            "train loss:  0.003408008487895131\n",
            "train loss:  0.02867850661277771\n",
            "train loss:  0.019810037687420845\n",
            "train loss:  0.02324068546295166\n",
            "train loss:  0.010951888747513294\n",
            "train loss:  0.02519734762609005\n",
            "train loss:  0.014781937003135681\n",
            "train loss:  0.0636884942650795\n",
            "train loss:  0.02844436839222908\n",
            "train loss:  0.01612139120697975\n",
            "train loss:  0.020169692113995552\n",
            "train loss:  0.01341032050549984\n",
            "train loss:  0.11927966773509979\n",
            "train loss:  0.12031242251396179\n",
            "train loss:  0.004566391929984093\n",
            "train loss:  0.013760894536972046\n",
            "train loss:  0.07663675397634506\n",
            "train loss:  0.03461876139044762\n",
            "train loss:  0.02839171327650547\n",
            "train loss:  0.07549657672643661\n",
            "train loss:  0.06546813994646072\n",
            "train loss:  0.036265965551137924\n",
            "train loss:  0.02759292721748352\n",
            "train loss:  0.009842385537922382\n",
            "train loss:  0.039844661951065063\n",
            "train loss:  0.01923929713666439\n",
            "train loss:  0.02123808115720749\n",
            "train loss:  0.03579677641391754\n",
            "train loss:  0.02197837270796299\n",
            "train loss:  0.016470665112137794\n",
            "train loss:  0.3111940920352936\n",
            "train loss:  0.0098653519526124\n",
            "train loss:  0.05705878138542175\n",
            "train loss:  0.04668482020497322\n",
            "train loss:  0.12605957686901093\n",
            "train loss:  0.01196676678955555\n",
            "train loss:  0.005149401258677244\n",
            "train loss:  0.03642375022172928\n",
            "train loss:  0.11307286471128464\n",
            "train loss:  0.041056107729673386\n",
            "train loss:  0.008948920294642448\n",
            "train loss:  0.016843656077980995\n",
            "train loss:  0.02623511292040348\n",
            "train loss:  0.01650601625442505\n",
            "train loss:  0.017567910254001617\n",
            "train loss:  0.05579759553074837\n",
            "train loss:  0.004423482809215784\n",
            "train loss:  0.030979115515947342\n",
            "train loss:  0.012655486352741718\n",
            "train loss:  0.007378068752586842\n",
            "train loss:  0.06840801239013672\n",
            "train loss:  0.09855666011571884\n",
            "train loss:  0.01126997359097004\n",
            "train loss:  0.057467784732580185\n",
            "train loss:  0.022385796532034874\n",
            "train loss:  0.022884691134095192\n",
            "train loss:  0.029571540653705597\n",
            "train loss:  0.03124089539051056\n",
            "train loss:  0.024511249735951424\n",
            "train loss:  0.03176764026284218\n",
            "train loss:  0.025412173941731453\n",
            "train loss:  0.11066824197769165\n",
            "train loss:  0.04225784167647362\n",
            "train loss:  0.0834031030535698\n",
            "train loss:  0.016017403453588486\n",
            "train loss:  0.025354595854878426\n",
            "train loss:  0.04268639162182808\n",
            "train loss:  0.039595868438482285\n",
            "train loss:  0.04536638408899307\n",
            "train loss:  0.01002880372107029\n",
            "train loss:  0.07193218916654587\n",
            "train loss:  0.03945831581950188\n",
            "train loss:  0.0006307557341642678\n",
            "train loss:  0.05262497439980507\n",
            "train loss:  0.05953776463866234\n",
            "train loss:  0.05409889295697212\n",
            "train loss:  0.03918888419866562\n",
            "train loss:  0.005261502228677273\n",
            "train loss:  0.020118044689297676\n",
            "train loss:  0.060462504625320435\n",
            "train loss:  0.0029449565336108208\n",
            "train loss:  0.13961480557918549\n",
            "train loss:  0.022308629006147385\n",
            "train loss:  0.05045982822775841\n",
            "train loss:  0.09570488333702087\n",
            "train loss:  0.04582105576992035\n",
            "train loss:  0.0052031055092811584\n",
            "train loss:  0.047521211206912994\n",
            "train loss:  0.023434553295373917\n",
            "train loss:  0.025149598717689514\n",
            "train loss:  0.03247275948524475\n",
            "train loss:  0.08212593197822571\n",
            "train loss:  0.07388810813426971\n",
            "train loss:  0.021581314504146576\n",
            "train loss:  0.030163107439875603\n",
            "train loss:  0.028691543266177177\n",
            "train loss:  0.031022781506180763\n",
            "train loss:  0.03633854165673256\n",
            "train loss:  0.0035039649810642004\n",
            "train loss:  0.0010343102039769292\n",
            "train loss:  0.020980913192033768\n",
            "train loss:  0.025993777438998222\n",
            "train loss:  0.01929614320397377\n",
            "train loss:  0.04115933179855347\n",
            "train loss:  0.0016355623956769705\n",
            "train loss:  0.035792384296655655\n",
            "train loss:  0.0009873983217403293\n",
            "train loss:  0.012565997429192066\n",
            "train loss:  0.030194999650120735\n",
            "train loss:  0.0798325389623642\n",
            "train loss:  0.025600524619221687\n",
            "train loss:  0.0087064728140831\n",
            "train loss:  0.0008790848078206182\n",
            "train loss:  0.00406073871999979\n",
            "train loss:  0.03288714215159416\n",
            "train loss:  0.014300757087767124\n",
            "train loss:  0.04009169712662697\n",
            "train loss:  0.028945662081241608\n",
            "train loss:  0.013581465929746628\n",
            "train loss:  0.07067828625440598\n",
            "train loss:  0.003501440864056349\n",
            "train loss:  0.039750535041093826\n",
            "train loss:  0.04927559942007065\n",
            "train loss:  0.0019342615269124508\n",
            "train loss:  0.00654556741937995\n",
            "train loss:  0.008633901365101337\n",
            "train loss:  0.012459754943847656\n",
            "train loss:  0.012352493591606617\n",
            "train loss:  0.024907086044549942\n",
            "train loss:  0.030036592856049538\n",
            "train loss:  0.0053375535644590855\n",
            "train loss:  0.022735437378287315\n",
            "train loss:  0.06284677237272263\n",
            "train loss:  0.05313782021403313\n",
            "train loss:  0.043712910264730453\n",
            "train loss:  0.1205194741487503\n",
            "train loss:  0.03089139610528946\n",
            "train loss:  0.025785665959119797\n",
            "train loss:  0.013311443850398064\n",
            "train loss:  0.03127420321106911\n",
            "train loss:  0.013136669993400574\n",
            "train loss:  0.03408833593130112\n",
            "train loss:  0.06753461807966232\n",
            "train loss:  0.01907944492995739\n",
            "train loss:  0.01896640844643116\n",
            "train loss:  0.033377524465322495\n",
            "train loss:  0.010045457631349564\n",
            "train loss:  0.012294469401240349\n",
            "train loss:  0.012884628027677536\n",
            "train loss:  0.016637727618217468\n",
            "train loss:  0.04065847769379616\n",
            "train loss:  0.19846118986606598\n",
            "train loss:  0.026888268068432808\n",
            "train loss:  0.00989462248980999\n",
            "train loss:  0.020510666072368622\n",
            "train loss:  0.011875509284436703\n",
            "train loss:  0.08722967654466629\n",
            "train loss:  0.006148382555693388\n",
            "train loss:  0.0047348919324576855\n",
            "train loss:  0.009323736652731895\n",
            "train loss:  0.04599165916442871\n",
            "train loss:  0.04149729758501053\n",
            "train loss:  0.0021907563786953688\n",
            "train loss:  0.027569565922021866\n",
            "train loss:  0.010718283243477345\n",
            "train loss:  0.036974988877773285\n",
            "train loss:  0.0028774505481123924\n",
            "train loss:  0.007121095433831215\n",
            "train loss:  0.06140344589948654\n",
            "train loss:  0.025601834058761597\n",
            "train loss:  0.025508075952529907\n",
            "train loss:  0.04326363652944565\n",
            "train loss:  0.03889419510960579\n",
            "train loss:  0.006184921599924564\n",
            "train loss:  0.010149245150387287\n",
            "train loss:  0.012367810122668743\n",
            "train loss:  0.007374720182269812\n",
            "train loss:  0.018137222155928612\n",
            "train loss:  0.03821413964033127\n",
            "train loss:  0.035315316170454025\n",
            "train loss:  0.06919421255588531\n",
            "train loss:  0.06492789089679718\n",
            "train loss:  0.04828064888715744\n",
            "train loss:  0.0009528976515866816\n",
            "train loss:  0.044593777507543564\n",
            "train loss:  0.010631018318235874\n",
            "train loss:  0.05872121825814247\n",
            "train loss:  0.11759274452924728\n",
            "train loss:  0.02898493967950344\n",
            "train loss:  0.07872205972671509\n",
            "train loss:  0.06016000732779503\n",
            "train loss:  0.03999301418662071\n",
            "train loss:  0.006010380573570728\n",
            "train loss:  0.05055093392729759\n",
            "train loss:  0.003115627681836486\n",
            "train loss:  0.015775170177221298\n",
            "train loss:  0.022635364904999733\n",
            "train loss:  0.08461226522922516\n",
            "train loss:  0.050533831119537354\n",
            "train loss:  0.015920868143439293\n",
            "train loss:  0.11819426715373993\n",
            "train loss:  0.024365969002246857\n",
            "train loss:  0.053620196878910065\n",
            "train loss:  0.02011556550860405\n",
            "train loss:  0.08439712226390839\n",
            "train loss:  0.13741619884967804\n",
            "train loss:  0.02460584044456482\n",
            "train loss:  0.010420113801956177\n",
            "train loss:  0.003974451683461666\n",
            "train loss:  0.026565823704004288\n",
            "train loss:  0.07241678237915039\n",
            "train loss:  0.00644097663462162\n",
            "train loss:  0.022693848237395287\n",
            "train loss:  0.02706141024827957\n",
            "train loss:  0.02664843574166298\n",
            "train loss:  0.011006752960383892\n",
            "train loss:  0.014123128727078438\n",
            "train loss:  0.06293706595897675\n",
            "train loss:  0.0027940759900957346\n",
            "train loss:  0.027139879763126373\n",
            "train loss:  0.0031330008059740067\n",
            "train loss:  0.022942692041397095\n",
            "train loss:  0.0727221667766571\n",
            "train loss:  0.01380402036011219\n",
            "train loss:  0.0023342894855886698\n",
            "train loss:  0.019091730937361717\n",
            "train loss:  0.06154797971248627\n",
            "train loss:  0.008872701786458492\n",
            "train loss:  0.014156119897961617\n",
            "train loss:  0.018991680815815926\n",
            "train loss:  0.011305851861834526\n",
            "train loss:  0.01029288675636053\n",
            "train loss:  0.10770667344331741\n",
            "train loss:  0.022579612210392952\n",
            "train loss:  0.009385023266077042\n",
            "train loss:  0.01943863183259964\n",
            "train loss:  0.1145472526550293\n",
            "train loss:  0.013337393291294575\n",
            "train loss:  0.06460586935281754\n",
            "train loss:  0.005821197293698788\n",
            "train loss:  0.00136866036336869\n",
            "train loss:  0.019964491948485374\n",
            "train loss:  0.06264565885066986\n",
            "train loss:  0.02570914477109909\n",
            "train loss:  0.016932466998696327\n",
            "train loss:  0.01004322525113821\n",
            "train loss:  0.0900888442993164\n",
            "train loss:  0.01016996894031763\n",
            "train loss:  0.0407286062836647\n",
            "train loss:  0.013261828571557999\n",
            "train loss:  0.014122206717729568\n",
            "train loss:  0.12129505723714828\n",
            "train loss:  0.002009477000683546\n",
            "train loss:  0.012280394323170185\n",
            "train loss:  0.019716020673513412\n",
            "train loss:  0.19137609004974365\n",
            "train loss:  0.05278419703245163\n",
            "train loss:  0.06444960832595825\n",
            "train loss:  0.019762489944696426\n",
            "train loss:  0.05569025129079819\n",
            "train loss:  0.002324228873476386\n",
            "train loss:  0.08052567392587662\n",
            "train loss:  0.034958045929670334\n",
            "train loss:  0.0485776886343956\n",
            "train loss:  0.014001683332026005\n",
            "train loss:  0.03375256061553955\n",
            "train loss:  0.012481292709708214\n",
            "train loss:  0.007372827734798193\n",
            "train loss:  0.03901737928390503\n",
            "train loss:  0.003851719433441758\n",
            "train loss:  0.011166480369865894\n",
            "train loss:  0.025001294910907745\n",
            "train loss:  0.061296965926885605\n",
            "train loss:  0.008437523618340492\n",
            "train loss:  0.06121363490819931\n",
            "train loss:  0.1386166661977768\n",
            "train loss:  0.1255873739719391\n",
            "train loss:  0.003018009476363659\n",
            "train loss:  0.002378933597356081\n",
            "train loss:  0.04415236413478851\n",
            "train loss:  0.1166389212012291\n",
            "train loss:  0.023944828659296036\n",
            "train loss:  0.09022565931081772\n",
            "train loss:  0.007710441015660763\n",
            "train loss:  0.005930059589445591\n",
            "train loss:  0.06081553176045418\n",
            "train loss:  0.02794068306684494\n",
            "train loss:  0.02691655047237873\n",
            "train loss:  0.012793025001883507\n",
            "train loss:  0.012062913738191128\n",
            "train loss:  0.060081422328948975\n",
            "train loss:  0.04095621779561043\n",
            "train loss:  0.003257933771237731\n",
            "train loss:  0.0055636814795434475\n",
            "train loss:  0.0658206045627594\n",
            "train loss:  0.012211505323648453\n",
            "train loss:  0.0698009729385376\n",
            "train loss:  0.0014426783891394734\n",
            "train loss:  0.01919466070830822\n",
            "train loss:  0.0056854719296097755\n",
            "train loss:  0.04115276783704758\n",
            "train loss:  0.014069781638681889\n",
            "train loss:  0.013318073935806751\n",
            "train loss:  0.09386428445577621\n",
            "train loss:  0.0004337600839789957\n",
            "train loss:  0.007556912023574114\n",
            "train loss:  0.0058423238806426525\n",
            "train loss:  0.009722994640469551\n",
            "train loss:  0.0027942596934735775\n",
            "train loss:  0.021923266351222992\n",
            "train loss:  0.005310555920004845\n",
            "train loss:  0.013228917494416237\n",
            "train loss:  0.043617937713861465\n",
            "train loss:  0.04862082749605179\n",
            "train loss:  0.05517462268471718\n",
            "train loss:  0.017360828816890717\n",
            "train loss:  0.03029729425907135\n",
            "train loss:  0.0576641745865345\n",
            "train loss:  0.03882717713713646\n",
            "train loss:  0.0829317644238472\n",
            "train loss:  0.005242655985057354\n",
            "train loss:  0.05728324502706528\n",
            "train loss:  0.01709483563899994\n",
            "train loss:  0.008609907701611519\n",
            "train loss:  0.040861826390028\n",
            "train loss:  0.024598760530352592\n",
            "train loss:  0.005577102769166231\n",
            "train loss:  0.02642170898616314\n",
            "train loss:  0.09743412584066391\n",
            "train loss:  0.07361873984336853\n",
            "train loss:  0.029251214116811752\n",
            "train loss:  0.016452793031930923\n",
            "train loss:  0.01941661536693573\n",
            "train loss:  0.06367678195238113\n",
            "train loss:  0.0007329214131459594\n",
            "train loss:  0.04208482429385185\n",
            "train loss:  0.017108265310525894\n",
            "train loss:  0.013401046395301819\n",
            "train loss:  0.03934844210743904\n",
            "train loss:  0.03673509880900383\n",
            "train loss:  0.02616865187883377\n",
            "train loss:  0.010405617766082287\n",
            "train loss:  0.021926360204815865\n",
            "train loss:  0.018493667244911194\n",
            "train loss:  0.03241823613643646\n",
            "train loss:  0.18359407782554626\n",
            "train loss:  0.059861455112695694\n",
            "train loss:  0.04898002743721008\n",
            "train loss:  0.008490040898323059\n",
            "train loss:  0.05991034582257271\n",
            "train loss:  0.08478948473930359\n",
            "train loss:  0.04682871326804161\n",
            "train loss:  0.08996469527482986\n",
            "train loss:  0.03189412131905556\n",
            "train loss:  0.021956827491521835\n",
            "train loss:  0.005217941477894783\n",
            "train loss:  0.02061268873512745\n",
            "train loss:  0.006560897454619408\n",
            "train loss:  0.001956542953848839\n",
            "train loss:  0.021902525797486305\n",
            "train loss:  0.029551487416028976\n",
            "train loss:  0.006382017862051725\n",
            "train loss:  0.06182824447751045\n",
            "train loss:  0.17300082743167877\n",
            "train loss:  0.028030380606651306\n",
            "train loss:  0.07861712574958801\n",
            "train loss:  0.1193985566496849\n",
            "train loss:  0.00655162800103426\n",
            "train loss:  0.027059579268097878\n",
            "train loss:  0.001461864565499127\n",
            "train loss:  0.05309566110372543\n",
            "train loss:  0.01695278473198414\n",
            "train loss:  0.0075497557409107685\n",
            "train loss:  0.134377121925354\n",
            "train loss:  0.06426358968019485\n",
            "train loss:  0.06235693022608757\n",
            "train loss:  0.0011113646905869246\n",
            "train loss:  0.0019727020990103483\n",
            "train loss:  0.021639663726091385\n",
            "train loss:  0.09373984485864639\n",
            "train loss:  0.0005673346458934247\n",
            "train loss:  0.008380478248000145\n",
            "train loss:  0.016041811555624008\n",
            "train loss:  0.017334526404738426\n",
            "train loss:  0.01035815104842186\n",
            "train loss:  0.013233883306384087\n",
            "train loss:  0.059593793004751205\n",
            "train loss:  0.03660621494054794\n",
            "train loss:  0.07911447435617447\n",
            "train loss:  0.044081248342990875\n",
            "train loss:  0.05460723489522934\n",
            "train loss:  0.06479581445455551\n",
            "train loss:  0.04255096614360809\n",
            "train loss:  0.0277408380061388\n",
            "train loss:  0.02197028137743473\n",
            "train loss:  0.014130583964288235\n",
            "train loss:  0.05690059810876846\n",
            "train loss:  0.016508089378476143\n",
            "train loss:  0.10899361222982407\n",
            "train loss:  0.03261541202664375\n",
            "train loss:  0.0039961496368050575\n",
            "train loss:  0.07373818010091782\n",
            "train loss:  0.029718944802880287\n",
            "train loss:  0.038979671895504\n",
            "train loss:  0.031309835612773895\n",
            "train loss:  0.11289563775062561\n",
            "train loss:  0.05908961594104767\n",
            "train loss:  0.08584567904472351\n",
            "train loss:  0.04149337857961655\n",
            "train loss:  0.07295984774827957\n",
            "train loss:  0.09943598508834839\n",
            "train loss:  0.004508624784648418\n",
            "train loss:  0.02233029529452324\n",
            "train loss:  0.06479433178901672\n",
            "train loss:  0.004054822493344545\n",
            "train loss:  0.021351974457502365\n",
            "train loss:  0.006378501653671265\n",
            "train loss:  0.01583259552717209\n",
            "train loss:  0.012224109843373299\n",
            "train loss:  0.018220970407128334\n",
            "train loss:  0.016755148768424988\n",
            "train loss:  0.022690141573548317\n",
            "train loss:  0.010561575181782246\n",
            "train loss:  0.0014735664008185267\n",
            "val_loss:  0.11845149099826813\n",
            "Epoch:  5\n",
            "train loss:  0.02377774938941002\n",
            "train loss:  0.06727529317140579\n",
            "train loss:  0.09721919149160385\n",
            "train loss:  0.0444725938141346\n",
            "train loss:  0.05780956521630287\n",
            "train loss:  0.00829310156404972\n",
            "train loss:  0.16948126256465912\n",
            "train loss:  0.09444772452116013\n",
            "train loss:  0.007363481447100639\n",
            "train loss:  0.0050270212814211845\n",
            "train loss:  0.014609083533287048\n",
            "train loss:  0.004718118812888861\n",
            "train loss:  0.06066223978996277\n",
            "train loss:  0.05334525555372238\n",
            "train loss:  0.08951377123594284\n",
            "train loss:  0.0287904255092144\n",
            "train loss:  0.020712658762931824\n",
            "train loss:  0.02233443595468998\n",
            "train loss:  0.06604182720184326\n",
            "train loss:  0.018648410215973854\n",
            "train loss:  0.006847210228443146\n",
            "train loss:  0.022827496752142906\n",
            "train loss:  0.10995881259441376\n",
            "train loss:  0.09202603995800018\n",
            "train loss:  0.02140098437666893\n",
            "train loss:  0.014985844492912292\n",
            "train loss:  0.01189358439296484\n",
            "train loss:  0.050083987414836884\n",
            "train loss:  0.06533406674861908\n",
            "train loss:  0.009239764884114265\n",
            "train loss:  0.09287169575691223\n",
            "train loss:  0.05288848653435707\n",
            "train loss:  0.12534919381141663\n",
            "train loss:  0.013944203965365887\n",
            "train loss:  0.04510420188307762\n",
            "train loss:  0.10141631215810776\n",
            "train loss:  0.005164840258657932\n",
            "train loss:  0.011114527471363544\n",
            "train loss:  0.13429862260818481\n",
            "train loss:  0.013365243561565876\n",
            "train loss:  0.03727245703339577\n",
            "train loss:  0.03196868672966957\n",
            "train loss:  0.11725173890590668\n",
            "train loss:  0.0664549320936203\n",
            "train loss:  0.10053545236587524\n",
            "train loss:  0.005528363864868879\n",
            "train loss:  0.0238338652998209\n",
            "train loss:  0.10483469069004059\n",
            "train loss:  0.04033547267317772\n",
            "train loss:  0.016232658177614212\n",
            "train loss:  0.013758216984570026\n",
            "train loss:  0.056774675846099854\n",
            "train loss:  0.0487869568169117\n",
            "train loss:  0.013725535944104195\n",
            "train loss:  0.010283770971000195\n",
            "train loss:  0.0414871908724308\n",
            "train loss:  0.048905808478593826\n",
            "train loss:  0.09773587435483932\n",
            "train loss:  0.07914476096630096\n",
            "train loss:  0.011066417209804058\n",
            "train loss:  0.005392555613070726\n",
            "train loss:  0.014381511136889458\n",
            "train loss:  0.0054674409329891205\n",
            "train loss:  0.04417957738041878\n",
            "train loss:  0.09443197399377823\n",
            "train loss:  0.049508046358823776\n",
            "train loss:  0.019321780651807785\n",
            "train loss:  0.0038227681070566177\n",
            "train loss:  0.024366114288568497\n",
            "train loss:  0.01302541047334671\n",
            "train loss:  0.011601856909692287\n",
            "train loss:  0.033922720700502396\n",
            "train loss:  0.040947966277599335\n",
            "train loss:  0.051353972405195236\n",
            "train loss:  0.08213631063699722\n",
            "train loss:  0.012815169990062714\n",
            "train loss:  0.08530030399560928\n",
            "train loss:  0.0037258388474583626\n",
            "train loss:  0.0033837025985121727\n",
            "train loss:  0.08415944874286652\n",
            "train loss:  0.01344790868461132\n",
            "train loss:  0.02926664613187313\n",
            "train loss:  0.004876126069575548\n",
            "train loss:  0.13158470392227173\n",
            "train loss:  0.019941585138440132\n",
            "train loss:  0.05241760239005089\n",
            "train loss:  0.020092369988560677\n",
            "train loss:  0.014401091262698174\n",
            "train loss:  0.11177540570497513\n",
            "train loss:  0.016112780198454857\n",
            "train loss:  0.010883080773055553\n",
            "train loss:  0.09652691334486008\n",
            "train loss:  0.004592034965753555\n",
            "train loss:  0.0020263574551790953\n",
            "train loss:  0.023777946829795837\n",
            "train loss:  0.013420315459370613\n",
            "train loss:  0.11735130846500397\n",
            "train loss:  0.021047528833150864\n",
            "train loss:  0.13581158220767975\n",
            "train loss:  0.1588667929172516\n",
            "train loss:  0.09589327126741409\n",
            "train loss:  0.13412359356880188\n",
            "train loss:  0.0654669851064682\n",
            "train loss:  0.0023714001290500164\n",
            "train loss:  0.0327410064637661\n",
            "train loss:  0.05871085822582245\n",
            "train loss:  0.0020304382778704166\n",
            "train loss:  0.1144566535949707\n",
            "train loss:  0.1306406855583191\n",
            "train loss:  0.10950267314910889\n",
            "train loss:  0.010318251326680183\n",
            "train loss:  0.028544217348098755\n",
            "train loss:  0.07685455679893494\n",
            "train loss:  0.07749716937541962\n",
            "train loss:  0.01731325313448906\n",
            "train loss:  0.03591481223702431\n",
            "train loss:  0.014940768480300903\n",
            "train loss:  0.01407186035066843\n",
            "train loss:  0.06216099485754967\n",
            "train loss:  0.01924148201942444\n",
            "train loss:  0.1295648217201233\n",
            "train loss:  0.09017716348171234\n",
            "train loss:  0.15832184255123138\n",
            "train loss:  0.016209471970796585\n",
            "train loss:  0.01725136861205101\n",
            "train loss:  0.012049896642565727\n",
            "train loss:  0.017831021919846535\n",
            "train loss:  0.0030672852881252766\n",
            "train loss:  0.0282437801361084\n",
            "train loss:  0.0838925838470459\n",
            "train loss:  0.036082152277231216\n",
            "train loss:  0.06391788274049759\n",
            "train loss:  0.008079670369625092\n",
            "train loss:  0.01639135554432869\n",
            "train loss:  0.005218640435487032\n",
            "train loss:  0.03306400403380394\n",
            "train loss:  0.001186460955068469\n",
            "train loss:  0.06513245403766632\n",
            "train loss:  0.009011250920593739\n",
            "train loss:  0.09995703399181366\n",
            "train loss:  0.013835687190294266\n",
            "train loss:  0.12722979485988617\n",
            "train loss:  0.026431066915392876\n",
            "train loss:  0.07140059024095535\n",
            "train loss:  0.002213293220847845\n",
            "train loss:  0.02320435270667076\n",
            "train loss:  0.0048605832271277905\n",
            "train loss:  0.029982350766658783\n",
            "train loss:  0.05233119800686836\n",
            "train loss:  0.047329336404800415\n",
            "train loss:  0.006620768457651138\n",
            "train loss:  0.033785972744226456\n",
            "train loss:  0.036203209310770035\n",
            "train loss:  0.007218869868665934\n",
            "train loss:  0.003312761662527919\n",
            "train loss:  0.027712786570191383\n",
            "train loss:  0.09753843396902084\n",
            "train loss:  0.010260066017508507\n",
            "train loss:  0.05496390908956528\n",
            "train loss:  0.005237981211394072\n",
            "train loss:  0.07759132981300354\n",
            "train loss:  0.046299614012241364\n",
            "train loss:  0.10461045801639557\n",
            "train loss:  0.03982773795723915\n",
            "train loss:  0.007081315852701664\n",
            "train loss:  0.03801049292087555\n",
            "train loss:  0.03253832086920738\n",
            "train loss:  0.010159678757190704\n",
            "train loss:  0.11920116096735\n",
            "train loss:  0.006056919228285551\n",
            "train loss:  0.01038436871021986\n",
            "train loss:  0.0007071011350490153\n",
            "train loss:  0.04633559659123421\n",
            "train loss:  0.1246093362569809\n",
            "train loss:  0.013860264793038368\n",
            "train loss:  0.05554923415184021\n",
            "train loss:  0.012345819734036922\n",
            "train loss:  0.035191792994737625\n",
            "train loss:  0.014885014854371548\n",
            "train loss:  0.02842111885547638\n",
            "train loss:  0.008997751399874687\n",
            "train loss:  0.008800485171377659\n",
            "train loss:  0.022907759994268417\n",
            "train loss:  0.011553952470421791\n",
            "train loss:  0.11454973369836807\n",
            "train loss:  0.03698727488517761\n",
            "train loss:  0.12419124692678452\n",
            "train loss:  0.06409257650375366\n",
            "train loss:  0.03966522961854935\n",
            "train loss:  0.05657067894935608\n",
            "train loss:  0.009389330632984638\n",
            "train loss:  0.047856297343969345\n",
            "train loss:  0.005087081342935562\n",
            "train loss:  0.019009988754987717\n",
            "train loss:  0.019606415182352066\n",
            "train loss:  0.09381747245788574\n",
            "train loss:  0.09034169465303421\n",
            "train loss:  0.032308127731084824\n",
            "train loss:  0.03152279928326607\n",
            "train loss:  0.018030522391200066\n",
            "train loss:  0.012144011445343494\n",
            "train loss:  0.048764560371637344\n",
            "train loss:  0.040529873222112656\n",
            "train loss:  0.04809578135609627\n",
            "train loss:  0.012263376265764236\n",
            "train loss:  0.05639316141605377\n",
            "train loss:  0.003131833393126726\n",
            "train loss:  0.052577707916498184\n",
            "train loss:  0.0063767386600375175\n",
            "train loss:  0.020120613276958466\n",
            "train loss:  0.02269144542515278\n",
            "train loss:  0.02698715403676033\n",
            "train loss:  0.002911187708377838\n",
            "train loss:  0.0506628155708313\n",
            "train loss:  0.012865949422121048\n",
            "train loss:  0.0017395870527252555\n",
            "train loss:  0.009873874485492706\n",
            "train loss:  0.005388298071920872\n",
            "train loss:  0.04396988824009895\n",
            "train loss:  0.021173734217882156\n",
            "train loss:  0.01683935523033142\n",
            "train loss:  0.04177910089492798\n",
            "train loss:  0.004197950474917889\n",
            "train loss:  0.0422387421131134\n",
            "train loss:  0.0020308250095695257\n",
            "train loss:  0.05305968225002289\n",
            "train loss:  0.013210300356149673\n",
            "train loss:  0.051437024027109146\n",
            "train loss:  0.04928676038980484\n",
            "train loss:  0.03802457079291344\n",
            "train loss:  0.17078174650669098\n",
            "train loss:  0.014311464503407478\n",
            "train loss:  0.04402880370616913\n",
            "train loss:  0.024425113573670387\n",
            "train loss:  0.08726294338703156\n",
            "train loss:  0.038474567234516144\n",
            "train loss:  0.01089573372155428\n",
            "train loss:  0.1188390925526619\n",
            "train loss:  0.004800258670002222\n",
            "train loss:  0.03888456150889397\n",
            "train loss:  0.026411470025777817\n",
            "train loss:  0.010612597689032555\n",
            "train loss:  0.010883248411118984\n",
            "train loss:  0.019622405990958214\n",
            "train loss:  0.08828376978635788\n",
            "train loss:  0.14563512802124023\n",
            "train loss:  0.1232350617647171\n",
            "train loss:  0.0025766249746084213\n",
            "train loss:  0.01290407869964838\n",
            "train loss:  0.01359298825263977\n",
            "train loss:  0.045731645077466965\n",
            "train loss:  0.0319930799305439\n",
            "train loss:  0.012900824658572674\n",
            "train loss:  0.014682434499263763\n",
            "train loss:  0.09911386668682098\n",
            "train loss:  0.059723127633333206\n",
            "train loss:  0.05130058526992798\n",
            "train loss:  0.04114479571580887\n",
            "train loss:  0.012651114724576473\n",
            "train loss:  0.021951880306005478\n",
            "train loss:  0.015026682987809181\n",
            "train loss:  0.07656987011432648\n",
            "train loss:  0.04464482143521309\n",
            "train loss:  0.0030662426725029945\n",
            "train loss:  0.0954722911119461\n",
            "train loss:  0.0046518235467374325\n",
            "train loss:  0.0744718387722969\n",
            "train loss:  0.052184704691171646\n",
            "train loss:  0.01808807998895645\n",
            "train loss:  0.026923656463623047\n",
            "train loss:  0.008044338785111904\n",
            "train loss:  0.01476321555674076\n",
            "train loss:  0.07074496895074844\n",
            "train loss:  0.06348637491464615\n",
            "train loss:  0.13203151524066925\n",
            "train loss:  0.017545398324728012\n",
            "train loss:  0.019796639680862427\n",
            "train loss:  0.08516968041658401\n",
            "train loss:  0.012602788396179676\n",
            "train loss:  0.008154855109751225\n",
            "train loss:  0.02767939679324627\n",
            "train loss:  0.03729496896266937\n",
            "train loss:  0.009731985628604889\n",
            "train loss:  0.041170623153448105\n",
            "train loss:  0.029333872720599174\n",
            "train loss:  0.08416030555963516\n",
            "train loss:  0.03493760898709297\n",
            "train loss:  0.00734314601868391\n",
            "train loss:  0.07521270960569382\n",
            "train loss:  0.007694480940699577\n",
            "train loss:  0.019327078014612198\n",
            "train loss:  0.01626608520746231\n",
            "train loss:  0.020998431369662285\n",
            "train loss:  0.003761489177122712\n",
            "train loss:  0.008020841516554356\n",
            "train loss:  0.020946919918060303\n",
            "train loss:  0.005792568903416395\n",
            "train loss:  0.017654791474342346\n",
            "train loss:  0.04021931812167168\n",
            "train loss:  0.07948491722345352\n",
            "train loss:  0.038807325065135956\n",
            "train loss:  0.0918840765953064\n",
            "train loss:  0.017004555091261864\n",
            "train loss:  0.10938804596662521\n",
            "train loss:  0.00867463555186987\n",
            "train loss:  0.005307946819812059\n",
            "train loss:  0.04327207803726196\n",
            "train loss:  0.0170853640884161\n",
            "train loss:  0.01603635959327221\n",
            "train loss:  0.016460679471492767\n",
            "train loss:  0.012867157347500324\n",
            "train loss:  0.00666173966601491\n",
            "train loss:  0.012520192191004753\n",
            "train loss:  0.02553708106279373\n",
            "train loss:  0.021685460582375526\n",
            "train loss:  0.02575659193098545\n",
            "train loss:  0.06863360852003098\n",
            "train loss:  0.015466618351638317\n",
            "train loss:  0.008622895926237106\n",
            "train loss:  0.026028208434581757\n",
            "train loss:  0.04416080191731453\n",
            "train loss:  0.009165501222014427\n",
            "train loss:  0.02316022291779518\n",
            "train loss:  0.0213413555175066\n",
            "train loss:  0.0020714695565402508\n",
            "train loss:  0.014553045853972435\n",
            "train loss:  0.006249620113521814\n",
            "train loss:  0.042378880083560944\n",
            "train loss:  0.004907808266580105\n",
            "train loss:  0.003343630349263549\n",
            "train loss:  0.0045846812427043915\n",
            "train loss:  0.04124201089143753\n",
            "train loss:  0.0020142383873462677\n",
            "train loss:  0.0222832802683115\n",
            "train loss:  0.003937424160540104\n",
            "train loss:  0.022534403949975967\n",
            "train loss:  0.004761308431625366\n",
            "train loss:  0.00638329004868865\n",
            "train loss:  0.11716347187757492\n",
            "train loss:  0.11625261604785919\n",
            "train loss:  0.024100665003061295\n",
            "train loss:  0.05455508828163147\n",
            "train loss:  0.0389421246945858\n",
            "train loss:  0.009940877556800842\n",
            "train loss:  0.042140841484069824\n",
            "train loss:  0.0051456959918141365\n",
            "train loss:  0.03728635981678963\n",
            "train loss:  0.020481355488300323\n",
            "train loss:  0.07351399958133698\n",
            "train loss:  0.09571032226085663\n",
            "train loss:  0.03915936499834061\n",
            "train loss:  0.0073523870669305325\n",
            "train loss:  0.006910743657499552\n",
            "train loss:  0.05013541877269745\n",
            "train loss:  0.13552367687225342\n",
            "train loss:  0.09199035167694092\n",
            "train loss:  0.03201262280344963\n",
            "train loss:  0.025591516867280006\n",
            "train loss:  0.02484295703470707\n",
            "train loss:  0.01727372035384178\n",
            "train loss:  0.03408302739262581\n",
            "train loss:  0.012752929702401161\n",
            "train loss:  0.05919397622346878\n",
            "train loss:  0.031928900629282\n",
            "train loss:  0.00840153731405735\n",
            "train loss:  0.018881184980273247\n",
            "train loss:  0.019357848912477493\n",
            "train loss:  0.02492869645357132\n",
            "train loss:  0.01735037937760353\n",
            "train loss:  0.015122239477932453\n",
            "train loss:  0.07979398965835571\n",
            "train loss:  0.08605925738811493\n",
            "train loss:  0.013132396154105663\n",
            "train loss:  0.05290435254573822\n",
            "train loss:  0.0036888623144477606\n",
            "train loss:  0.008233420550823212\n",
            "train loss:  0.028386447578668594\n",
            "train loss:  0.09576232731342316\n",
            "train loss:  0.009081009775400162\n",
            "train loss:  0.013994245789945126\n",
            "train loss:  0.038877081125974655\n",
            "train loss:  0.0426546111702919\n",
            "train loss:  0.0345185212790966\n",
            "train loss:  0.0266802366822958\n",
            "train loss:  0.003466459224000573\n",
            "train loss:  0.031172780320048332\n",
            "train loss:  0.028048766776919365\n",
            "train loss:  0.00488278828561306\n",
            "train loss:  0.03853495791554451\n",
            "train loss:  0.00816536508500576\n",
            "train loss:  0.01919194497168064\n",
            "train loss:  0.029714010655879974\n",
            "train loss:  0.021509889513254166\n",
            "train loss:  0.012586355209350586\n",
            "train loss:  0.023160627111792564\n",
            "train loss:  0.005608274135738611\n",
            "train loss:  0.018416645005345345\n",
            "train loss:  0.08727731555700302\n",
            "train loss:  0.04985776171088219\n",
            "train loss:  0.011075882241129875\n",
            "train loss:  0.0009702149545773864\n",
            "train loss:  0.02413364127278328\n",
            "train loss:  0.022401556372642517\n",
            "train loss:  0.05871463567018509\n",
            "train loss:  0.01590026170015335\n",
            "train loss:  0.0064530447125434875\n",
            "train loss:  0.06650969386100769\n",
            "train loss:  0.0439012385904789\n",
            "train loss:  0.01271011307835579\n",
            "train loss:  0.09980246424674988\n",
            "train loss:  0.0067718843929469585\n",
            "train loss:  0.006714164279401302\n",
            "train loss:  0.14530232548713684\n",
            "train loss:  0.031034404411911964\n",
            "train loss:  0.04754399135708809\n",
            "train loss:  0.0019930663984268904\n",
            "train loss:  0.04571480676531792\n",
            "train loss:  0.01951303891837597\n",
            "train loss:  0.027871638536453247\n",
            "train loss:  0.02545037306845188\n",
            "train loss:  0.0013111328007653356\n",
            "train loss:  0.01793050952255726\n",
            "train loss:  0.01374189741909504\n",
            "train loss:  0.01703580468893051\n",
            "train loss:  0.02053666301071644\n",
            "train loss:  0.12346945703029633\n",
            "train loss:  0.08809877187013626\n",
            "train loss:  0.004482018295675516\n",
            "train loss:  0.029162123799324036\n",
            "train loss:  0.0007370113744400442\n",
            "train loss:  0.09502101689577103\n",
            "train loss:  0.00627592159435153\n",
            "train loss:  0.01067252829670906\n",
            "train loss:  0.012940344400703907\n",
            "train loss:  0.06606166809797287\n",
            "train loss:  0.04710807651281357\n",
            "train loss:  0.013464697636663914\n",
            "train loss:  0.06840968877077103\n",
            "train loss:  0.008311060257256031\n",
            "train loss:  0.003700928995385766\n",
            "train loss:  0.036820292472839355\n",
            "train loss:  0.007269453722983599\n",
            "train loss:  0.013452433049678802\n",
            "train loss:  0.017817528918385506\n",
            "train loss:  0.026414694264531136\n",
            "train loss:  0.004679406061768532\n",
            "train loss:  0.01148055400699377\n",
            "train loss:  0.01847095414996147\n",
            "train loss:  0.11318129301071167\n",
            "train loss:  0.024184802547097206\n",
            "train loss:  0.005884233396500349\n",
            "train loss:  0.02679968625307083\n",
            "train loss:  0.13884204626083374\n",
            "train loss:  0.04294424504041672\n",
            "train loss:  0.010557739064097404\n",
            "train loss:  0.016974154859781265\n",
            "train loss:  0.05407823994755745\n",
            "train loss:  0.02980206161737442\n",
            "train loss:  0.00873262993991375\n",
            "train loss:  0.007229952607303858\n",
            "train loss:  0.005119208246469498\n",
            "train loss:  0.01381844561547041\n",
            "train loss:  0.016938913613557816\n",
            "train loss:  0.0013478874461725354\n",
            "train loss:  0.008144288323819637\n",
            "train loss:  0.001960166497156024\n",
            "train loss:  0.2810388207435608\n",
            "train loss:  0.010165641084313393\n",
            "train loss:  0.013667305931448936\n",
            "train loss:  0.05752849951386452\n",
            "train loss:  0.09011314809322357\n",
            "train loss:  0.006095612421631813\n",
            "train loss:  0.007269613444805145\n",
            "train loss:  0.012280328199267387\n",
            "train loss:  0.055311333388090134\n",
            "train loss:  0.011876827105879784\n",
            "train loss:  0.006988018285483122\n",
            "train loss:  0.03791205212473869\n",
            "train loss:  0.05316486954689026\n",
            "train loss:  0.027092095464468002\n",
            "train loss:  0.0035587758757174015\n",
            "train loss:  0.023933613672852516\n",
            "train loss:  0.0011182992020621896\n",
            "train loss:  0.024028560146689415\n",
            "train loss:  0.018208367750048637\n",
            "train loss:  0.002189596416428685\n",
            "train loss:  0.017153382301330566\n",
            "train loss:  0.02637512795627117\n",
            "train loss:  0.06573091447353363\n",
            "train loss:  0.015979155898094177\n",
            "train loss:  0.010815640911459923\n",
            "train loss:  0.0029986051376909018\n",
            "train loss:  0.07901741564273834\n",
            "train loss:  0.003449175739660859\n",
            "train loss:  0.0907030925154686\n",
            "train loss:  0.0064752972684800625\n",
            "train loss:  0.03710268810391426\n",
            "train loss:  0.05868717283010483\n",
            "train loss:  0.009716285392642021\n",
            "train loss:  0.013325878418982029\n",
            "train loss:  0.017497071996331215\n",
            "train loss:  0.0023952918127179146\n",
            "train loss:  0.020017806440591812\n",
            "train loss:  0.04008251801133156\n",
            "train loss:  0.08160815387964249\n",
            "train loss:  0.02265784703195095\n",
            "train loss:  0.09052777290344238\n",
            "train loss:  0.012215789407491684\n",
            "train loss:  0.003934728913009167\n",
            "train loss:  0.08099767565727234\n",
            "train loss:  0.0051303342916071415\n",
            "train loss:  0.033965300768613815\n",
            "train loss:  0.0054752761498093605\n",
            "train loss:  0.015125354751944542\n",
            "train loss:  0.004762446042150259\n",
            "train loss:  0.05393591150641441\n",
            "train loss:  0.004339301958680153\n",
            "train loss:  0.11022170633077621\n",
            "train loss:  0.17643745243549347\n",
            "train loss:  0.04308449849486351\n",
            "train loss:  0.04761248454451561\n",
            "train loss:  0.030111417174339294\n",
            "train loss:  0.009941356256604195\n",
            "train loss:  0.04377760738134384\n",
            "train loss:  0.0735493004322052\n",
            "train loss:  0.017477555200457573\n",
            "train loss:  0.014850257895886898\n",
            "train loss:  0.044362541288137436\n",
            "train loss:  0.11239319294691086\n",
            "train loss:  0.01826932467520237\n",
            "train loss:  0.006945512257516384\n",
            "train loss:  0.053202252835035324\n",
            "train loss:  0.05175919085741043\n",
            "train loss:  0.0226920023560524\n",
            "train loss:  0.001086884061805904\n",
            "train loss:  0.016016371548175812\n",
            "train loss:  0.11292470246553421\n",
            "train loss:  0.050951696932315826\n",
            "train loss:  0.004580868873745203\n",
            "train loss:  0.09401419013738632\n",
            "train loss:  0.0004224899457767606\n",
            "train loss:  0.03769460320472717\n",
            "train loss:  0.0007637524395249784\n",
            "train loss:  0.038220349699258804\n",
            "train loss:  0.012347524985671043\n",
            "train loss:  0.21417373418807983\n",
            "train loss:  0.004875065293163061\n",
            "train loss:  0.00815896037966013\n",
            "train loss:  0.0010487306863069534\n",
            "train loss:  0.001264920923858881\n",
            "train loss:  0.043133825063705444\n",
            "train loss:  0.016034288331866264\n",
            "train loss:  0.04662490263581276\n",
            "train loss:  0.05751870200037956\n",
            "train loss:  0.006768910214304924\n",
            "train loss:  0.0683567151427269\n",
            "train loss:  0.011634351685643196\n",
            "train loss:  0.064870186150074\n",
            "train loss:  0.011965462937951088\n",
            "train loss:  0.02789526805281639\n",
            "train loss:  0.014302246272563934\n",
            "train loss:  0.01794794201850891\n",
            "train loss:  0.003448588540777564\n",
            "train loss:  0.05733053758740425\n",
            "train loss:  0.05715528503060341\n",
            "train loss:  0.0024629845283925533\n",
            "train loss:  0.025961706414818764\n",
            "train loss:  0.005600952543318272\n",
            "train loss:  0.00748022086918354\n",
            "train loss:  0.039890971034765244\n",
            "train loss:  0.019665732979774475\n",
            "train loss:  0.11234986037015915\n",
            "train loss:  0.01825420930981636\n",
            "train loss:  0.020076189190149307\n",
            "train loss:  0.03546776995062828\n",
            "train loss:  0.062291745096445084\n",
            "train loss:  0.04587786644697189\n",
            "train loss:  0.04065534472465515\n",
            "train loss:  0.009679600596427917\n",
            "train loss:  0.009563829749822617\n",
            "train loss:  0.04618482291698456\n",
            "train loss:  0.01557440496981144\n",
            "train loss:  0.015157947316765785\n",
            "train loss:  0.004489768296480179\n",
            "train loss:  0.020294511690735817\n",
            "train loss:  0.003489395370706916\n",
            "train loss:  0.0313694067299366\n",
            "train loss:  0.17298263311386108\n",
            "train loss:  0.01183332223445177\n",
            "train loss:  0.002735310001298785\n",
            "train loss:  0.010453565046191216\n",
            "train loss:  0.004312090575695038\n",
            "train loss:  0.015530501492321491\n",
            "train loss:  0.035900045186281204\n",
            "train loss:  0.00942555908113718\n",
            "train loss:  0.018053848296403885\n",
            "train loss:  0.07560915499925613\n",
            "train loss:  0.035193368792533875\n",
            "train loss:  0.003521107602864504\n",
            "train loss:  0.003706079674884677\n",
            "train loss:  0.020379111170768738\n",
            "train loss:  0.026510970667004585\n",
            "train loss:  0.017061397433280945\n",
            "train loss:  0.0011027667205780745\n",
            "train loss:  0.04188840463757515\n",
            "train loss:  0.026338379830121994\n",
            "train loss:  0.015151379629969597\n",
            "train loss:  0.03787016496062279\n",
            "train loss:  0.027004241943359375\n",
            "train loss:  0.012835496105253696\n",
            "train loss:  0.005378680769354105\n",
            "train loss:  0.014210766181349754\n",
            "train loss:  0.01620921492576599\n",
            "train loss:  0.0029430873692035675\n",
            "train loss:  0.019590390846133232\n",
            "train loss:  0.058193907141685486\n",
            "train loss:  0.07946020364761353\n",
            "train loss:  0.003398535307496786\n",
            "train loss:  0.017862992361187935\n",
            "train loss:  0.0008950978517532349\n",
            "train loss:  0.006845087744295597\n",
            "train loss:  0.024388674646615982\n",
            "train loss:  0.006500910967588425\n",
            "train loss:  0.040526289492845535\n",
            "train loss:  0.010058384388685226\n",
            "train loss:  0.07565012574195862\n",
            "train loss:  0.05112655088305473\n",
            "train loss:  0.0444556288421154\n",
            "train loss:  0.016514940187335014\n",
            "train loss:  0.006219265516847372\n",
            "train loss:  0.000902501167729497\n",
            "train loss:  0.024348657578229904\n",
            "train loss:  0.02225222811102867\n",
            "train loss:  0.023192984983325005\n",
            "train loss:  0.005783699918538332\n",
            "train loss:  0.005058768205344677\n",
            "train loss:  0.08168841153383255\n",
            "train loss:  0.015988530591130257\n",
            "train loss:  0.027145350351929665\n",
            "train loss:  0.04051831737160683\n",
            "train loss:  0.04323948547244072\n",
            "train loss:  0.04479682818055153\n",
            "train loss:  0.03812562674283981\n",
            "train loss:  0.0403880700469017\n",
            "train loss:  0.006910775788128376\n",
            "train loss:  0.017583927139639854\n",
            "train loss:  0.0765383169054985\n",
            "train loss:  0.03333905339241028\n",
            "train loss:  0.03402593731880188\n",
            "train loss:  0.008503147400915623\n",
            "train loss:  0.0009622233919799328\n",
            "train loss:  0.006942326668649912\n",
            "train loss:  0.015136786736547947\n",
            "train loss:  0.061588071286678314\n",
            "train loss:  0.004518867935985327\n",
            "train loss:  0.054280418902635574\n",
            "train loss:  0.004115146119147539\n",
            "train loss:  0.02639784663915634\n",
            "train loss:  0.041238099336624146\n",
            "train loss:  0.058201540261507034\n",
            "train loss:  0.0015738526126369834\n",
            "train loss:  0.02026348002254963\n",
            "train loss:  0.0032145578879863024\n",
            "train loss:  0.027072874829173088\n",
            "train loss:  0.011022768914699554\n",
            "train loss:  0.022556515410542488\n",
            "train loss:  0.01112985797226429\n",
            "train loss:  0.009297965094447136\n",
            "train loss:  0.04060676321387291\n",
            "train loss:  0.0019597969949245453\n",
            "train loss:  0.07003148645162582\n",
            "train loss:  0.029042625799775124\n",
            "train loss:  0.10580720007419586\n",
            "train loss:  0.00241011637263\n",
            "train loss:  0.15579727292060852\n",
            "train loss:  0.04024273157119751\n",
            "train loss:  0.00543499318882823\n",
            "train loss:  0.00033191527472808957\n",
            "train loss:  0.19734100997447968\n",
            "train loss:  0.08503593504428864\n",
            "train loss:  0.02596072293817997\n",
            "train loss:  0.010416104458272457\n",
            "train loss:  0.025152822956442833\n",
            "train loss:  0.00238491571508348\n",
            "train loss:  0.10964944213628769\n",
            "train loss:  0.01490647904574871\n",
            "train loss:  0.08436065167188644\n",
            "train loss:  0.057599037885665894\n",
            "train loss:  0.006376633420586586\n",
            "train loss:  0.04154202342033386\n",
            "train loss:  0.035861656069755554\n",
            "train loss:  0.21038950979709625\n",
            "train loss:  0.03721155226230621\n",
            "train loss:  0.003122669644653797\n",
            "train loss:  0.007334868889302015\n",
            "train loss:  0.058845825493335724\n",
            "train loss:  0.016174200922250748\n",
            "train loss:  0.03740724176168442\n",
            "train loss:  0.0020747731905430555\n",
            "train loss:  0.08269314467906952\n",
            "train loss:  0.04366130754351616\n",
            "train loss:  0.07551561295986176\n",
            "train loss:  0.0273328498005867\n",
            "train loss:  0.010467004962265491\n",
            "train loss:  0.07822737097740173\n",
            "train loss:  0.0020183445885777473\n",
            "train loss:  0.022916702553629875\n",
            "train loss:  0.01575581356883049\n",
            "train loss:  0.0027448530308902264\n",
            "train loss:  0.0007308153435587883\n",
            "train loss:  0.017963655292987823\n",
            "train loss:  0.006317224819213152\n",
            "train loss:  0.16391117870807648\n",
            "train loss:  0.01780608482658863\n",
            "train loss:  0.002714931732043624\n",
            "train loss:  0.007190875709056854\n",
            "train loss:  0.08876778185367584\n",
            "train loss:  0.017939681187272072\n",
            "train loss:  0.01774100586771965\n",
            "train loss:  0.0037942007184028625\n",
            "train loss:  0.011248376220464706\n",
            "train loss:  0.08134230226278305\n",
            "train loss:  0.011290660127997398\n",
            "train loss:  0.006913934368640184\n",
            "train loss:  0.009458974003791809\n",
            "train loss:  0.005623306147754192\n",
            "train loss:  0.015929821878671646\n",
            "train loss:  0.004332493059337139\n",
            "train loss:  0.0028224396519362926\n",
            "train loss:  0.10203102976083755\n",
            "train loss:  0.004485501442104578\n",
            "train loss:  0.0063309683464467525\n",
            "train loss:  0.015623603947460651\n",
            "train loss:  0.008926774375140667\n",
            "train loss:  0.011387698352336884\n",
            "train loss:  0.001729172538034618\n",
            "train loss:  0.0025913193821907043\n",
            "train loss:  0.007345253136008978\n",
            "train loss:  0.014648248441517353\n",
            "train loss:  0.06844320148229599\n",
            "train loss:  0.00299625052139163\n",
            "train loss:  0.009008100256323814\n",
            "train loss:  0.00354170729406178\n",
            "train loss:  0.017556024715304375\n",
            "train loss:  0.04991888999938965\n",
            "train loss:  0.02452576346695423\n",
            "train loss:  0.005752159748226404\n",
            "train loss:  0.022274134680628777\n",
            "train loss:  0.03967313468456268\n",
            "train loss:  0.08014566451311111\n",
            "train loss:  0.03664271533489227\n",
            "train loss:  0.013867325149476528\n",
            "train loss:  0.0033221824560314417\n",
            "train loss:  0.028577588498592377\n",
            "train loss:  0.004534290637820959\n",
            "train loss:  0.06403421610593796\n",
            "train loss:  0.0004484735836740583\n",
            "train loss:  0.04227137565612793\n",
            "train loss:  0.04137543588876724\n",
            "train loss:  0.03822938725352287\n",
            "train loss:  0.0051009864546358585\n",
            "train loss:  0.00976142380386591\n",
            "train loss:  0.0013454619329422712\n",
            "train loss:  0.033592432737350464\n",
            "train loss:  0.02403685450553894\n",
            "train loss:  0.022321589291095734\n",
            "train loss:  0.020633433014154434\n",
            "train loss:  0.013216874562203884\n",
            "train loss:  0.061935026198625565\n",
            "train loss:  0.06283514201641083\n",
            "train loss:  0.004107697866857052\n",
            "train loss:  0.017119603231549263\n",
            "train loss:  0.03718610852956772\n",
            "train loss:  0.03133399412035942\n",
            "train loss:  0.01938539370894432\n",
            "train loss:  0.012290150858461857\n",
            "train loss:  0.020253822207450867\n",
            "train loss:  0.007372564636170864\n",
            "train loss:  0.020173393189907074\n",
            "train loss:  0.07033079862594604\n",
            "train loss:  0.036297306418418884\n",
            "train loss:  0.11410314589738846\n",
            "train loss:  0.02793259359896183\n",
            "train loss:  0.014348099939525127\n",
            "train loss:  0.008744136430323124\n",
            "train loss:  0.012292849831283092\n",
            "train loss:  0.013751511462032795\n",
            "train loss:  0.04685647040605545\n",
            "train loss:  0.06245832145214081\n",
            "train loss:  0.06022420525550842\n",
            "train loss:  0.010868484154343605\n",
            "train loss:  0.005931798834353685\n",
            "train loss:  0.016694901511073112\n",
            "train loss:  0.06850657612085342\n",
            "train loss:  0.003923503216356039\n",
            "train loss:  0.036540787667036057\n",
            "train loss:  0.010651673190295696\n",
            "train loss:  0.007954769767820835\n",
            "train loss:  0.07241063565015793\n",
            "train loss:  0.11360012739896774\n",
            "train loss:  0.01892803981900215\n",
            "train loss:  0.11009079962968826\n",
            "train loss:  0.05603570118546486\n",
            "train loss:  0.004114558920264244\n",
            "train loss:  0.028223291039466858\n",
            "train loss:  0.0050664981827139854\n",
            "train loss:  0.002770842984318733\n",
            "train loss:  0.014424039982259274\n",
            "train loss:  0.01885041408240795\n",
            "train loss:  0.06298495829105377\n",
            "train loss:  0.027478797361254692\n",
            "train loss:  0.0030332952737808228\n",
            "train loss:  0.005079963244497776\n",
            "train loss:  0.03440501540899277\n",
            "train loss:  0.032364509999752045\n",
            "train loss:  0.047510698437690735\n",
            "train loss:  0.03121231123805046\n",
            "train loss:  0.01389773003757\n",
            "train loss:  0.004901649430394173\n",
            "train loss:  0.016666630282998085\n",
            "train loss:  0.01244678720831871\n",
            "train loss:  0.08119743317365646\n",
            "train loss:  0.08967504650354385\n",
            "train loss:  0.0468548983335495\n",
            "train loss:  0.16190564632415771\n",
            "train loss:  0.04091838374733925\n",
            "train loss:  0.01888810284435749\n",
            "train loss:  0.037700239568948746\n",
            "train loss:  0.013035536743700504\n",
            "train loss:  0.10256917774677277\n",
            "train loss:  0.059686191380023956\n",
            "train loss:  0.005787781439721584\n",
            "train loss:  0.03499964624643326\n",
            "train loss:  0.004016323946416378\n",
            "train loss:  0.012102006934583187\n",
            "train loss:  0.019349267706274986\n",
            "train loss:  0.010301179252564907\n",
            "train loss:  0.04618034511804581\n",
            "train loss:  0.07207170128822327\n",
            "train loss:  0.005348169710487127\n",
            "train loss:  0.01802770048379898\n",
            "train loss:  0.07793763279914856\n",
            "train loss:  0.017524123191833496\n",
            "train loss:  0.16297271847724915\n",
            "train loss:  0.015149811282753944\n",
            "train loss:  0.050558481365442276\n",
            "train loss:  0.059645649045705795\n",
            "train loss:  0.0023441084194928408\n",
            "train loss:  0.002953724469989538\n",
            "train loss:  0.04083665832877159\n",
            "train loss:  0.0006080057937651873\n",
            "train loss:  0.014334232546389103\n",
            "train loss:  0.0168809425085783\n",
            "train loss:  0.0003420574066694826\n",
            "train loss:  0.002192562911659479\n",
            "train loss:  0.011547163128852844\n",
            "train loss:  0.012551109306514263\n",
            "train loss:  0.0454595685005188\n",
            "train loss:  0.0004586019495036453\n",
            "train loss:  0.00100784283131361\n",
            "val_loss:  0.09743612259626389\n",
            "Epoch:  6\n",
            "train loss:  0.07559248059988022\n",
            "train loss:  0.019239481538534164\n",
            "train loss:  0.0075765978544950485\n",
            "train loss:  0.07696039229631424\n",
            "train loss:  0.00486288545653224\n",
            "train loss:  0.0031471550464630127\n",
            "train loss:  0.06160307303071022\n",
            "train loss:  0.029902338981628418\n",
            "train loss:  0.017826121300458908\n",
            "train loss:  0.022940652444958687\n",
            "train loss:  0.020002806559205055\n",
            "train loss:  0.0015257901977747679\n",
            "train loss:  0.012042345479130745\n",
            "train loss:  0.03498239442706108\n",
            "train loss:  0.010017970576882362\n",
            "train loss:  0.007118629291653633\n",
            "train loss:  0.06406041979789734\n",
            "train loss:  0.015468748286366463\n",
            "train loss:  0.12299733608961105\n",
            "train loss:  0.01941079832613468\n",
            "train loss:  0.015597893856465816\n",
            "train loss:  0.05322224274277687\n",
            "train loss:  0.14820560812950134\n",
            "train loss:  0.0485609695315361\n",
            "train loss:  0.004687082022428513\n",
            "train loss:  0.01603817194700241\n",
            "train loss:  0.00401759147644043\n",
            "train loss:  0.006663465406745672\n",
            "train loss:  0.02110888808965683\n",
            "train loss:  0.0030447791796177626\n",
            "train loss:  0.040558844804763794\n",
            "train loss:  0.036742035299539566\n",
            "train loss:  0.023374268785119057\n",
            "train loss:  0.007980195805430412\n",
            "train loss:  0.05520036816596985\n",
            "train loss:  0.16579669713974\n",
            "train loss:  0.0575961172580719\n",
            "train loss:  0.05758276954293251\n",
            "train loss:  0.10675399750471115\n",
            "train loss:  0.018385181203484535\n",
            "train loss:  0.07375717908143997\n",
            "train loss:  0.025607237592339516\n",
            "train loss:  0.010180589742958546\n",
            "train loss:  0.10860547423362732\n",
            "train loss:  0.0033723965752869844\n",
            "train loss:  0.0029821060597896576\n",
            "train loss:  0.031493134796619415\n",
            "train loss:  0.05190012976527214\n",
            "train loss:  0.07384046167135239\n",
            "train loss:  0.032634325325489044\n",
            "train loss:  0.05099194124341011\n",
            "train loss:  0.004922949708998203\n",
            "train loss:  0.023446518927812576\n",
            "train loss:  0.055320847779512405\n",
            "train loss:  0.08308658003807068\n",
            "train loss:  0.04269339144229889\n",
            "train loss:  0.08780229836702347\n",
            "train loss:  0.035039495676755905\n",
            "train loss:  0.035634443163871765\n",
            "train loss:  0.0005134015227667987\n",
            "train loss:  0.022336332127451897\n",
            "train loss:  0.019983725622296333\n",
            "train loss:  0.022280724719166756\n",
            "train loss:  0.03614341840147972\n",
            "train loss:  0.07984575629234314\n",
            "train loss:  0.06950747966766357\n",
            "train loss:  0.0058244154788553715\n",
            "train loss:  0.025248294696211815\n",
            "train loss:  0.005216906778514385\n",
            "train loss:  0.005189057905226946\n",
            "train loss:  0.019352560862898827\n",
            "train loss:  0.05832274630665779\n",
            "train loss:  0.006535613443702459\n",
            "train loss:  0.014927995391190052\n",
            "train loss:  0.006774780340492725\n",
            "train loss:  0.02619527094066143\n",
            "train loss:  0.04852663725614548\n",
            "train loss:  0.02815171144902706\n",
            "train loss:  0.05453300476074219\n",
            "train loss:  0.04840666428208351\n",
            "train loss:  0.04300851374864578\n",
            "train loss:  0.025279846042394638\n",
            "train loss:  0.009558690711855888\n",
            "train loss:  0.13937987387180328\n",
            "train loss:  0.01612486131489277\n",
            "train loss:  0.02015337347984314\n",
            "train loss:  0.02726694382727146\n",
            "train loss:  0.010223401710391045\n",
            "train loss:  0.06596080213785172\n",
            "train loss:  0.003097100183367729\n",
            "train loss:  0.004602157510817051\n",
            "train loss:  0.035234954208135605\n",
            "train loss:  0.0023608088959008455\n",
            "train loss:  0.008378703147172928\n",
            "train loss:  0.009251156821846962\n",
            "train loss:  0.08630332350730896\n",
            "train loss:  0.13434967398643494\n",
            "train loss:  0.010160509496927261\n",
            "train loss:  0.05989077687263489\n",
            "train loss:  0.16753703355789185\n",
            "train loss:  0.0952230766415596\n",
            "train loss:  0.12293007224798203\n",
            "train loss:  0.0632559135556221\n",
            "train loss:  0.002193029969930649\n",
            "train loss:  0.012787883169949055\n",
            "train loss:  0.024131320416927338\n",
            "train loss:  0.001287739840336144\n",
            "train loss:  0.0664963647723198\n",
            "train loss:  0.07329496741294861\n",
            "train loss:  0.10540669411420822\n",
            "train loss:  0.013220899738371372\n",
            "train loss:  0.07915834337472916\n",
            "train loss:  0.019885927438735962\n",
            "train loss:  0.018359333276748657\n",
            "train loss:  0.044247496873140335\n",
            "train loss:  0.019058648496866226\n",
            "train loss:  0.0033935748506337404\n",
            "train loss:  0.009568962268531322\n",
            "train loss:  0.008958057500422001\n",
            "train loss:  0.04426142945885658\n",
            "train loss:  0.020085176452994347\n",
            "train loss:  0.03699207305908203\n",
            "train loss:  0.02543758414685726\n",
            "train loss:  0.04624468833208084\n",
            "train loss:  0.012021892704069614\n",
            "train loss:  0.014814202673733234\n",
            "train loss:  0.016996921971440315\n",
            "train loss:  0.008118551224470139\n",
            "train loss:  0.034360550343990326\n",
            "train loss:  0.02471461147069931\n",
            "train loss:  0.03664793446660042\n",
            "train loss:  0.014196624979376793\n",
            "train loss:  0.0031845809426158667\n",
            "train loss:  0.005053956527262926\n",
            "train loss:  0.0014924572315067053\n",
            "train loss:  0.020806239917874336\n",
            "train loss:  0.002697308547794819\n",
            "train loss:  0.02457723207771778\n",
            "train loss:  0.0069947573356330395\n",
            "train loss:  0.06939812749624252\n",
            "train loss:  0.030579088255763054\n",
            "train loss:  0.03961746022105217\n",
            "train loss:  0.004233440384268761\n",
            "train loss:  0.04819902032613754\n",
            "train loss:  0.002321345265954733\n",
            "train loss:  0.011283686384558678\n",
            "train loss:  0.014774058014154434\n",
            "train loss:  0.0417432077229023\n",
            "train loss:  0.06748932600021362\n",
            "train loss:  0.036709439009428024\n",
            "train loss:  0.002045791130512953\n",
            "train loss:  0.05287054553627968\n",
            "train loss:  0.00723061291500926\n",
            "train loss:  0.004209230188280344\n",
            "train loss:  0.002733685774728656\n",
            "train loss:  0.04644488915801048\n",
            "train loss:  0.04499555006623268\n",
            "train loss:  0.018849099054932594\n",
            "train loss:  0.05775686725974083\n",
            "train loss:  0.008495268411934376\n",
            "train loss:  0.01235751062631607\n",
            "train loss:  0.012496927753090858\n",
            "train loss:  0.13340109586715698\n",
            "train loss:  0.010223415680229664\n",
            "train loss:  0.0057715303264558315\n",
            "train loss:  0.003844569204375148\n",
            "train loss:  0.025536643341183662\n",
            "train loss:  0.005441288463771343\n",
            "train loss:  0.07629923522472382\n",
            "train loss:  0.0028264904394745827\n",
            "train loss:  0.0017634951509535313\n",
            "train loss:  0.0007068538689054549\n",
            "train loss:  0.008140039630234241\n",
            "train loss:  0.12948192656040192\n",
            "train loss:  0.00785570777952671\n",
            "train loss:  0.0630325898528099\n",
            "train loss:  0.00954441912472248\n",
            "train loss:  0.012602097354829311\n",
            "train loss:  0.014501423574984074\n",
            "train loss:  0.02528313547372818\n",
            "train loss:  0.02766742743551731\n",
            "train loss:  0.004663444124162197\n",
            "train loss:  0.0028525611851364374\n",
            "train loss:  0.016302550211548805\n",
            "train loss:  0.10732107609510422\n",
            "train loss:  0.026971904560923576\n",
            "train loss:  0.05769425258040428\n",
            "train loss:  0.03910527378320694\n",
            "train loss:  0.02161766029894352\n",
            "train loss:  0.0056312717497348785\n",
            "train loss:  0.006779195740818977\n",
            "train loss:  0.0077302404679358006\n",
            "train loss:  0.011640060693025589\n",
            "train loss:  0.012650910764932632\n",
            "train loss:  0.044743020087480545\n",
            "train loss:  0.08003497868776321\n",
            "train loss:  0.0010328935459256172\n",
            "train loss:  0.032968830317258835\n",
            "train loss:  0.00684609217569232\n",
            "train loss:  0.009014921262860298\n",
            "train loss:  0.01908387430012226\n",
            "train loss:  0.0016802866011857986\n",
            "train loss:  0.06170205771923065\n",
            "train loss:  0.019354989752173424\n",
            "train loss:  0.002252749865874648\n",
            "train loss:  0.0690978467464447\n",
            "train loss:  0.004844594281166792\n",
            "train loss:  0.020012447610497475\n",
            "train loss:  0.0074819112196564674\n",
            "train loss:  0.025538219138979912\n",
            "train loss:  0.0055847796611487865\n",
            "train loss:  0.005758773069828749\n",
            "train loss:  0.0026992696803063154\n",
            "train loss:  0.062205489724874496\n",
            "train loss:  0.0647253692150116\n",
            "train loss:  0.0058460840955376625\n",
            "train loss:  0.031348977237939835\n",
            "train loss:  0.03809924051165581\n",
            "train loss:  0.018741115927696228\n",
            "train loss:  0.0007568307337351143\n",
            "train loss:  0.009936550632119179\n",
            "train loss:  0.011467492207884789\n",
            "train loss:  0.008780230768024921\n",
            "train loss:  0.04977455735206604\n",
            "train loss:  0.004166318103671074\n",
            "train loss:  0.046186622232198715\n",
            "train loss:  0.03834955766797066\n",
            "train loss:  0.12013029307126999\n",
            "train loss:  0.042769905179739\n",
            "train loss:  0.03705309331417084\n",
            "train loss:  0.0036978425923734903\n",
            "train loss:  0.03400188684463501\n",
            "train loss:  0.06277011334896088\n",
            "train loss:  0.028337780386209488\n",
            "train loss:  0.04191742092370987\n",
            "train loss:  0.03551170974969864\n",
            "train loss:  0.06110547482967377\n",
            "train loss:  0.12810131907463074\n",
            "train loss:  0.003500966355204582\n",
            "train loss:  0.06967339664697647\n",
            "train loss:  0.030309580266475677\n",
            "train loss:  0.043424300849437714\n",
            "train loss:  0.03632083535194397\n",
            "train loss:  0.025790896266698837\n",
            "train loss:  0.004405538085848093\n",
            "train loss:  0.05080275982618332\n",
            "train loss:  0.09777636080980301\n",
            "train loss:  0.003128558164462447\n",
            "train loss:  0.06239410117268562\n",
            "train loss:  0.05820639058947563\n",
            "train loss:  0.003354353364557028\n",
            "train loss:  0.09792812168598175\n",
            "train loss:  0.0192352756857872\n",
            "train loss:  0.03215150162577629\n",
            "train loss:  0.002865669783204794\n",
            "train loss:  0.05631502717733383\n",
            "train loss:  0.03073517046868801\n",
            "train loss:  0.03506188839673996\n",
            "train loss:  0.0021251048892736435\n",
            "train loss:  0.011132552288472652\n",
            "train loss:  0.019894035533070564\n",
            "train loss:  0.039840053766965866\n",
            "train loss:  0.08194496482610703\n",
            "train loss:  0.004220086149871349\n",
            "train loss:  0.07845860719680786\n",
            "train loss:  0.0014278534799814224\n",
            "train loss:  0.09733936935663223\n",
            "train loss:  0.06719256192445755\n",
            "train loss:  0.034953393042087555\n",
            "train loss:  0.005396442022174597\n",
            "train loss:  0.026783205568790436\n",
            "train loss:  0.012374053709208965\n",
            "train loss:  0.0044202376157045364\n",
            "train loss:  0.004735495429486036\n",
            "train loss:  0.09231176227331161\n",
            "train loss:  0.03957618772983551\n",
            "train loss:  0.10015153884887695\n",
            "train loss:  0.021843744441866875\n",
            "train loss:  0.020985672250390053\n",
            "train loss:  0.1081298366189003\n",
            "train loss:  0.025102056562900543\n",
            "train loss:  0.028030043467879295\n",
            "train loss:  0.008019610308110714\n",
            "train loss:  0.020803311839699745\n",
            "train loss:  0.06124919280409813\n",
            "train loss:  0.010811150074005127\n",
            "train loss:  0.002274847822263837\n",
            "train loss:  0.01999676786363125\n",
            "train loss:  0.04236714541912079\n",
            "train loss:  0.010921637527644634\n",
            "train loss:  0.016500193625688553\n",
            "train loss:  0.00968572124838829\n",
            "train loss:  0.011808457784354687\n",
            "train loss:  0.00163150520529598\n",
            "train loss:  0.006778968498110771\n",
            "train loss:  0.0030182842165231705\n",
            "train loss:  0.02100490964949131\n",
            "train loss:  0.0938204899430275\n",
            "train loss:  0.04280265420675278\n",
            "train loss:  0.05069209262728691\n",
            "train loss:  0.05794338136911392\n",
            "train loss:  0.04140905663371086\n",
            "train loss:  0.025923892855644226\n",
            "train loss:  0.033958811312913895\n",
            "train loss:  0.008454078808426857\n",
            "train loss:  0.0007659395923838019\n",
            "train loss:  0.07687502354383469\n",
            "train loss:  0.005078661721199751\n",
            "train loss:  0.007155215833336115\n",
            "train loss:  0.005857652518898249\n",
            "train loss:  0.0023693435359746218\n",
            "train loss:  0.052013885229825974\n",
            "train loss:  0.004489090759307146\n",
            "train loss:  0.06970050930976868\n",
            "train loss:  0.006585190072655678\n",
            "train loss:  0.016372060403227806\n",
            "train loss:  0.03526598960161209\n",
            "train loss:  0.025666695088148117\n",
            "train loss:  0.04627605155110359\n",
            "train loss:  0.00784518662840128\n",
            "train loss:  0.007190570700913668\n",
            "train loss:  0.009521585889160633\n",
            "train loss:  0.030678987503051758\n",
            "train loss:  0.009757778607308865\n",
            "train loss:  0.0027806733269244432\n",
            "train loss:  0.020051950588822365\n",
            "train loss:  0.0070359185338020325\n",
            "train loss:  0.008868792094290257\n",
            "train loss:  0.011179063469171524\n",
            "train loss:  0.006867609452456236\n",
            "train loss:  0.02681051567196846\n",
            "train loss:  0.004906463902443647\n",
            "train loss:  0.011594277806580067\n",
            "train loss:  0.0016816535498946905\n",
            "train loss:  0.00951140932738781\n",
            "train loss:  0.015525585040450096\n",
            "train loss:  0.006509893573820591\n",
            "train loss:  0.006485507357865572\n",
            "train loss:  0.1043425053358078\n",
            "train loss:  0.04504184424877167\n",
            "train loss:  0.005523330997675657\n",
            "train loss:  0.020674731582403183\n",
            "train loss:  0.010309923440217972\n",
            "train loss:  0.004815959837287664\n",
            "train loss:  0.01485024206340313\n",
            "train loss:  0.003306793514639139\n",
            "train loss:  0.016218267381191254\n",
            "train loss:  0.06800162047147751\n",
            "train loss:  0.026025069877505302\n",
            "train loss:  0.061647433787584305\n",
            "train loss:  0.034844815731048584\n",
            "train loss:  0.012435723096132278\n",
            "train loss:  0.002161326352506876\n",
            "train loss:  0.02578769065439701\n",
            "train loss:  0.15152397751808167\n",
            "train loss:  0.0065359631553292274\n",
            "train loss:  0.007684628013521433\n",
            "train loss:  0.012461753562092781\n",
            "train loss:  0.010207323357462883\n",
            "train loss:  0.006323365494608879\n",
            "train loss:  0.01782781258225441\n",
            "train loss:  0.014996111392974854\n",
            "train loss:  0.011669836938381195\n",
            "train loss:  0.00684448191896081\n",
            "train loss:  0.02252594754099846\n",
            "train loss:  0.03801127150654793\n",
            "train loss:  0.034348826855421066\n",
            "train loss:  0.0010685586603358388\n",
            "train loss:  0.000846937473397702\n",
            "train loss:  0.003685390343889594\n",
            "train loss:  0.013387669809162617\n",
            "train loss:  0.011229339987039566\n",
            "train loss:  0.001186102512292564\n",
            "train loss:  0.05159863457083702\n",
            "train loss:  0.00036921637365594506\n",
            "train loss:  0.005258368793874979\n",
            "train loss:  0.013631125912070274\n",
            "train loss:  0.02194756455719471\n",
            "train loss:  0.0025730193592607975\n",
            "train loss:  0.004425404593348503\n",
            "train loss:  0.007981296628713608\n",
            "train loss:  0.040349919348955154\n",
            "train loss:  0.01880308985710144\n",
            "train loss:  0.003181600710377097\n",
            "train loss:  0.008895709179341793\n",
            "train loss:  0.008955633267760277\n",
            "train loss:  0.03571112081408501\n",
            "train loss:  0.011844431050121784\n",
            "train loss:  0.01107825804501772\n",
            "train loss:  0.04195433109998703\n",
            "train loss:  0.034171897917985916\n",
            "train loss:  0.01509382575750351\n",
            "train loss:  0.003919364418834448\n",
            "train loss:  0.011394348926842213\n",
            "train loss:  0.0016300642164424062\n",
            "train loss:  0.002081264043226838\n",
            "train loss:  0.08613476902246475\n",
            "train loss:  0.0034066764637827873\n",
            "train loss:  0.025614988058805466\n",
            "train loss:  0.004428590647876263\n",
            "train loss:  0.0023092604242265224\n",
            "train loss:  0.006714326795190573\n",
            "train loss:  0.036505889147520065\n",
            "train loss:  0.013522539287805557\n",
            "train loss:  0.08300456404685974\n",
            "train loss:  0.015349364839494228\n",
            "train loss:  0.01516139879822731\n",
            "train loss:  0.007551669608801603\n",
            "train loss:  0.03715331107378006\n",
            "train loss:  0.053041670471429825\n",
            "train loss:  0.003554015886038542\n",
            "train loss:  0.007034492678940296\n",
            "train loss:  0.12433770298957825\n",
            "train loss:  0.005679120775312185\n",
            "train loss:  0.01902165822684765\n",
            "train loss:  0.0008915822254493833\n",
            "train loss:  0.08904082328081131\n",
            "train loss:  0.061730314046144485\n",
            "train loss:  0.000511886493768543\n",
            "train loss:  0.002207818441092968\n",
            "train loss:  0.006741728633642197\n",
            "train loss:  0.03308068960905075\n",
            "train loss:  0.00229008961468935\n",
            "train loss:  0.0334300622344017\n",
            "train loss:  0.06278857588768005\n",
            "train loss:  0.20198266208171844\n",
            "train loss:  0.008016661740839481\n",
            "train loss:  0.013403127901256084\n",
            "train loss:  0.09373147040605545\n",
            "train loss:  0.02639736980199814\n",
            "train loss:  0.17013970017433167\n",
            "train loss:  0.006406813394278288\n",
            "train loss:  0.0018512378446757793\n",
            "train loss:  0.026251956820487976\n",
            "train loss:  0.03782285749912262\n",
            "train loss:  0.013390309177339077\n",
            "train loss:  0.007954196073114872\n",
            "train loss:  0.10197911411523819\n",
            "train loss:  0.035635385662317276\n",
            "train loss:  0.002412476111203432\n",
            "train loss:  0.014500508084893227\n",
            "train loss:  0.030750975012779236\n",
            "train loss:  0.023992957547307014\n",
            "train loss:  0.04182077944278717\n",
            "train loss:  0.003518192796036601\n",
            "train loss:  0.0005751987919211388\n",
            "train loss:  0.016390562057495117\n",
            "train loss:  0.0017190066864714026\n",
            "train loss:  0.06985348463058472\n",
            "train loss:  0.013207235373556614\n",
            "train loss:  0.04860560968518257\n",
            "train loss:  0.08313076198101044\n",
            "train loss:  0.011401118710637093\n",
            "train loss:  0.10281740874052048\n",
            "train loss:  0.025067981332540512\n",
            "train loss:  0.028704913333058357\n",
            "train loss:  0.10008899122476578\n",
            "train loss:  0.013949872925877571\n",
            "train loss:  0.015754807740449905\n",
            "train loss:  0.02727496810257435\n",
            "train loss:  0.012245254591107368\n",
            "train loss:  0.009691791608929634\n",
            "train loss:  0.054320327937603\n",
            "train loss:  0.01839754916727543\n",
            "train loss:  0.020884715020656586\n",
            "train loss:  0.032940059900283813\n",
            "train loss:  0.041076723486185074\n",
            "train loss:  0.005587338469922543\n",
            "train loss:  0.044965241104364395\n",
            "train loss:  0.06265052407979965\n",
            "train loss:  0.13988453149795532\n",
            "train loss:  0.04267134517431259\n",
            "train loss:  0.008681830950081348\n",
            "train loss:  0.007137739565223455\n",
            "train loss:  0.00829355325549841\n",
            "train loss:  0.05359378084540367\n",
            "train loss:  0.008209346793591976\n",
            "train loss:  0.0551912821829319\n",
            "train loss:  0.01517071295529604\n",
            "train loss:  0.038807462900877\n",
            "train loss:  0.0017849109135568142\n",
            "train loss:  0.03577306121587753\n",
            "train loss:  0.0011718771420419216\n",
            "train loss:  0.016572730615735054\n",
            "train loss:  0.029173584654927254\n",
            "train loss:  0.0007229537586681545\n",
            "train loss:  0.006214507389813662\n",
            "train loss:  0.06247057765722275\n",
            "train loss:  0.006595089565962553\n",
            "train loss:  0.0308509673923254\n",
            "train loss:  0.004771074280142784\n",
            "train loss:  0.018993157893419266\n",
            "train loss:  0.004872766323387623\n",
            "train loss:  0.009869065135717392\n",
            "train loss:  0.030327748507261276\n",
            "train loss:  0.11451318114995956\n",
            "train loss:  0.04591450095176697\n",
            "train loss:  0.08540283888578415\n",
            "train loss:  0.010354394093155861\n",
            "train loss:  0.06431053578853607\n",
            "train loss:  0.0015689160209149122\n",
            "train loss:  0.01182616502046585\n",
            "train loss:  0.01678006537258625\n",
            "train loss:  0.03400509059429169\n",
            "train loss:  0.08303575962781906\n",
            "train loss:  0.006641717627644539\n",
            "train loss:  0.017316676676273346\n",
            "train loss:  0.058055102825164795\n",
            "train loss:  0.00034451697138138115\n",
            "train loss:  0.057668525725603104\n",
            "train loss:  0.02295532077550888\n",
            "train loss:  0.03004678338766098\n",
            "train loss:  0.02575744315981865\n",
            "train loss:  0.006808190140873194\n",
            "train loss:  0.011531475000083447\n",
            "train loss:  0.005316141992807388\n",
            "train loss:  0.0020398523192852736\n",
            "train loss:  0.061811432242393494\n",
            "train loss:  0.06948621571063995\n",
            "train loss:  0.06849013268947601\n",
            "train loss:  0.030085256323218346\n",
            "train loss:  0.007788120303303003\n",
            "train loss:  0.0020810733549296856\n",
            "train loss:  0.02105627954006195\n",
            "train loss:  0.04217943921685219\n",
            "train loss:  0.01630701683461666\n",
            "train loss:  0.0008277774904854596\n",
            "train loss:  0.008237647823989391\n",
            "train loss:  0.12855671346187592\n",
            "train loss:  0.006153362337499857\n",
            "train loss:  0.0059668924659490585\n",
            "train loss:  0.01912566088140011\n",
            "train loss:  0.0407925620675087\n",
            "train loss:  0.04994840919971466\n",
            "train loss:  0.0040467591024935246\n",
            "train loss:  0.0005936433444730937\n",
            "train loss:  0.009709996171295643\n",
            "train loss:  0.031570084393024445\n",
            "train loss:  0.0014447380090132356\n",
            "train loss:  0.031217407435178757\n",
            "train loss:  0.0010860782349482179\n",
            "train loss:  0.010693755000829697\n",
            "train loss:  0.0012130143586546183\n",
            "train loss:  0.011807244271039963\n",
            "train loss:  0.003781563602387905\n",
            "train loss:  0.06691855937242508\n",
            "train loss:  0.004969773348420858\n",
            "train loss:  0.027391131967306137\n",
            "train loss:  0.010439036414027214\n",
            "train loss:  0.0017660886514931917\n",
            "train loss:  0.08448701351881027\n",
            "train loss:  0.0164242684841156\n",
            "train loss:  0.011806908994913101\n",
            "train loss:  0.01274299155920744\n",
            "train loss:  0.0037618945352733135\n",
            "train loss:  0.020787255838513374\n",
            "train loss:  0.008741470053792\n",
            "train loss:  0.04037757217884064\n",
            "train loss:  0.07610496878623962\n",
            "train loss:  0.00554842222481966\n",
            "train loss:  0.00453695235773921\n",
            "train loss:  0.00829214695841074\n",
            "train loss:  0.09133285284042358\n",
            "train loss:  0.07838144153356552\n",
            "train loss:  0.03118814527988434\n",
            "train loss:  0.01611548289656639\n",
            "train loss:  0.005853896960616112\n",
            "train loss:  0.015574836172163486\n",
            "train loss:  0.02612636424601078\n",
            "train loss:  0.025743093341588974\n",
            "train loss:  0.03079543448984623\n",
            "train loss:  0.025770025327801704\n",
            "train loss:  0.1084088534116745\n",
            "train loss:  0.010179923847317696\n",
            "train loss:  0.08268585056066513\n",
            "train loss:  0.014141479507088661\n",
            "train loss:  0.026598969474434853\n",
            "train loss:  0.026923079043626785\n",
            "train loss:  0.007826740853488445\n",
            "train loss:  0.00116458791308105\n",
            "train loss:  0.012737181968986988\n",
            "train loss:  0.05602850392460823\n",
            "train loss:  0.015163546428084373\n",
            "train loss:  0.041045043617486954\n",
            "train loss:  0.10221680998802185\n",
            "train loss:  0.023110387846827507\n",
            "train loss:  0.017656531184911728\n",
            "train loss:  0.09633717685937881\n",
            "train loss:  0.039650578051805496\n",
            "train loss:  0.04769562557339668\n",
            "train loss:  0.10492799431085587\n",
            "train loss:  0.03252251073718071\n",
            "train loss:  0.06363815069198608\n",
            "train loss:  0.0034374017268419266\n",
            "train loss:  0.015273736789822578\n",
            "train loss:  0.004945062100887299\n",
            "train loss:  0.05480188876390457\n",
            "train loss:  0.012349670752882957\n",
            "train loss:  0.06590825319290161\n",
            "train loss:  0.00531731778755784\n",
            "train loss:  0.05667160451412201\n",
            "train loss:  0.013310934416949749\n",
            "train loss:  0.020643966272473335\n",
            "train loss:  0.003231406444683671\n",
            "train loss:  0.0557713508605957\n",
            "train loss:  0.00429177051410079\n",
            "train loss:  0.01516350544989109\n",
            "train loss:  0.08840180188417435\n",
            "train loss:  0.011265337467193604\n",
            "train loss:  0.0025833554100245237\n",
            "train loss:  0.03250841423869133\n",
            "train loss:  0.002008802490308881\n",
            "train loss:  0.00963502936065197\n",
            "train loss:  0.019817696884274483\n",
            "train loss:  0.03919772431254387\n",
            "train loss:  0.043176256120204926\n",
            "train loss:  0.04029029607772827\n",
            "train loss:  0.033010151237249374\n",
            "train loss:  0.026734765619039536\n",
            "train loss:  0.0035105927381664515\n",
            "train loss:  0.019426897168159485\n",
            "train loss:  0.0016103790840134025\n",
            "train loss:  0.01745779439806938\n",
            "train loss:  0.009290484711527824\n",
            "train loss:  0.061588507145643234\n",
            "train loss:  0.06101284548640251\n",
            "train loss:  0.04385138303041458\n",
            "train loss:  0.053017277270555496\n",
            "train loss:  0.029131580144166946\n",
            "train loss:  0.013046986423432827\n",
            "train loss:  0.0008862419635988772\n",
            "train loss:  0.05357014387845993\n",
            "train loss:  0.03226635232567787\n",
            "train loss:  0.013829156756401062\n",
            "train loss:  0.005476630758494139\n",
            "train loss:  0.0010617130901664495\n",
            "train loss:  0.0699893981218338\n",
            "train loss:  0.01009284146130085\n",
            "train loss:  0.00934007577598095\n",
            "train loss:  0.0039177886210381985\n",
            "train loss:  0.08702804148197174\n",
            "train loss:  0.06630698591470718\n",
            "train loss:  0.004318522289395332\n",
            "train loss:  0.02391188219189644\n",
            "train loss:  0.010675433091819286\n",
            "train loss:  0.04319390654563904\n",
            "train loss:  0.02083319053053856\n",
            "train loss:  0.0007259104168042541\n",
            "train loss:  0.01861765794456005\n",
            "train loss:  0.0032914099283516407\n",
            "train loss:  0.00118972675409168\n",
            "train loss:  0.055059608072042465\n",
            "train loss:  0.03113558515906334\n",
            "train loss:  0.006392624229192734\n",
            "train loss:  0.003052961779758334\n",
            "train loss:  0.025744261220097542\n",
            "train loss:  0.00047514677862636745\n",
            "train loss:  0.026437923312187195\n",
            "train loss:  0.005549135152250528\n",
            "train loss:  0.007161813788115978\n",
            "train loss:  0.0015740846283733845\n",
            "train loss:  0.048907626420259476\n",
            "train loss:  0.0767221748828888\n",
            "train loss:  0.006082234438508749\n",
            "train loss:  0.006285603623837233\n",
            "train loss:  0.0019330880604684353\n",
            "train loss:  0.00736366119235754\n",
            "train loss:  0.0018039176939055324\n",
            "train loss:  0.024198681116104126\n",
            "train loss:  0.011415651068091393\n",
            "train loss:  0.0688265711069107\n",
            "train loss:  0.012339048087596893\n",
            "train loss:  0.04689156264066696\n",
            "train loss:  0.023373663425445557\n",
            "train loss:  0.006057139486074448\n",
            "train loss:  0.015005098655819893\n",
            "train loss:  0.0019049086840823293\n",
            "train loss:  0.00042933967779390514\n",
            "train loss:  0.12531869113445282\n",
            "train loss:  0.010534944012761116\n",
            "train loss:  0.09285783767700195\n",
            "train loss:  0.02180611714720726\n",
            "train loss:  0.1490982472896576\n",
            "train loss:  0.011337844654917717\n",
            "train loss:  0.009959793649613857\n",
            "train loss:  0.005912609398365021\n",
            "train loss:  0.008842676877975464\n",
            "train loss:  0.04244720935821533\n",
            "train loss:  0.011108407750725746\n",
            "train loss:  0.02327694371342659\n",
            "train loss:  0.044605553150177\n",
            "train loss:  0.1634064018726349\n",
            "train loss:  0.0019603425171226263\n",
            "train loss:  0.07703001797199249\n",
            "train loss:  0.006503980606794357\n",
            "train loss:  0.05013253539800644\n",
            "train loss:  0.032757095992565155\n",
            "train loss:  0.04634568467736244\n",
            "train loss:  0.007325374986976385\n",
            "train loss:  0.019926046952605247\n",
            "train loss:  0.006748507730662823\n",
            "train loss:  0.05545637011528015\n",
            "train loss:  0.0013350013177841902\n",
            "train loss:  0.00035001832293346524\n",
            "train loss:  0.0685640349984169\n",
            "train loss:  0.009011736139655113\n",
            "train loss:  0.024488234892487526\n",
            "train loss:  0.00341246509924531\n",
            "train loss:  0.005886024795472622\n",
            "train loss:  0.0009378462564200163\n",
            "train loss:  0.005064934492111206\n",
            "train loss:  0.0017207614146173\n",
            "train loss:  0.07004889845848083\n",
            "train loss:  0.002018897794187069\n",
            "train loss:  0.0077598560601472855\n",
            "train loss:  0.006258409935981035\n",
            "train loss:  0.12393301725387573\n",
            "train loss:  0.034338660538196564\n",
            "train loss:  0.08557234704494476\n",
            "train loss:  0.0035298403818160295\n",
            "train loss:  0.0005339522613212466\n",
            "train loss:  0.0011499524116516113\n",
            "train loss:  0.0076829856261610985\n",
            "train loss:  0.026054227724671364\n",
            "train loss:  0.004869129974395037\n",
            "train loss:  0.00015268528659362346\n",
            "train loss:  0.006866461597383022\n",
            "train loss:  0.1781700998544693\n",
            "train loss:  0.010299758985638618\n",
            "train loss:  0.008139992132782936\n",
            "train loss:  0.04888829216361046\n",
            "train loss:  0.00267540430650115\n",
            "train loss:  0.0958537831902504\n",
            "train loss:  0.000592551426962018\n",
            "train loss:  0.008366117253899574\n",
            "train loss:  0.003935327287763357\n",
            "train loss:  0.013028254732489586\n",
            "train loss:  0.011883746832609177\n",
            "train loss:  0.033076923340559006\n",
            "train loss:  0.0008812154992483556\n",
            "train loss:  0.00036039025872014463\n",
            "train loss:  0.006374396849423647\n",
            "train loss:  0.01370800007134676\n",
            "train loss:  0.004666742403060198\n",
            "train loss:  0.0005105537711642683\n",
            "train loss:  0.009686700068414211\n",
            "train loss:  0.0019446435617282987\n",
            "train loss:  0.004684200510382652\n",
            "train loss:  0.03569561615586281\n",
            "train loss:  0.002106637693941593\n",
            "train loss:  0.0054336898028850555\n",
            "train loss:  0.007948928512632847\n",
            "train loss:  0.002677635056897998\n",
            "train loss:  0.028500089421868324\n",
            "train loss:  0.008853676728904247\n",
            "train loss:  0.04405265301465988\n",
            "train loss:  0.0005201805615797639\n",
            "train loss:  0.007663600146770477\n",
            "train loss:  0.005384017247706652\n",
            "train loss:  0.04749513044953346\n",
            "train loss:  0.017521096393465996\n",
            "train loss:  0.01970866322517395\n",
            "train loss:  0.002500403905287385\n",
            "train loss:  0.015575572848320007\n",
            "train loss:  0.005577834323048592\n",
            "train loss:  0.06039208918809891\n",
            "train loss:  0.00825707707554102\n",
            "train loss:  0.01692318171262741\n",
            "train loss:  0.22733668982982635\n",
            "train loss:  0.00956675410270691\n",
            "train loss:  0.001249520806595683\n",
            "train loss:  0.04555056616663933\n",
            "train loss:  0.0009462876478210092\n",
            "train loss:  0.00695109274238348\n",
            "train loss:  0.04259764775633812\n",
            "train loss:  0.007071558386087418\n",
            "train loss:  0.04453183710575104\n",
            "train loss:  0.0037245405837893486\n",
            "train loss:  0.01461697742342949\n",
            "train loss:  0.033684734255075455\n",
            "train loss:  0.020145302638411522\n",
            "train loss:  0.13619600236415863\n",
            "train loss:  0.01411973312497139\n",
            "train loss:  0.01906660571694374\n",
            "train loss:  0.010819248855113983\n",
            "train loss:  0.003130346769466996\n",
            "train loss:  0.08256834000349045\n",
            "train loss:  0.03721979260444641\n",
            "train loss:  0.09847047924995422\n",
            "train loss:  0.018099894747138023\n",
            "train loss:  0.12474016845226288\n",
            "train loss:  0.000735909619834274\n",
            "train loss:  0.002246734919026494\n",
            "train loss:  0.03644702956080437\n",
            "train loss:  0.002208475023508072\n",
            "train loss:  0.029834723100066185\n",
            "train loss:  0.051754970103502274\n",
            "train loss:  0.014600412920117378\n",
            "train loss:  0.0524318628013134\n",
            "train loss:  0.00661107525229454\n",
            "train loss:  0.017599672079086304\n",
            "train loss:  0.032038286328315735\n",
            "train loss:  0.12063533067703247\n",
            "train loss:  0.012264104560017586\n",
            "train loss:  0.030492136254906654\n",
            "train loss:  0.007031070534139872\n",
            "train loss:  0.004550053738057613\n",
            "train loss:  0.004740513861179352\n",
            "train loss:  0.007870417088270187\n",
            "train loss:  0.007330490741878748\n",
            "train loss:  0.005583364982157946\n",
            "train loss:  0.00045067371684126556\n",
            "train loss:  0.05541280284523964\n",
            "train loss:  0.00589521462097764\n",
            "train loss:  0.010899693705141544\n",
            "train loss:  0.03592275083065033\n",
            "train loss:  0.03590092062950134\n",
            "train loss:  0.02395176701247692\n",
            "train loss:  0.023166287690401077\n",
            "train loss:  0.0037041197065263987\n",
            "train loss:  0.009276147931814194\n",
            "train loss:  0.022685889154672623\n",
            "train loss:  0.014385628513991833\n",
            "train loss:  0.08520957827568054\n",
            "train loss:  0.04536043107509613\n",
            "train loss:  0.0218408964574337\n",
            "train loss:  0.02929353527724743\n",
            "train loss:  0.0592380091547966\n",
            "train loss:  0.008983601815998554\n",
            "train loss:  0.10286851972341537\n",
            "train loss:  0.02224143035709858\n",
            "train loss:  0.01440610270947218\n",
            "train loss:  0.05918745696544647\n",
            "train loss:  0.0414305254817009\n",
            "train loss:  0.05236729606986046\n",
            "train loss:  0.007823403924703598\n",
            "train loss:  0.002595487516373396\n",
            "train loss:  0.03054504282772541\n",
            "train loss:  0.031460244208574295\n",
            "train loss:  0.00976752769201994\n",
            "train loss:  0.04638257250189781\n",
            "train loss:  0.0057619367726147175\n",
            "train loss:  0.04043035954236984\n",
            "train loss:  0.0505070798099041\n",
            "train loss:  0.05865329131484032\n",
            "train loss:  0.08694454282522202\n",
            "train loss:  0.030939433723688126\n",
            "train loss:  0.004448594059795141\n",
            "train loss:  0.03245856612920761\n",
            "train loss:  0.03519247844815254\n",
            "train loss:  0.000609604234341532\n",
            "train loss:  0.04805469885468483\n",
            "train loss:  0.006687398999929428\n",
            "train loss:  0.007220317609608173\n",
            "train loss:  0.003931393381208181\n",
            "train loss:  0.004088031128048897\n",
            "train loss:  0.07968825846910477\n",
            "train loss:  0.011087491177022457\n",
            "train loss:  0.004217209294438362\n",
            "train loss:  0.0003983881033491343\n",
            "val_loss:  0.09805760532617569\n",
            "Epoch:  7\n",
            "train loss:  0.05777699127793312\n",
            "train loss:  0.018814684823155403\n",
            "train loss:  0.08245853334665298\n",
            "train loss:  0.019031893461942673\n",
            "train loss:  0.032329969108104706\n",
            "train loss:  0.0020376897882670164\n",
            "train loss:  0.025567609816789627\n",
            "train loss:  0.06366435438394547\n",
            "train loss:  0.0020674595143646\n",
            "train loss:  0.007129361853003502\n",
            "train loss:  0.009082530625164509\n",
            "train loss:  0.007567415479570627\n",
            "train loss:  0.015666300430893898\n",
            "train loss:  0.0934566780924797\n",
            "train loss:  0.0068898615427315235\n",
            "train loss:  0.01696254312992096\n",
            "train loss:  0.007815200835466385\n",
            "train loss:  0.018883395940065384\n",
            "train loss:  0.0292062871158123\n",
            "train loss:  0.019029444083571434\n",
            "train loss:  0.00596954021602869\n",
            "train loss:  0.055346377193927765\n",
            "train loss:  0.0377281978726387\n",
            "train loss:  0.04217303544282913\n",
            "train loss:  0.0013552841264754534\n",
            "train loss:  0.00391174852848053\n",
            "train loss:  0.009606121107935905\n",
            "train loss:  0.1005234494805336\n",
            "train loss:  0.00987384282052517\n",
            "train loss:  0.002717683557420969\n",
            "train loss:  0.08518723398447037\n",
            "train loss:  0.005477817263454199\n",
            "train loss:  0.023458413779735565\n",
            "train loss:  0.0016416468424722552\n",
            "train loss:  0.017546283081173897\n",
            "train loss:  0.040754273533821106\n",
            "train loss:  0.0053596762008965015\n",
            "train loss:  0.01036585122346878\n",
            "train loss:  0.13508030772209167\n",
            "train loss:  0.0024603595957159996\n",
            "train loss:  0.02243303321301937\n",
            "train loss:  0.004044017754495144\n",
            "train loss:  0.09072554111480713\n",
            "train loss:  0.006382153369486332\n",
            "train loss:  0.026058951392769814\n",
            "train loss:  0.003097074804827571\n",
            "train loss:  0.04118293523788452\n",
            "train loss:  0.00985032320022583\n",
            "train loss:  0.12398830056190491\n",
            "train loss:  0.016544735059142113\n",
            "train loss:  0.05660311505198479\n",
            "train loss:  0.013201347552239895\n",
            "train loss:  0.009761005640029907\n",
            "train loss:  0.038161780685186386\n",
            "train loss:  0.05746724084019661\n",
            "train loss:  0.01939106173813343\n",
            "train loss:  0.00781888049095869\n",
            "train loss:  0.22282376885414124\n",
            "train loss:  0.03036048449575901\n",
            "train loss:  0.012286622077226639\n",
            "train loss:  0.014329182915389538\n",
            "train loss:  0.006585479713976383\n",
            "train loss:  0.01963687874376774\n",
            "train loss:  0.02152864821255207\n",
            "train loss:  0.043656811118125916\n",
            "train loss:  0.04693792387843132\n",
            "train loss:  0.03350388631224632\n",
            "train loss:  0.004618009552359581\n",
            "train loss:  0.005980330053716898\n",
            "train loss:  0.0031350511126220226\n",
            "train loss:  0.0069117117673158646\n",
            "train loss:  0.052389632910490036\n",
            "train loss:  0.008830411359667778\n",
            "train loss:  0.01050475798547268\n",
            "train loss:  0.00466826930642128\n",
            "train loss:  0.06300662457942963\n",
            "train loss:  0.08499917387962341\n",
            "train loss:  0.007924294099211693\n",
            "train loss:  0.0005905620637349784\n",
            "train loss:  0.03365858271718025\n",
            "train loss:  0.0029804580844938755\n",
            "train loss:  0.0021100782323628664\n",
            "train loss:  0.015943000093102455\n",
            "train loss:  0.08918778598308563\n",
            "train loss:  0.0008935127407312393\n",
            "train loss:  0.022552622482180595\n",
            "train loss:  0.004969263449311256\n",
            "train loss:  0.0183444544672966\n",
            "train loss:  0.027811981737613678\n",
            "train loss:  0.02124999277293682\n",
            "train loss:  0.11181793361902237\n",
            "train loss:  0.025787143036723137\n",
            "train loss:  0.0020311407279223204\n",
            "train loss:  0.0023979065008461475\n",
            "train loss:  0.03793463110923767\n",
            "train loss:  0.07989869266748428\n",
            "train loss:  0.033438168466091156\n",
            "train loss:  0.0037043846677988768\n",
            "train loss:  0.030097059905529022\n",
            "train loss:  0.11095508933067322\n",
            "train loss:  0.058425091207027435\n",
            "train loss:  0.13145560026168823\n",
            "train loss:  0.01607305370271206\n",
            "train loss:  0.04573919251561165\n",
            "train loss:  0.030837126076221466\n",
            "train loss:  0.1154605895280838\n",
            "train loss:  0.001096842810511589\n",
            "train loss:  0.04610651358962059\n",
            "train loss:  0.06575322896242142\n",
            "train loss:  0.12443602830171585\n",
            "train loss:  0.002562452806159854\n",
            "train loss:  0.0023612896911799908\n",
            "train loss:  0.0427335724234581\n",
            "train loss:  0.014953000470995903\n",
            "train loss:  0.04380059242248535\n",
            "train loss:  0.12882119417190552\n",
            "train loss:  0.004527552518993616\n",
            "train loss:  0.08891122043132782\n",
            "train loss:  0.04686528444290161\n",
            "train loss:  0.013705429621040821\n",
            "train loss:  0.012124525383114815\n",
            "train loss:  0.02087220922112465\n",
            "train loss:  0.0054678949527442455\n",
            "train loss:  0.03089761547744274\n",
            "train loss:  0.006878729443997145\n",
            "train loss:  0.007872366346418858\n",
            "train loss:  0.01265374943614006\n",
            "train loss:  0.013059115968644619\n",
            "train loss:  0.01003784965723753\n",
            "train loss:  0.040380291640758514\n",
            "train loss:  0.05950075387954712\n",
            "train loss:  0.008706399239599705\n",
            "train loss:  0.0010708856862038374\n",
            "train loss:  0.017125030979514122\n",
            "train loss:  0.0032158552203327417\n",
            "train loss:  0.009386644698679447\n",
            "train loss:  0.0005912948399782181\n",
            "train loss:  0.006501058116555214\n",
            "train loss:  0.002015879610553384\n",
            "train loss:  0.02173089049756527\n",
            "train loss:  0.002564827911555767\n",
            "train loss:  0.019240010529756546\n",
            "train loss:  0.003673041705042124\n",
            "train loss:  0.029569005593657494\n",
            "train loss:  0.008907460607588291\n",
            "train loss:  0.029586343094706535\n",
            "train loss:  0.007179129868745804\n",
            "train loss:  0.007914665155112743\n",
            "train loss:  0.07413271814584732\n",
            "train loss:  0.054044194519519806\n",
            "train loss:  0.024473611265420914\n",
            "train loss:  0.017367614433169365\n",
            "train loss:  0.0022972347214818\n",
            "train loss:  0.001552390051074326\n",
            "train loss:  0.0005469785537570715\n",
            "train loss:  0.015612060204148293\n",
            "train loss:  0.03335265815258026\n",
            "train loss:  0.0007421542541123927\n",
            "train loss:  0.03956417366862297\n",
            "train loss:  0.015801940113306046\n",
            "train loss:  0.0022762694861739874\n",
            "train loss:  0.017134910449385643\n",
            "train loss:  0.09779733419418335\n",
            "train loss:  0.049806997179985046\n",
            "train loss:  0.009049742482602596\n",
            "train loss:  0.007882448844611645\n",
            "train loss:  0.00541366171091795\n",
            "train loss:  0.0032982106786221266\n",
            "train loss:  0.03150780871510506\n",
            "train loss:  0.01718982309103012\n",
            "train loss:  0.01256183348596096\n",
            "train loss:  0.00036664956132881343\n",
            "train loss:  0.035741791129112244\n",
            "train loss:  0.10333727300167084\n",
            "train loss:  0.007183318957686424\n",
            "train loss:  0.11080630868673325\n",
            "train loss:  0.011768345721065998\n",
            "train loss:  0.042357347905635834\n",
            "train loss:  0.11615049839019775\n",
            "train loss:  0.05266742408275604\n",
            "train loss:  0.019798122346401215\n",
            "train loss:  0.008328301832079887\n",
            "train loss:  0.003418226493522525\n",
            "train loss:  0.013171251863241196\n",
            "train loss:  0.016077984124422073\n",
            "train loss:  0.045594580471515656\n",
            "train loss:  0.06164347380399704\n",
            "train loss:  0.03190954402089119\n",
            "train loss:  0.008822990581393242\n",
            "train loss:  0.015200017020106316\n",
            "train loss:  0.039549827575683594\n",
            "train loss:  0.040624842047691345\n",
            "train loss:  0.01284896582365036\n",
            "train loss:  0.009543591178953648\n",
            "train loss:  0.03160850331187248\n",
            "train loss:  0.011529041454195976\n",
            "train loss:  0.004569992423057556\n",
            "train loss:  0.02307474985718727\n",
            "train loss:  0.004282236099243164\n",
            "train loss:  0.07457663118839264\n",
            "train loss:  0.04821203649044037\n",
            "train loss:  0.045362550765275955\n",
            "train loss:  0.0070459553971886635\n",
            "train loss:  0.027990950271487236\n",
            "train loss:  0.0022550849243998528\n",
            "train loss:  0.09660694748163223\n",
            "train loss:  0.004324616864323616\n",
            "train loss:  0.019444186240434647\n",
            "train loss:  0.00574256107211113\n",
            "train loss:  0.026697907596826553\n",
            "train loss:  0.005550346802920103\n",
            "train loss:  0.003033596556633711\n",
            "train loss:  0.0129406051710248\n",
            "train loss:  0.004457623232156038\n",
            "train loss:  0.01056655403226614\n",
            "train loss:  0.003659397130832076\n",
            "train loss:  0.009765861555933952\n",
            "train loss:  0.010310657322406769\n",
            "train loss:  0.0530407689511776\n",
            "train loss:  0.005709496792405844\n",
            "train loss:  0.008138804696500301\n",
            "train loss:  0.005121718160808086\n",
            "train loss:  0.052935890853405\n",
            "train loss:  0.0628272220492363\n",
            "train loss:  0.0003286796563770622\n",
            "train loss:  0.05330797657370567\n",
            "train loss:  0.001791822724044323\n",
            "train loss:  0.00616964278742671\n",
            "train loss:  0.0035357936285436153\n",
            "train loss:  0.03720766305923462\n",
            "train loss:  0.0005044395220465958\n",
            "train loss:  0.05949390307068825\n",
            "train loss:  0.010482167825102806\n",
            "train loss:  0.013341092504560947\n",
            "train loss:  0.018206067383289337\n",
            "train loss:  0.04331044480204582\n",
            "train loss:  0.01659156009554863\n",
            "train loss:  0.06462152302265167\n",
            "train loss:  0.009609300643205643\n",
            "train loss:  0.04176310449838638\n",
            "train loss:  0.013213747180998325\n",
            "train loss:  0.005231526214629412\n",
            "train loss:  0.006887121591717005\n",
            "train loss:  0.0031463680788874626\n",
            "train loss:  0.0009938543662428856\n",
            "train loss:  0.005463183857500553\n",
            "train loss:  0.11158141493797302\n",
            "train loss:  0.00035586225567385554\n",
            "train loss:  0.029743699356913567\n",
            "train loss:  0.0008988233166746795\n",
            "train loss:  0.009682309813797474\n",
            "train loss:  0.011067932471632957\n",
            "train loss:  0.00796868558973074\n",
            "train loss:  0.0006722832331433892\n",
            "train loss:  0.006887592375278473\n",
            "train loss:  0.0024035018868744373\n",
            "train loss:  0.0043819621205329895\n",
            "train loss:  0.015009813942015171\n",
            "train loss:  0.03309163823723793\n",
            "train loss:  0.026746287941932678\n",
            "train loss:  0.002738683484494686\n",
            "train loss:  0.037208039313554764\n",
            "train loss:  0.08854914456605911\n",
            "train loss:  0.008692437782883644\n",
            "train loss:  0.03877319023013115\n",
            "train loss:  0.009004448540508747\n",
            "train loss:  0.06079486757516861\n",
            "train loss:  0.010425697080790997\n",
            "train loss:  0.02480071224272251\n",
            "train loss:  0.01097143068909645\n",
            "train loss:  0.008817614056169987\n",
            "train loss:  0.005268706940114498\n",
            "train loss:  0.00040354664088226855\n",
            "train loss:  0.08186759799718857\n",
            "train loss:  0.03378443792462349\n",
            "train loss:  0.0012628795811906457\n",
            "train loss:  0.005935520399361849\n",
            "train loss:  0.0036564634647220373\n",
            "train loss:  0.16348573565483093\n",
            "train loss:  0.002017458900809288\n",
            "train loss:  0.018619460985064507\n",
            "train loss:  0.03442759066820145\n",
            "train loss:  0.008568791672587395\n",
            "train loss:  0.02951754629611969\n",
            "train loss:  0.027559014037251472\n",
            "train loss:  0.001262157573364675\n",
            "train loss:  0.009314884431660175\n",
            "train loss:  0.019466958940029144\n",
            "train loss:  0.015592938289046288\n",
            "train loss:  0.06767100840806961\n",
            "train loss:  0.051078297197818756\n",
            "train loss:  0.0026139041874557734\n",
            "train loss:  0.0046493238769471645\n",
            "train loss:  0.06376662850379944\n",
            "train loss:  0.031199803575873375\n",
            "train loss:  0.03272159770131111\n",
            "train loss:  0.045672763139009476\n",
            "train loss:  0.03081377223134041\n",
            "train loss:  0.030123718082904816\n",
            "train loss:  0.048176009207963943\n",
            "train loss:  0.0542885959148407\n",
            "train loss:  0.02030070312321186\n",
            "train loss:  0.06454932689666748\n",
            "train loss:  0.08431272208690643\n",
            "train loss:  0.020249830558896065\n",
            "train loss:  0.0008788792183622718\n",
            "train loss:  0.017220735549926758\n",
            "train loss:  0.00202561030164361\n",
            "train loss:  0.0019467438105493784\n",
            "train loss:  0.03606807440519333\n",
            "train loss:  0.002626236528158188\n",
            "train loss:  0.013386853970587254\n",
            "train loss:  0.0027945959009230137\n",
            "train loss:  0.02383761666715145\n",
            "train loss:  0.012679189443588257\n",
            "train loss:  0.03820822015404701\n",
            "train loss:  0.13481438159942627\n",
            "train loss:  0.0032594650983810425\n",
            "train loss:  0.0030022819992154837\n",
            "train loss:  0.03150695189833641\n",
            "train loss:  0.05181498825550079\n",
            "train loss:  0.049984924495220184\n",
            "train loss:  0.01587645895779133\n",
            "train loss:  0.008850309997797012\n",
            "train loss:  0.0035255667753517628\n",
            "train loss:  0.0033261491917073727\n",
            "train loss:  0.002549167023971677\n",
            "train loss:  0.007514617871493101\n",
            "train loss:  0.088774174451828\n",
            "train loss:  0.015050564892590046\n",
            "train loss:  0.016667833551764488\n",
            "train loss:  0.005814205855131149\n",
            "train loss:  0.004477465525269508\n",
            "train loss:  0.06514297425746918\n",
            "train loss:  0.05191352218389511\n",
            "train loss:  0.05044649541378021\n",
            "train loss:  0.019232064485549927\n",
            "train loss:  0.0045385705307126045\n",
            "train loss:  0.11896298080682755\n",
            "train loss:  0.031103642657399178\n",
            "train loss:  0.0943034365773201\n",
            "train loss:  0.006426364183425903\n",
            "train loss:  0.01296570710837841\n",
            "train loss:  0.002466057427227497\n",
            "train loss:  0.017091378569602966\n",
            "train loss:  0.004466005600988865\n",
            "train loss:  0.003630527062341571\n",
            "train loss:  0.0053368499502539635\n",
            "train loss:  0.046266403049230576\n",
            "train loss:  0.033939529210329056\n",
            "train loss:  0.022508345544338226\n",
            "train loss:  0.021354787051677704\n",
            "train loss:  0.007546456530690193\n",
            "train loss:  0.016220904886722565\n",
            "train loss:  0.1641254723072052\n",
            "train loss:  0.07588565349578857\n",
            "train loss:  0.011064966209232807\n",
            "train loss:  0.021215785294771194\n",
            "train loss:  0.00904997531324625\n",
            "train loss:  0.011207415722310543\n",
            "train loss:  0.0020413824822753668\n",
            "train loss:  0.03036542609333992\n",
            "train loss:  0.0077765947207808495\n",
            "train loss:  0.005592593923211098\n",
            "train loss:  0.027556244283914566\n",
            "train loss:  0.004096260294318199\n",
            "train loss:  0.05456886440515518\n",
            "train loss:  0.0011136495741084218\n",
            "train loss:  0.04073425382375717\n",
            "train loss:  0.004346679430454969\n",
            "train loss:  0.025236887857317924\n",
            "train loss:  0.014772482216358185\n",
            "train loss:  0.03570231795310974\n",
            "train loss:  0.017680983990430832\n",
            "train loss:  0.0011661358876153827\n",
            "train loss:  0.027226179838180542\n",
            "train loss:  0.004537601955235004\n",
            "train loss:  0.0029624125454574823\n",
            "train loss:  0.00530270766466856\n",
            "train loss:  0.07480379194021225\n",
            "train loss:  0.026760131120681763\n",
            "train loss:  0.05254698917269707\n",
            "train loss:  0.003762359730899334\n",
            "train loss:  0.009400338865816593\n",
            "train loss:  0.019158678129315376\n",
            "train loss:  0.025901494547724724\n",
            "train loss:  0.025245426222682\n",
            "train loss:  0.028817886486649513\n",
            "train loss:  0.0012934993719682097\n",
            "train loss:  0.0072727869264781475\n",
            "train loss:  0.10193637758493423\n",
            "train loss:  0.027025796473026276\n",
            "train loss:  0.005331684369593859\n",
            "train loss:  0.012149649672210217\n",
            "train loss:  0.016478383913636208\n",
            "train loss:  0.00010749483772087842\n",
            "train loss:  0.034366436302661896\n",
            "train loss:  0.014857050962746143\n",
            "train loss:  0.018283501267433167\n",
            "train loss:  0.001040852046571672\n",
            "train loss:  0.00045999264693818986\n",
            "train loss:  0.005827764514833689\n",
            "train loss:  0.01479289960116148\n",
            "train loss:  0.013001777231693268\n",
            "train loss:  0.06958173215389252\n",
            "train loss:  0.003209513844922185\n",
            "train loss:  0.054485972970724106\n",
            "train loss:  0.007820605300366879\n",
            "train loss:  0.1063300147652626\n",
            "train loss:  0.07083936780691147\n",
            "train loss:  0.0014473134651780128\n",
            "train loss:  0.005443253554403782\n",
            "train loss:  0.13156579434871674\n",
            "train loss:  0.02751142717897892\n",
            "train loss:  0.029397832229733467\n",
            "train loss:  0.0011963137658312917\n",
            "train loss:  0.024602551013231277\n",
            "train loss:  0.03788255527615547\n",
            "train loss:  0.003486441448330879\n",
            "train loss:  0.006635484751313925\n",
            "train loss:  0.004633093718439341\n",
            "train loss:  0.011946087703108788\n",
            "train loss:  0.002970401430502534\n",
            "train loss:  0.06723883002996445\n",
            "train loss:  0.04407784715294838\n",
            "train loss:  0.13240131735801697\n",
            "train loss:  0.022887809202075005\n",
            "train loss:  0.07208056002855301\n",
            "train loss:  0.0408698245882988\n",
            "train loss:  0.0018946558702737093\n",
            "train loss:  0.06664728373289108\n",
            "train loss:  0.013632206246256828\n",
            "train loss:  0.06376762688159943\n",
            "train loss:  0.028910886496305466\n",
            "train loss:  0.02358449064195156\n",
            "train loss:  0.012772210873663425\n",
            "train loss:  0.027234716340899467\n",
            "train loss:  0.07760965824127197\n",
            "train loss:  0.019523734226822853\n",
            "train loss:  0.0025908611714839935\n",
            "train loss:  0.18239638209342957\n",
            "train loss:  0.06722757965326309\n",
            "train loss:  0.007931530475616455\n",
            "train loss:  0.03885314241051674\n",
            "train loss:  0.007313819136470556\n",
            "train loss:  0.032300662249326706\n",
            "train loss:  0.022907517850399017\n",
            "train loss:  0.002465565223246813\n",
            "train loss:  0.015005695633590221\n",
            "train loss:  0.029693374410271645\n",
            "train loss:  0.003339069662615657\n",
            "train loss:  0.016032889485359192\n",
            "train loss:  0.10665836930274963\n",
            "train loss:  0.0329979732632637\n",
            "train loss:  0.018325475975871086\n",
            "train loss:  0.030742838978767395\n",
            "train loss:  0.06735606491565704\n",
            "train loss:  0.04617654159665108\n",
            "train loss:  0.05182709917426109\n",
            "train loss:  0.03312375769019127\n",
            "train loss:  0.004491116851568222\n",
            "train loss:  0.026945188641548157\n",
            "train loss:  0.025362541899085045\n",
            "train loss:  0.0024591132532805204\n",
            "train loss:  0.0012990039540454745\n",
            "train loss:  0.04234994575381279\n",
            "train loss:  0.02875952050089836\n",
            "train loss:  0.004944371059536934\n",
            "train loss:  0.06097130849957466\n",
            "train loss:  0.04787710681557655\n",
            "train loss:  0.051560014486312866\n",
            "train loss:  0.03096180409193039\n",
            "train loss:  0.006561320275068283\n",
            "train loss:  0.012235436588525772\n",
            "train loss:  0.011725368909537792\n",
            "train loss:  0.016011608764529228\n",
            "train loss:  0.06186719611287117\n",
            "train loss:  0.0129723334684968\n",
            "train loss:  0.00239121587947011\n",
            "train loss:  0.027016960084438324\n",
            "train loss:  0.00672138249501586\n",
            "train loss:  0.017492610961198807\n",
            "train loss:  0.000435099471360445\n",
            "train loss:  0.008253504522144794\n",
            "train loss:  0.0014946855371817946\n",
            "train loss:  0.0006663515232503414\n",
            "train loss:  0.008301900699734688\n",
            "train loss:  0.029820730909705162\n",
            "train loss:  0.0008132505463436246\n",
            "train loss:  0.04160622879862785\n",
            "train loss:  0.03586543723940849\n",
            "train loss:  0.05843471363186836\n",
            "train loss:  0.003489612601697445\n",
            "train loss:  0.0009265709668397903\n",
            "train loss:  0.04390991851687431\n",
            "train loss:  0.007025348953902721\n",
            "train loss:  0.022439876571297646\n",
            "train loss:  0.007501604966819286\n",
            "train loss:  0.020502809435129166\n",
            "train loss:  0.005507131107151508\n",
            "train loss:  0.0167147908359766\n",
            "train loss:  0.043568242341279984\n",
            "train loss:  0.03881734609603882\n",
            "train loss:  0.030849523842334747\n",
            "train loss:  0.043063122779130936\n",
            "train loss:  0.0021806182339787483\n",
            "train loss:  0.07716476917266846\n",
            "train loss:  0.006729701068252325\n",
            "train loss:  0.0005100777489133179\n",
            "train loss:  0.04184214025735855\n",
            "train loss:  0.006686503067612648\n",
            "train loss:  0.022127248346805573\n",
            "train loss:  0.005212758667767048\n",
            "train loss:  0.002018045401200652\n",
            "train loss:  0.027679575607180595\n",
            "train loss:  0.018296657130122185\n",
            "train loss:  0.009864198975265026\n",
            "train loss:  0.15481223165988922\n",
            "train loss:  0.08794406056404114\n",
            "train loss:  0.04307841882109642\n",
            "train loss:  0.02301030606031418\n",
            "train loss:  0.0017675089184194803\n",
            "train loss:  0.00024354405468329787\n",
            "train loss:  0.04780327156186104\n",
            "train loss:  0.04158592224121094\n",
            "train loss:  0.018528323620557785\n",
            "train loss:  6.588861288037151e-05\n",
            "train loss:  0.03501800447702408\n",
            "train loss:  0.159058079123497\n",
            "train loss:  0.0006013872334733605\n",
            "train loss:  0.010482034645974636\n",
            "train loss:  0.023714395239949226\n",
            "train loss:  0.014597428031265736\n",
            "train loss:  0.032875906676054\n",
            "train loss:  0.006748010404407978\n",
            "train loss:  0.00034712188062258065\n",
            "train loss:  0.04122796282172203\n",
            "train loss:  0.013501966372132301\n",
            "train loss:  0.0014286718796938658\n",
            "train loss:  0.048362474888563156\n",
            "train loss:  0.005252184811979532\n",
            "train loss:  0.013899937272071838\n",
            "train loss:  0.003925911616533995\n",
            "train loss:  0.010357783176004887\n",
            "train loss:  0.006957343779504299\n",
            "train loss:  0.050806283950805664\n",
            "train loss:  0.006520944181829691\n",
            "train loss:  0.012148295529186726\n",
            "train loss:  0.005168625619262457\n",
            "train loss:  0.0031310885678976774\n",
            "train loss:  0.015785347670316696\n",
            "train loss:  0.002580697648227215\n",
            "train loss:  0.008453017100691795\n",
            "train loss:  0.02773250639438629\n",
            "train loss:  0.001023094286210835\n",
            "train loss:  0.0703694298863411\n",
            "train loss:  0.053127940744161606\n",
            "train loss:  0.020258165895938873\n",
            "train loss:  0.05663231760263443\n",
            "train loss:  0.0005383433890528977\n",
            "train loss:  0.012910219840705395\n",
            "train loss:  0.0053589362651109695\n",
            "train loss:  0.052245132625103\n",
            "train loss:  0.026304125785827637\n",
            "train loss:  0.0077150678262114525\n",
            "train loss:  0.0032716472633183002\n",
            "train loss:  0.013175763189792633\n",
            "train loss:  0.0007199633400887251\n",
            "train loss:  0.03978012129664421\n",
            "train loss:  0.0027493261732161045\n",
            "train loss:  0.015415376983582973\n",
            "train loss:  0.016538210213184357\n",
            "train loss:  0.0017970798071473837\n",
            "train loss:  0.0008255543070845306\n",
            "train loss:  0.032260484993457794\n",
            "train loss:  0.01934892311692238\n",
            "train loss:  0.006780670955777168\n",
            "train loss:  0.12480242550373077\n",
            "train loss:  0.06742321699857712\n",
            "train loss:  0.0015180829213932157\n",
            "train loss:  0.0532987006008625\n",
            "train loss:  0.01912563480436802\n",
            "train loss:  0.013633177615702152\n",
            "train loss:  0.033462315797805786\n",
            "train loss:  0.005156076978892088\n",
            "train loss:  0.0050328862853348255\n",
            "train loss:  0.01247277483344078\n",
            "train loss:  0.05956687405705452\n",
            "train loss:  0.008979354053735733\n",
            "train loss:  0.0009671402513049543\n",
            "train loss:  0.022219151258468628\n",
            "train loss:  0.02255220338702202\n",
            "train loss:  0.13968680799007416\n",
            "train loss:  0.06446149200201035\n",
            "train loss:  0.011118615977466106\n",
            "train loss:  0.030114132910966873\n",
            "train loss:  0.05628450959920883\n",
            "train loss:  0.02179960161447525\n",
            "train loss:  0.006427269894629717\n",
            "train loss:  0.012728262692689896\n",
            "train loss:  0.0597662553191185\n",
            "train loss:  0.014943482354283333\n",
            "train loss:  0.0037067767698317766\n",
            "train loss:  0.0382695309817791\n",
            "train loss:  0.03734681382775307\n",
            "train loss:  0.047986868768930435\n",
            "train loss:  0.027884067967534065\n",
            "train loss:  0.060866717249155045\n",
            "train loss:  0.011193908751010895\n",
            "train loss:  0.009148635901510715\n",
            "train loss:  0.030377140268683434\n",
            "train loss:  0.0005687900120392442\n",
            "train loss:  0.0016040432965382934\n",
            "train loss:  0.006257601547986269\n",
            "train loss:  0.020970338955521584\n",
            "train loss:  0.033922500908374786\n",
            "train loss:  0.06706573814153671\n",
            "train loss:  0.03641871362924576\n",
            "train loss:  0.013004010543227196\n",
            "train loss:  0.002258769702166319\n",
            "train loss:  0.001275319722481072\n",
            "train loss:  0.011101949028670788\n",
            "train loss:  0.0007652231142856181\n",
            "train loss:  0.050433460623025894\n",
            "train loss:  0.007227844558656216\n",
            "train loss:  0.13866645097732544\n",
            "train loss:  0.00516645610332489\n",
            "train loss:  0.020126812160015106\n",
            "train loss:  0.014806583523750305\n",
            "train loss:  0.03853926062583923\n",
            "train loss:  0.0019300635904073715\n",
            "train loss:  0.027985157445073128\n",
            "train loss:  0.048631902784109116\n",
            "train loss:  0.022111553698778152\n",
            "train loss:  0.037254974246025085\n",
            "train loss:  0.0032067582942545414\n",
            "train loss:  0.006181912496685982\n",
            "train loss:  0.058470409363508224\n",
            "train loss:  0.056969914585351944\n",
            "train loss:  0.00044957082718610764\n",
            "train loss:  0.04043823108077049\n",
            "train loss:  0.02638082765042782\n",
            "train loss:  0.07786354422569275\n",
            "train loss:  0.00515186320990324\n",
            "train loss:  0.0017408806597813964\n",
            "train loss:  0.003753862576559186\n",
            "train loss:  0.030007267370820045\n",
            "train loss:  0.0011540441773831844\n",
            "train loss:  0.005502365995198488\n",
            "train loss:  0.00849144160747528\n",
            "train loss:  0.006474815309047699\n",
            "train loss:  0.003718333085998893\n",
            "train loss:  0.004662046674638987\n",
            "train loss:  0.0018080179579555988\n",
            "train loss:  0.003470195224508643\n",
            "train loss:  0.014382228255271912\n",
            "train loss:  0.0014264154015108943\n",
            "train loss:  0.05962808430194855\n",
            "train loss:  0.006600461434572935\n",
            "train loss:  0.0010087445843964815\n",
            "train loss:  0.001185770845040679\n",
            "train loss:  0.0035241609439253807\n",
            "train loss:  0.003381352173164487\n",
            "train loss:  0.02067238837480545\n",
            "train loss:  0.012999697588384151\n",
            "train loss:  0.0010318453423678875\n",
            "train loss:  0.003285242710262537\n",
            "train loss:  0.05017203837633133\n",
            "train loss:  0.01962292194366455\n",
            "train loss:  0.008080124855041504\n",
            "train loss:  0.022525226697325706\n",
            "train loss:  0.03810951113700867\n",
            "train loss:  0.043050944805145264\n",
            "train loss:  0.018547123298048973\n",
            "train loss:  0.005891073495149612\n",
            "train loss:  0.011352305300533772\n",
            "train loss:  0.0007138492073863745\n",
            "train loss:  0.0002419115335214883\n",
            "train loss:  0.007150998339056969\n",
            "train loss:  0.003142736153677106\n",
            "train loss:  0.00658473139628768\n",
            "train loss:  0.05281378701329231\n",
            "train loss:  0.052874770015478134\n",
            "train loss:  0.006769665516912937\n",
            "train loss:  0.005111244972795248\n",
            "train loss:  0.0041249413043260574\n",
            "train loss:  0.0019285675371065736\n",
            "train loss:  0.05549051612615585\n",
            "train loss:  0.00021754766930826008\n",
            "train loss:  0.002340887673199177\n",
            "train loss:  0.0023195003159344196\n",
            "train loss:  0.13464412093162537\n",
            "train loss:  0.011803068220615387\n",
            "train loss:  0.01735709235072136\n",
            "train loss:  0.0005553370574489236\n",
            "train loss:  0.07066787034273148\n",
            "train loss:  0.008361863903701305\n",
            "train loss:  0.053788382560014725\n",
            "train loss:  0.021177692338824272\n",
            "train loss:  0.051473550498485565\n",
            "train loss:  0.007195490878075361\n",
            "train loss:  0.0755661204457283\n",
            "train loss:  0.0013229348696768284\n",
            "train loss:  0.004526162054389715\n",
            "train loss:  0.0457942970097065\n",
            "train loss:  0.0005360142095014453\n",
            "train loss:  0.024944785982370377\n",
            "train loss:  0.00987265631556511\n",
            "train loss:  0.0022872351109981537\n",
            "train loss:  0.000727435399312526\n",
            "train loss:  0.04511706531047821\n",
            "train loss:  0.008210070431232452\n",
            "train loss:  0.131646528840065\n",
            "train loss:  0.055100761353969574\n",
            "train loss:  0.058898914605379105\n",
            "train loss:  0.019276458770036697\n",
            "train loss:  0.011924340389668941\n",
            "train loss:  0.0031007167417556047\n",
            "train loss:  0.014738457277417183\n",
            "train loss:  0.0004961118102073669\n",
            "train loss:  0.0018584838835522532\n",
            "train loss:  0.01672077178955078\n",
            "train loss:  0.010979529470205307\n",
            "train loss:  0.01960555464029312\n",
            "train loss:  0.010611310601234436\n",
            "train loss:  0.009145736694335938\n",
            "train loss:  0.0774795189499855\n",
            "train loss:  0.20148850977420807\n",
            "train loss:  0.002598908729851246\n",
            "train loss:  0.05957769230008125\n",
            "train loss:  0.05381624028086662\n",
            "train loss:  0.0012001397553831339\n",
            "train loss:  0.008396442048251629\n",
            "train loss:  0.02220703475177288\n",
            "train loss:  0.0070800515823066235\n",
            "train loss:  0.004684333223849535\n",
            "train loss:  0.0015823380090296268\n",
            "train loss:  0.01371651329100132\n",
            "train loss:  0.0076627060770988464\n",
            "train loss:  0.0009495713165961206\n",
            "train loss:  0.0004715900868177414\n",
            "train loss:  0.0016682937275618315\n",
            "train loss:  0.010706635192036629\n",
            "train loss:  0.01693529076874256\n",
            "train loss:  0.008174113929271698\n",
            "train loss:  0.008809257298707962\n",
            "train loss:  0.04677216708660126\n",
            "train loss:  0.03207980468869209\n",
            "train loss:  0.0049295565113425255\n",
            "train loss:  0.0015996001893654466\n",
            "train loss:  0.04674361273646355\n",
            "train loss:  0.003171271411702037\n",
            "train loss:  0.019285323098301888\n",
            "train loss:  0.001493134186603129\n",
            "train loss:  0.003698445623740554\n",
            "train loss:  0.016478106379508972\n",
            "train loss:  0.0005221794708631933\n",
            "train loss:  0.017840025946497917\n",
            "train loss:  0.015101518481969833\n",
            "train loss:  0.005803131964057684\n",
            "train loss:  0.0003690696903504431\n",
            "train loss:  0.03347961977124214\n",
            "train loss:  0.004944166634231806\n",
            "train loss:  0.06792600452899933\n",
            "train loss:  0.047017160803079605\n",
            "train loss:  0.0160978976637125\n",
            "train loss:  0.02782522700726986\n",
            "train loss:  0.0505787618458271\n",
            "train loss:  0.10565367341041565\n",
            "train loss:  0.035763468593358994\n",
            "train loss:  0.006789939943701029\n",
            "train loss:  0.0031530121341347694\n",
            "train loss:  0.020138271152973175\n",
            "train loss:  0.009290214627981186\n",
            "train loss:  0.03223977982997894\n",
            "train loss:  0.030632169917225838\n",
            "train loss:  0.025982463732361794\n",
            "train loss:  0.003844483057036996\n",
            "train loss:  0.007102827541530132\n",
            "train loss:  0.02714245393872261\n",
            "train loss:  0.03861942142248154\n",
            "train loss:  0.10611100494861603\n",
            "train loss:  0.059057850390672684\n",
            "train loss:  0.02711658552289009\n",
            "train loss:  0.014557280577719212\n",
            "train loss:  0.03341469168663025\n",
            "train loss:  0.037882644683122635\n",
            "train loss:  0.0073806303553283215\n",
            "train loss:  0.027173878625035286\n",
            "train loss:  0.011549889110028744\n",
            "train loss:  0.010122928768396378\n",
            "train loss:  0.0021598816383630037\n",
            "train loss:  0.03179968148469925\n",
            "train loss:  0.004642108455300331\n",
            "train loss:  0.0054401070810854435\n",
            "train loss:  0.03014201670885086\n",
            "train loss:  0.050801247358322144\n",
            "train loss:  0.0039970059879124165\n",
            "train loss:  0.008766775950789452\n",
            "train loss:  0.13597646355628967\n",
            "train loss:  0.005437408108264208\n",
            "train loss:  0.0013916508760303259\n",
            "train loss:  0.009803066030144691\n",
            "train loss:  0.018934383988380432\n",
            "train loss:  0.0017401577206328511\n",
            "train loss:  0.023817215114831924\n",
            "train loss:  0.006535216234624386\n",
            "train loss:  0.006810199934989214\n",
            "train loss:  0.02573418617248535\n",
            "train loss:  0.014977043494582176\n",
            "train loss:  0.026649601757526398\n",
            "train loss:  0.00022956301108933985\n",
            "train loss:  0.04574723541736603\n",
            "train loss:  0.0010166651336476207\n",
            "train loss:  0.020170582458376884\n",
            "train loss:  0.08899812400341034\n",
            "train loss:  0.019135858863592148\n",
            "train loss:  0.0033658246975392103\n",
            "train loss:  0.000928405555896461\n",
            "train loss:  0.003039325587451458\n",
            "train loss:  0.013649920001626015\n",
            "train loss:  0.09335590153932571\n",
            "train loss:  0.008185201324522495\n",
            "train loss:  0.07279603183269501\n",
            "train loss:  0.09627372771501541\n",
            "train loss:  0.026110246777534485\n",
            "train loss:  0.0026686214841902256\n",
            "train loss:  0.004051104187965393\n",
            "train loss:  0.017363237217068672\n",
            "train loss:  0.05374477431178093\n",
            "train loss:  0.0014247751096263528\n",
            "train loss:  0.0013208318268880248\n",
            "train loss:  0.03148326277732849\n",
            "train loss:  0.021922772750258446\n",
            "train loss:  0.004581173416227102\n",
            "train loss:  0.09316551685333252\n",
            "train loss:  0.03788134083151817\n",
            "train loss:  0.08391502499580383\n",
            "train loss:  0.06052540987730026\n",
            "train loss:  0.010447302833199501\n",
            "train loss:  0.012031802907586098\n",
            "train loss:  0.006043928675353527\n",
            "train loss:  0.033036522567272186\n",
            "train loss:  0.019818851724267006\n",
            "train loss:  0.0008920822292566299\n",
            "train loss:  0.019144481047987938\n",
            "train loss:  0.013173064216971397\n",
            "train loss:  0.011882281862199306\n",
            "train loss:  0.011334112845361233\n",
            "train loss:  0.03765707090497017\n",
            "train loss:  0.0029527393635362387\n",
            "train loss:  0.025799378752708435\n",
            "train loss:  0.003435986116528511\n",
            "train loss:  0.007105158641934395\n",
            "train loss:  0.017573781311511993\n",
            "train loss:  0.016481386497616768\n",
            "train loss:  0.002387547167018056\n",
            "train loss:  0.004518873989582062\n",
            "train loss:  0.00018782181723508984\n",
            "train loss:  0.005788115784525871\n",
            "val_loss:  0.11975041031837463\n",
            "Epoch:  8\n",
            "train loss:  0.06823784857988358\n",
            "train loss:  0.02925059199333191\n",
            "train loss:  0.006814105901867151\n",
            "train loss:  0.00935476552695036\n",
            "train loss:  0.013245157897472382\n",
            "train loss:  0.0008041642722673714\n",
            "train loss:  0.05916593223810196\n",
            "train loss:  0.00681287469342351\n",
            "train loss:  0.00012891596998088062\n",
            "train loss:  0.02651343122124672\n",
            "train loss:  0.0013667693128809333\n",
            "train loss:  0.008414034731686115\n",
            "train loss:  0.00579279288649559\n",
            "train loss:  0.11160273104906082\n",
            "train loss:  0.014216580428183079\n",
            "train loss:  0.006732753477990627\n",
            "train loss:  0.007368914317339659\n",
            "train loss:  0.10174006223678589\n",
            "train loss:  0.05369498208165169\n",
            "train loss:  0.01991323009133339\n",
            "train loss:  0.007937539368867874\n",
            "train loss:  0.00512727303430438\n",
            "train loss:  0.014041646383702755\n",
            "train loss:  0.005123634822666645\n",
            "train loss:  0.02334759570658207\n",
            "train loss:  0.05627048388123512\n",
            "train loss:  0.015771159902215004\n",
            "train loss:  0.05683338642120361\n",
            "train loss:  0.051388245075941086\n",
            "train loss:  0.0012385117588564754\n",
            "train loss:  0.15091855823993683\n",
            "train loss:  0.013606458902359009\n",
            "train loss:  0.00320024904794991\n",
            "train loss:  0.007902484387159348\n",
            "train loss:  0.10145334154367447\n",
            "train loss:  0.0633394867181778\n",
            "train loss:  0.0022859626915305853\n",
            "train loss:  0.011326015926897526\n",
            "train loss:  0.06468083709478378\n",
            "train loss:  0.0024920087307691574\n",
            "train loss:  0.09192793816328049\n",
            "train loss:  0.03408434987068176\n",
            "train loss:  0.03866956755518913\n",
            "train loss:  0.0015176545130088925\n",
            "train loss:  0.0010171009926125407\n",
            "train loss:  0.0008002577815204859\n",
            "train loss:  0.0017457196954637766\n",
            "train loss:  0.010577050037682056\n",
            "train loss:  0.029486840590834618\n",
            "train loss:  0.015271692536771297\n",
            "train loss:  0.03306276351213455\n",
            "train loss:  0.04725850000977516\n",
            "train loss:  0.027869831770658493\n",
            "train loss:  0.0031170211732387543\n",
            "train loss:  0.008616173639893532\n",
            "train loss:  0.026487745344638824\n",
            "train loss:  0.037783823907375336\n",
            "train loss:  0.05103468522429466\n",
            "train loss:  0.0034254940692335367\n",
            "train loss:  0.06539610773324966\n",
            "train loss:  0.05059874430298805\n",
            "train loss:  0.11200709640979767\n",
            "train loss:  0.0018983149202540517\n",
            "train loss:  0.1725650131702423\n",
            "train loss:  0.014926394447684288\n",
            "train loss:  0.0020976467058062553\n",
            "train loss:  0.04890143498778343\n",
            "train loss:  0.053708966821432114\n",
            "train loss:  0.0005261199548840523\n",
            "train loss:  0.001699198386631906\n",
            "train loss:  0.0040464820340275764\n",
            "train loss:  0.08736945688724518\n",
            "train loss:  0.00834784097969532\n",
            "train loss:  0.04964383319020271\n",
            "train loss:  0.07097010314464569\n",
            "train loss:  0.033253367990255356\n",
            "train loss:  0.03250208497047424\n",
            "train loss:  0.00026275491109117866\n",
            "train loss:  0.0004401328624226153\n",
            "train loss:  0.05485036224126816\n",
            "train loss:  0.0391661636531353\n",
            "train loss:  0.0006076471763662994\n",
            "train loss:  0.0008074503857642412\n",
            "train loss:  0.11899297684431076\n",
            "train loss:  0.004433970432728529\n",
            "train loss:  0.035015229135751724\n",
            "train loss:  0.00175197864882648\n",
            "train loss:  0.004133172333240509\n",
            "train loss:  0.006169736385345459\n",
            "train loss:  0.06158939376473427\n",
            "train loss:  0.017348991706967354\n",
            "train loss:  0.0027562633622437716\n",
            "train loss:  0.0005336458561941981\n",
            "train loss:  0.0029684968758374453\n",
            "train loss:  0.000984801328741014\n",
            "train loss:  0.02730993926525116\n",
            "train loss:  0.10786866396665573\n",
            "train loss:  0.001559408032335341\n",
            "train loss:  0.0158802792429924\n",
            "train loss:  0.08842732012271881\n",
            "train loss:  0.018146976828575134\n",
            "train loss:  0.07853838801383972\n",
            "train loss:  0.012714657001197338\n",
            "train loss:  0.0095470966771245\n",
            "train loss:  0.0015186022501438856\n",
            "train loss:  0.032860301434993744\n",
            "train loss:  0.001862488454207778\n",
            "train loss:  0.029591940343379974\n",
            "train loss:  0.10208293050527573\n",
            "train loss:  0.14063887298107147\n",
            "train loss:  0.010991736315190792\n",
            "train loss:  0.0008861555834300816\n",
            "train loss:  0.04360273852944374\n",
            "train loss:  0.010095535777509212\n",
            "train loss:  0.009537762962281704\n",
            "train loss:  0.021428827196359634\n",
            "train loss:  0.0015116065042093396\n",
            "train loss:  0.023475658148527145\n",
            "train loss:  0.0806223675608635\n",
            "train loss:  0.03188025951385498\n",
            "train loss:  0.058332931250333786\n",
            "train loss:  0.0626770481467247\n",
            "train loss:  0.02731809951364994\n",
            "train loss:  0.005432321690022945\n",
            "train loss:  0.011836768127977848\n",
            "train loss:  0.006819333415478468\n",
            "train loss:  0.02001991868019104\n",
            "train loss:  0.0007658738177269697\n",
            "train loss:  0.0385403037071228\n",
            "train loss:  0.06302311271429062\n",
            "train loss:  0.02663481794297695\n",
            "train loss:  0.005686688702553511\n",
            "train loss:  0.0020114530343562365\n",
            "train loss:  0.015053853392601013\n",
            "train loss:  0.01199987344443798\n",
            "train loss:  0.06458441913127899\n",
            "train loss:  0.0014543452998623252\n",
            "train loss:  0.004347209352999926\n",
            "train loss:  0.0068149324506521225\n",
            "train loss:  0.16833940148353577\n",
            "train loss:  0.0019237924134358764\n",
            "train loss:  0.006909296847879887\n",
            "train loss:  0.04900256544351578\n",
            "train loss:  0.024967744946479797\n",
            "train loss:  0.0008418594370596111\n",
            "train loss:  0.008617512881755829\n",
            "train loss:  0.0429094061255455\n",
            "train loss:  0.0014519061660394073\n",
            "train loss:  0.030417578294873238\n",
            "train loss:  0.015972232446074486\n",
            "train loss:  0.0006956287543289363\n",
            "train loss:  0.07479772716760635\n",
            "train loss:  0.09944029897451401\n",
            "train loss:  0.0030800311360508204\n",
            "train loss:  0.012556014582514763\n",
            "train loss:  0.03672287240624428\n",
            "train loss:  0.11445079743862152\n",
            "train loss:  0.01175785344094038\n",
            "train loss:  0.03672106936573982\n",
            "train loss:  0.015163023956120014\n",
            "train loss:  0.001892338041216135\n",
            "train loss:  0.040891263633966446\n",
            "train loss:  0.07212217897176743\n",
            "train loss:  0.005552841350436211\n",
            "train loss:  0.037388067692518234\n",
            "train loss:  0.007237061858177185\n",
            "train loss:  0.008688870817422867\n",
            "train loss:  0.012633324600756168\n",
            "train loss:  0.04918484389781952\n",
            "train loss:  0.010929173789918423\n",
            "train loss:  0.04967793822288513\n",
            "train loss:  0.0006131772533990443\n",
            "train loss:  0.025022180750966072\n",
            "train loss:  0.048358991742134094\n",
            "train loss:  0.01549743302166462\n",
            "train loss:  0.03065544366836548\n",
            "train loss:  0.016698617488145828\n",
            "train loss:  0.07483895123004913\n",
            "train loss:  0.02424602396786213\n",
            "train loss:  0.03131943941116333\n",
            "train loss:  0.03855891525745392\n",
            "train loss:  0.014457517303526402\n",
            "train loss:  0.0035391245037317276\n",
            "train loss:  0.007236998528242111\n",
            "train loss:  0.05071709305047989\n",
            "train loss:  0.004887365270406008\n",
            "train loss:  0.023396411910653114\n",
            "train loss:  0.04853396117687225\n",
            "train loss:  0.010637602768838406\n",
            "train loss:  0.004840944427996874\n",
            "train loss:  0.04122462496161461\n",
            "train loss:  0.010590811260044575\n",
            "train loss:  0.013583342544734478\n",
            "train loss:  0.0027873748913407326\n",
            "train loss:  0.011057799682021141\n",
            "train loss:  0.05411424860358238\n",
            "train loss:  0.0027369491290301085\n",
            "train loss:  0.06326897442340851\n",
            "train loss:  0.03454233333468437\n",
            "train loss:  0.0019574305042624474\n",
            "train loss:  0.002070413436740637\n",
            "train loss:  0.02545243687927723\n",
            "train loss:  0.11385403573513031\n",
            "train loss:  0.0029607219621539116\n",
            "train loss:  0.015170660801231861\n",
            "train loss:  0.029262876138091087\n",
            "train loss:  0.0006460827426053584\n",
            "train loss:  0.06834414601325989\n",
            "train loss:  0.007775808684527874\n",
            "train loss:  0.023471759632229805\n",
            "train loss:  0.008471216075122356\n",
            "train loss:  0.024702420458197594\n",
            "train loss:  0.0005499277613125741\n",
            "train loss:  0.07696635276079178\n",
            "train loss:  0.015941910445690155\n",
            "train loss:  0.0009396986570209265\n",
            "train loss:  0.00722977239638567\n",
            "train loss:  0.0018645792733877897\n",
            "train loss:  0.003711467143148184\n",
            "train loss:  0.0017417364288121462\n",
            "train loss:  0.013605596497654915\n",
            "train loss:  0.07935140281915665\n",
            "train loss:  0.03885342925786972\n",
            "train loss:  0.007103191688656807\n",
            "train loss:  0.00116284703835845\n",
            "train loss:  0.032514508813619614\n",
            "train loss:  0.017262142151594162\n",
            "train loss:  0.005290278699249029\n",
            "train loss:  0.003377096727490425\n",
            "train loss:  0.02401764504611492\n",
            "train loss:  0.05055677145719528\n",
            "train loss:  0.015127258375287056\n",
            "train loss:  0.04191184416413307\n",
            "train loss:  0.032939910888671875\n",
            "train loss:  0.08879756182432175\n",
            "train loss:  0.03771038353443146\n",
            "train loss:  0.031086871400475502\n",
            "train loss:  0.057719603180885315\n",
            "train loss:  0.056018274277448654\n",
            "train loss:  0.08146316558122635\n",
            "train loss:  0.00600037444382906\n",
            "train loss:  0.012632939033210278\n",
            "train loss:  0.006613831035792828\n",
            "train loss:  0.029351990669965744\n",
            "train loss:  0.010291555896401405\n",
            "train loss:  0.1052948608994484\n",
            "train loss:  0.13512325286865234\n",
            "train loss:  0.0005931086488999426\n",
            "train loss:  0.03533027321100235\n",
            "train loss:  0.00361437164247036\n",
            "train loss:  0.0068945130333304405\n",
            "train loss:  0.013811018317937851\n",
            "train loss:  0.007706587202847004\n",
            "train loss:  0.008421161212027073\n",
            "train loss:  0.0007493048324249685\n",
            "train loss:  0.01087233703583479\n",
            "train loss:  0.02909175679087639\n",
            "train loss:  0.05234745144844055\n",
            "train loss:  0.004205918870866299\n",
            "train loss:  0.01454770378768444\n",
            "train loss:  0.016689665615558624\n",
            "train loss:  0.057413119822740555\n",
            "train loss:  0.00868306215852499\n",
            "train loss:  0.0006900669541209936\n",
            "train loss:  0.06385745853185654\n",
            "train loss:  0.001888529397547245\n",
            "train loss:  0.02344045415520668\n",
            "train loss:  0.0447075329720974\n",
            "train loss:  0.01489781029522419\n",
            "train loss:  0.021903764456510544\n",
            "train loss:  0.009976298548281193\n",
            "train loss:  0.011332855559885502\n",
            "train loss:  0.008222592063248158\n",
            "train loss:  0.030903028324246407\n",
            "train loss:  0.0665830597281456\n",
            "train loss:  0.005880251992493868\n",
            "train loss:  0.04668284207582474\n",
            "train loss:  0.017506876960396767\n",
            "train loss:  0.003639006521552801\n",
            "train loss:  0.006408405490219593\n",
            "train loss:  0.07970993965864182\n",
            "train loss:  0.0038249779026955366\n",
            "train loss:  0.017342766746878624\n",
            "train loss:  0.03915956988930702\n",
            "train loss:  0.022874008864164352\n",
            "train loss:  0.04518573358654976\n",
            "train loss:  0.020576493814587593\n",
            "train loss:  0.004372718743979931\n",
            "train loss:  0.0016218848759308457\n",
            "train loss:  0.003100711852312088\n",
            "train loss:  0.022255485877394676\n",
            "train loss:  0.004345159512013197\n",
            "train loss:  0.017038829624652863\n",
            "train loss:  0.015861472114920616\n",
            "train loss:  0.026740774512290955\n",
            "train loss:  0.02276897244155407\n",
            "train loss:  0.003622241783887148\n",
            "train loss:  0.0408816933631897\n",
            "train loss:  0.10256466269493103\n",
            "train loss:  0.019222049042582512\n",
            "train loss:  0.06171318516135216\n",
            "train loss:  0.007715737447142601\n",
            "train loss:  0.0033962319139391184\n",
            "train loss:  0.010545937344431877\n",
            "train loss:  0.010365355759859085\n",
            "train loss:  0.002079068450257182\n",
            "train loss:  0.011448793113231659\n",
            "train loss:  0.0013858788879588246\n",
            "train loss:  0.0023714876733720303\n",
            "train loss:  0.0002528358018025756\n",
            "train loss:  0.007365009281784296\n",
            "train loss:  0.005385900381952524\n",
            "train loss:  0.0008121643913909793\n",
            "train loss:  0.018011221662163734\n",
            "train loss:  0.007489751558750868\n",
            "train loss:  0.002280956134200096\n",
            "train loss:  0.00527223339304328\n",
            "train loss:  0.018852878361940384\n",
            "train loss:  0.0045759836211800575\n",
            "train loss:  0.019017620012164116\n",
            "train loss:  0.027386531233787537\n",
            "train loss:  0.022324027493596077\n",
            "train loss:  0.018573280423879623\n",
            "train loss:  0.025116102769970894\n",
            "train loss:  0.029185036197304726\n",
            "train loss:  0.02110159397125244\n",
            "train loss:  0.0005043915589340031\n",
            "train loss:  0.05159901827573776\n",
            "train loss:  0.0005687036318704486\n",
            "train loss:  0.0006525659118779004\n",
            "train loss:  0.04531211033463478\n",
            "train loss:  0.00287489197216928\n",
            "train loss:  0.0005984523450024426\n",
            "train loss:  0.010919168591499329\n",
            "train loss:  0.019625840708613396\n",
            "train loss:  0.014594892039895058\n",
            "train loss:  0.0029173935763537884\n",
            "train loss:  0.00808583665639162\n",
            "train loss:  0.014549752697348595\n",
            "train loss:  0.11316753178834915\n",
            "train loss:  0.038986142724752426\n",
            "train loss:  0.01690029352903366\n",
            "train loss:  0.009908106178045273\n",
            "train loss:  0.003812219016253948\n",
            "train loss:  0.024072367697954178\n",
            "train loss:  0.02661481499671936\n",
            "train loss:  0.047176145017147064\n",
            "train loss:  0.0029755064751952887\n",
            "train loss:  0.036263056099414825\n",
            "train loss:  0.12168996036052704\n",
            "train loss:  0.008536268956959248\n",
            "train loss:  0.008754382841289043\n",
            "train loss:  0.032596319913864136\n",
            "train loss:  0.00419923709705472\n",
            "train loss:  0.1676454097032547\n",
            "train loss:  0.011524183675646782\n",
            "train loss:  0.02436509169638157\n",
            "train loss:  0.004082437139004469\n",
            "train loss:  0.04593290388584137\n",
            "train loss:  0.002200486371293664\n",
            "train loss:  0.011407344602048397\n",
            "train loss:  0.027807706966996193\n",
            "train loss:  0.03752625361084938\n",
            "train loss:  0.05709115043282509\n",
            "train loss:  0.0023682175669819117\n",
            "train loss:  0.0062387241050601006\n",
            "train loss:  0.008675538003444672\n",
            "train loss:  0.0019583210814744234\n",
            "train loss:  0.00014800917415414006\n",
            "train loss:  0.0008107798057608306\n",
            "train loss:  0.0010726551990956068\n",
            "train loss:  0.01786782778799534\n",
            "train loss:  0.021370822563767433\n",
            "train loss:  0.029937533661723137\n",
            "train loss:  0.00438729440793395\n",
            "train loss:  0.0017597430851310492\n",
            "train loss:  0.024017764255404472\n",
            "train loss:  0.0025973033625632524\n",
            "train loss:  0.0070801242254674435\n",
            "train loss:  0.007857737131416798\n",
            "train loss:  0.002769129117950797\n",
            "train loss:  0.05011672526597977\n",
            "train loss:  0.029536563903093338\n",
            "train loss:  0.02837299183011055\n",
            "train loss:  0.00023696324205957353\n",
            "train loss:  0.01848192699253559\n",
            "train loss:  0.020584870129823685\n",
            "train loss:  0.01565103605389595\n",
            "train loss:  0.011179229244589806\n",
            "train loss:  0.05237320438027382\n",
            "train loss:  0.004494804888963699\n",
            "train loss:  0.0961720272898674\n",
            "train loss:  0.0018745039124041796\n",
            "train loss:  0.015758078545331955\n",
            "train loss:  0.004282335750758648\n",
            "train loss:  0.004774080589413643\n",
            "train loss:  0.006312929093837738\n",
            "train loss:  0.0013659197138622403\n",
            "train loss:  0.04010036960244179\n",
            "train loss:  0.13100045919418335\n",
            "train loss:  0.00033711237483657897\n",
            "train loss:  0.009556645527482033\n",
            "train loss:  0.005032242275774479\n",
            "train loss:  0.010274560190737247\n",
            "train loss:  0.018075160682201385\n",
            "train loss:  0.014835660345852375\n",
            "train loss:  0.018713541328907013\n",
            "train loss:  0.008189365267753601\n",
            "train loss:  0.0007925488171167672\n",
            "train loss:  0.017927421256899834\n",
            "train loss:  0.001936496701091528\n",
            "train loss:  0.004419613629579544\n",
            "train loss:  0.018346531316637993\n",
            "train loss:  0.001271630171686411\n",
            "train loss:  0.03403056040406227\n",
            "train loss:  0.0011188546195626259\n",
            "train loss:  0.021145517006516457\n",
            "train loss:  0.014371657744050026\n",
            "train loss:  0.013231651857495308\n",
            "train loss:  0.001039260532706976\n",
            "train loss:  0.0009789258474484086\n",
            "train loss:  0.03138332813978195\n",
            "train loss:  0.025219863280653954\n",
            "train loss:  0.0011974069057032466\n",
            "train loss:  0.03565521910786629\n",
            "train loss:  0.10507671535015106\n",
            "train loss:  0.06418617069721222\n",
            "train loss:  0.0063513764180243015\n",
            "train loss:  0.02882387302815914\n",
            "train loss:  0.02793966419994831\n",
            "train loss:  0.13727328181266785\n",
            "train loss:  0.011071604676544666\n",
            "train loss:  0.0021210135892033577\n",
            "train loss:  0.010761355981230736\n",
            "train loss:  0.008971002884209156\n",
            "train loss:  0.058459486812353134\n",
            "train loss:  0.021368278190493584\n",
            "train loss:  0.0015512225218117237\n",
            "train loss:  0.0007130549638532102\n",
            "train loss:  0.07346510887145996\n",
            "train loss:  0.0005131394718773663\n",
            "train loss:  0.003984377253800631\n",
            "train loss:  0.0019668254535645247\n",
            "train loss:  0.020903468132019043\n",
            "train loss:  0.0007556224009022117\n",
            "train loss:  0.006842578295618296\n",
            "train loss:  0.04134969785809517\n",
            "train loss:  0.010361218824982643\n",
            "train loss:  0.04446377605199814\n",
            "train loss:  0.029288118705153465\n",
            "train loss:  0.01667354628443718\n",
            "train loss:  0.0021245917305350304\n",
            "train loss:  0.102858766913414\n",
            "train loss:  0.035860754549503326\n",
            "train loss:  0.02464119903743267\n",
            "train loss:  0.07888546586036682\n",
            "train loss:  0.04708231985569\n",
            "train loss:  0.003113522194325924\n",
            "train loss:  0.017082255333662033\n",
            "train loss:  0.002551558893173933\n",
            "train loss:  0.0015121817123144865\n",
            "train loss:  0.004044946748763323\n",
            "train loss:  0.012184602208435535\n",
            "train loss:  0.06284274905920029\n",
            "train loss:  0.012758973054587841\n",
            "train loss:  0.004087152425199747\n",
            "train loss:  0.04600224643945694\n",
            "train loss:  0.005098579451441765\n",
            "train loss:  0.09957708418369293\n",
            "train loss:  0.06129668653011322\n",
            "train loss:  0.12783189117908478\n",
            "train loss:  0.021359004080295563\n",
            "train loss:  0.0021943366155028343\n",
            "train loss:  0.048217497766017914\n",
            "train loss:  0.0010056054452434182\n",
            "train loss:  0.07928146421909332\n",
            "train loss:  0.00032689564977772534\n",
            "train loss:  0.003714259248226881\n",
            "train loss:  0.0008212360553443432\n",
            "train loss:  0.0023386836983263493\n",
            "train loss:  0.0033058919943869114\n",
            "train loss:  0.025519922375679016\n",
            "train loss:  0.02500472590327263\n",
            "train loss:  0.007988186553120613\n",
            "train loss:  0.013334863819181919\n",
            "train loss:  0.0002202914038207382\n",
            "train loss:  0.06061515957117081\n",
            "train loss:  0.1394132673740387\n",
            "train loss:  0.021009916439652443\n",
            "train loss:  0.0026697050780057907\n",
            "train loss:  0.0090975072234869\n",
            "train loss:  0.003323246957734227\n",
            "train loss:  0.03324976935982704\n",
            "train loss:  0.0006070903036743402\n",
            "train loss:  0.04624088853597641\n",
            "train loss:  0.0010091594886034727\n",
            "train loss:  0.031135834753513336\n",
            "train loss:  0.02621477283537388\n",
            "train loss:  0.0398748442530632\n",
            "train loss:  0.03493918478488922\n",
            "train loss:  0.009091012179851532\n",
            "train loss:  0.0011304671643301845\n",
            "train loss:  0.0012978713493794203\n",
            "train loss:  0.016172245144844055\n",
            "train loss:  0.025171585381031036\n",
            "train loss:  0.001326981233432889\n",
            "train loss:  0.018437407910823822\n",
            "train loss:  0.00813104398548603\n",
            "train loss:  0.00660494901239872\n",
            "train loss:  0.1323927491903305\n",
            "train loss:  0.025812216103076935\n",
            "train loss:  0.03739972040057182\n",
            "train loss:  0.007565999403595924\n",
            "train loss:  0.08331024646759033\n",
            "train loss:  0.007704576011747122\n",
            "train loss:  0.0022611997555941343\n",
            "train loss:  0.00038389948895201087\n",
            "train loss:  0.017708968371152878\n",
            "train loss:  0.0034657693468034267\n",
            "train loss:  0.021978851407766342\n",
            "train loss:  0.005898012779653072\n",
            "train loss:  0.01474884245544672\n",
            "train loss:  0.03315411135554314\n",
            "train loss:  0.01133356336504221\n",
            "train loss:  0.002800510497763753\n",
            "train loss:  0.04142944514751434\n",
            "train loss:  0.001216743839904666\n",
            "train loss:  0.0312846340239048\n",
            "train loss:  0.1349220871925354\n",
            "train loss:  0.009168709628283978\n",
            "train loss:  0.028055671602487564\n",
            "train loss:  0.003832525573670864\n",
            "train loss:  0.03155246004462242\n",
            "train loss:  0.0007419734611175954\n",
            "train loss:  0.00016877286543603987\n",
            "train loss:  0.019822193309664726\n",
            "train loss:  0.06791958212852478\n",
            "train loss:  0.04054303094744682\n",
            "train loss:  0.0007485920796170831\n",
            "train loss:  0.022185711190104485\n",
            "train loss:  0.0015368377789855003\n",
            "train loss:  0.007219202350825071\n",
            "train loss:  0.00046257389476522803\n",
            "train loss:  0.0408550500869751\n",
            "train loss:  0.009053398855030537\n",
            "train loss:  0.12867037951946259\n",
            "train loss:  0.0031880135647952557\n",
            "train loss:  0.0030563361942768097\n",
            "train loss:  0.0005534057854674757\n",
            "train loss:  0.021792598068714142\n",
            "train loss:  0.0034077840391546488\n",
            "train loss:  0.021767187863588333\n",
            "train loss:  0.004142292309552431\n",
            "train loss:  0.022508779540657997\n",
            "train loss:  0.003609053324908018\n",
            "train loss:  0.01978173293173313\n",
            "train loss:  0.0008895974024198949\n",
            "train loss:  0.03285236284136772\n",
            "train loss:  0.034965965896844864\n",
            "train loss:  0.0007526116096414626\n",
            "train loss:  0.0023491885513067245\n",
            "train loss:  0.0036060414277017117\n",
            "train loss:  0.006231212522834539\n",
            "train loss:  0.01335461251437664\n",
            "train loss:  0.00325557473115623\n",
            "train loss:  0.005237030331045389\n",
            "train loss:  0.002182380296289921\n",
            "train loss:  0.00012888565834145993\n",
            "train loss:  0.017968105152249336\n",
            "train loss:  0.05832075700163841\n",
            "train loss:  0.006938502192497253\n",
            "train loss:  0.003888805629685521\n",
            "train loss:  0.014563022181391716\n",
            "train loss:  0.02619500271975994\n",
            "train loss:  0.012023025192320347\n",
            "train loss:  0.015116357244551182\n",
            "train loss:  0.07000789046287537\n",
            "train loss:  0.0012353892670944333\n",
            "train loss:  0.011075297370553017\n",
            "train loss:  0.019885728135704994\n",
            "train loss:  0.06631216406822205\n",
            "train loss:  0.014192251488566399\n",
            "train loss:  0.001613066648133099\n",
            "train loss:  0.04224269092082977\n",
            "train loss:  0.011412782594561577\n",
            "train loss:  0.003959115594625473\n",
            "train loss:  0.007490146439522505\n",
            "train loss:  0.12604357302188873\n",
            "train loss:  0.0016409228555858135\n",
            "train loss:  0.0014347605174407363\n",
            "train loss:  0.0004560227971524\n",
            "train loss:  0.04273834452033043\n",
            "train loss:  0.004302267916500568\n",
            "train loss:  0.015339188277721405\n",
            "train loss:  0.023725755512714386\n",
            "train loss:  0.03157668933272362\n",
            "train loss:  0.01630646549165249\n",
            "train loss:  0.0619298592209816\n",
            "train loss:  0.005884186830371618\n",
            "train loss:  0.003126113209873438\n",
            "train loss:  0.04197811707854271\n",
            "train loss:  0.000842905486933887\n",
            "train loss:  0.007447886746376753\n",
            "train loss:  0.0020479108206927776\n",
            "train loss:  0.004969337489455938\n",
            "train loss:  0.007250606082379818\n",
            "train loss:  0.0005618361756205559\n",
            "train loss:  0.013853752985596657\n",
            "train loss:  0.013164480216801167\n",
            "train loss:  0.0008704598294571042\n",
            "train loss:  0.0013067846884950995\n",
            "train loss:  0.000670898356474936\n",
            "train loss:  0.07176210731267929\n",
            "train loss:  0.005833769217133522\n",
            "train loss:  0.040549591183662415\n",
            "train loss:  0.004596160724759102\n",
            "train loss:  0.048669539391994476\n",
            "train loss:  0.013437937945127487\n",
            "train loss:  0.026392236351966858\n",
            "train loss:  0.0010687519097700715\n",
            "train loss:  0.0004928308771923184\n",
            "train loss:  0.0029875540640205145\n",
            "train loss:  0.0006056959391571581\n",
            "train loss:  0.06231237202882767\n",
            "train loss:  0.021497737616300583\n",
            "train loss:  0.023454606533050537\n",
            "train loss:  0.05064988508820534\n",
            "train loss:  0.030277801677584648\n",
            "train loss:  0.0077706375159323215\n",
            "train loss:  0.018799131736159325\n",
            "train loss:  0.018708501011133194\n",
            "train loss:  0.00889101903885603\n",
            "train loss:  0.014711561612784863\n",
            "train loss:  0.053433388471603394\n",
            "train loss:  0.004085379187017679\n",
            "train loss:  0.0001797336881281808\n",
            "train loss:  0.02703496441245079\n",
            "train loss:  0.004386646207422018\n",
            "train loss:  0.022578654810786247\n",
            "train loss:  0.0013117544585838914\n",
            "train loss:  0.008263831958174706\n",
            "train loss:  0.03304312378168106\n",
            "train loss:  0.007771402131766081\n",
            "train loss:  0.003314684145152569\n",
            "train loss:  0.0020627439953386784\n",
            "train loss:  0.0015611236449331045\n",
            "train loss:  0.0195296797901392\n",
            "train loss:  0.009014705196022987\n",
            "train loss:  0.015065844170749187\n",
            "train loss:  0.00535990297794342\n",
            "train loss:  0.001983926398679614\n",
            "train loss:  0.09753657877445221\n",
            "train loss:  0.034132104367017746\n",
            "train loss:  0.014753692783415318\n",
            "train loss:  0.05089981481432915\n",
            "train loss:  0.009707298129796982\n",
            "train loss:  0.002774181542918086\n",
            "train loss:  0.009738673456013203\n",
            "train loss:  0.005645379889756441\n",
            "train loss:  0.06757044792175293\n",
            "train loss:  0.0008208468789234757\n",
            "train loss:  0.0011664140038192272\n",
            "train loss:  0.0011227328795939684\n",
            "train loss:  0.004880281165242195\n",
            "train loss:  0.013487418182194233\n",
            "train loss:  0.005363565403968096\n",
            "train loss:  0.011227997951209545\n",
            "train loss:  0.0020663009490817785\n",
            "train loss:  0.014701982028782368\n",
            "train loss:  0.001855387818068266\n",
            "train loss:  0.08306171745061874\n",
            "train loss:  0.015933511778712273\n",
            "train loss:  0.030651431530714035\n",
            "train loss:  0.021164054051041603\n",
            "train loss:  0.00012676621554419398\n",
            "train loss:  0.04274370148777962\n",
            "train loss:  0.000559871201403439\n",
            "train loss:  0.008646706119179726\n",
            "train loss:  0.21158240735530853\n",
            "train loss:  0.000736648216843605\n",
            "train loss:  0.0174730084836483\n",
            "train loss:  0.011816155165433884\n",
            "train loss:  0.005485374480485916\n",
            "train loss:  0.0007003367645666003\n",
            "train loss:  0.004412401933223009\n",
            "train loss:  0.06292865425348282\n",
            "train loss:  0.03506002202630043\n",
            "train loss:  0.10285036265850067\n",
            "train loss:  0.00046639019274152815\n",
            "train loss:  0.0008147162734530866\n",
            "train loss:  0.012366172857582569\n",
            "train loss:  0.06024046614766121\n",
            "train loss:  0.019373074173927307\n",
            "train loss:  0.007979679852724075\n",
            "train loss:  0.0014087206218391657\n",
            "train loss:  0.009660893119871616\n",
            "train loss:  0.0014628085773438215\n",
            "train loss:  0.04401563107967377\n",
            "train loss:  0.010696069337427616\n",
            "train loss:  0.0058506326749920845\n",
            "train loss:  0.000838861393276602\n",
            "train loss:  0.016467854380607605\n",
            "train loss:  0.028498677536845207\n",
            "train loss:  5.452858385979198e-05\n",
            "train loss:  0.0010215502697974443\n",
            "train loss:  0.010150272399187088\n",
            "train loss:  0.04643213748931885\n",
            "train loss:  0.009988348931074142\n",
            "train loss:  0.014501825906336308\n",
            "train loss:  0.00024564447812736034\n",
            "train loss:  0.004776469431817532\n",
            "train loss:  0.031708553433418274\n",
            "train loss:  0.02569442242383957\n",
            "train loss:  0.015341129153966904\n",
            "train loss:  0.0011396727059036493\n",
            "train loss:  0.0009098267182707787\n",
            "train loss:  0.07606804370880127\n",
            "train loss:  0.004177364055067301\n",
            "train loss:  0.04595518484711647\n",
            "train loss:  0.000547790143173188\n",
            "train loss:  0.0005582515150308609\n",
            "train loss:  0.023977035656571388\n",
            "train loss:  0.025085875764489174\n",
            "train loss:  0.0027114145923405886\n",
            "train loss:  0.03786049783229828\n",
            "train loss:  6.910806405358016e-05\n",
            "train loss:  0.007852228358387947\n",
            "train loss:  0.0032760680187493563\n",
            "train loss:  0.0030555319972336292\n",
            "train loss:  0.06495015323162079\n",
            "train loss:  0.0010348135838285089\n",
            "train loss:  0.00021284185640979558\n",
            "train loss:  0.016995234414935112\n",
            "train loss:  0.0011914590140804648\n",
            "train loss:  0.008505141362547874\n",
            "train loss:  0.004697740077972412\n",
            "train loss:  0.0005907188169658184\n",
            "train loss:  0.010337051004171371\n",
            "train loss:  0.0006807228783145547\n",
            "train loss:  5.957141183898784e-05\n",
            "train loss:  0.007239419035613537\n",
            "train loss:  0.005275198258459568\n",
            "train loss:  0.1679629683494568\n",
            "train loss:  0.07456730306148529\n",
            "train loss:  0.0063780006021261215\n",
            "train loss:  0.00933082029223442\n",
            "train loss:  0.016108576208353043\n",
            "train loss:  0.003930918872356415\n",
            "train loss:  0.004592456854879856\n",
            "train loss:  0.0015170193510130048\n",
            "train loss:  0.06403496116399765\n",
            "train loss:  0.06043127179145813\n",
            "train loss:  0.0021513947285711765\n",
            "train loss:  0.01021579746156931\n",
            "train loss:  0.02026819996535778\n",
            "train loss:  0.07124245911836624\n",
            "train loss:  0.007109541911631823\n",
            "train loss:  0.037993330508470535\n",
            "train loss:  0.038757093250751495\n",
            "train loss:  0.002553938888013363\n",
            "train loss:  0.02496306225657463\n",
            "train loss:  0.0013778003631159663\n",
            "train loss:  0.0036488205660134554\n",
            "train loss:  0.11836445331573486\n",
            "train loss:  0.00861233752220869\n",
            "train loss:  0.05865633860230446\n",
            "train loss:  0.011734671890735626\n",
            "train loss:  0.03652603179216385\n",
            "train loss:  0.038191232830286026\n",
            "train loss:  0.054270997643470764\n",
            "train loss:  0.0004552589962258935\n",
            "train loss:  0.013475636951625347\n",
            "train loss:  0.027396349236369133\n",
            "train loss:  0.011795196682214737\n",
            "train loss:  0.03282458707690239\n",
            "train loss:  0.007021389435976744\n",
            "train loss:  0.036880336701869965\n",
            "train loss:  0.014087377116084099\n",
            "train loss:  0.002070716116577387\n",
            "train loss:  0.025447262451052666\n",
            "train loss:  0.06456393003463745\n",
            "train loss:  0.056774310767650604\n",
            "train loss:  0.05098572000861168\n",
            "train loss:  0.03619999438524246\n",
            "train loss:  0.005837397649884224\n",
            "train loss:  0.0022053869906812906\n",
            "train loss:  0.005079436115920544\n",
            "train loss:  0.040531087666749954\n",
            "train loss:  0.02281777933239937\n",
            "train loss:  0.10454997420310974\n",
            "train loss:  0.004978071432560682\n",
            "train loss:  0.00013726022734772414\n",
            "train loss:  0.0009127952507697046\n",
            "train loss:  0.0016496791504323483\n",
            "train loss:  0.00030516565311700106\n",
            "train loss:  0.04828654229640961\n",
            "train loss:  0.0013176880311220884\n",
            "train loss:  0.0012297144858166575\n",
            "train loss:  0.0922161415219307\n",
            "train loss:  0.009593808092176914\n",
            "train loss:  0.045178402215242386\n",
            "train loss:  0.16063959896564484\n",
            "train loss:  0.020392607897520065\n",
            "train loss:  0.04649874567985535\n",
            "train loss:  0.016663119196891785\n",
            "train loss:  0.006358780432492495\n",
            "train loss:  0.02502703107893467\n",
            "train loss:  0.0012265542754903436\n",
            "train loss:  0.005553914699703455\n",
            "train loss:  0.023916810750961304\n",
            "train loss:  0.06516337394714355\n",
            "train loss:  9.655103349359706e-05\n",
            "train loss:  0.0003556181909516454\n",
            "train loss:  0.00013083021622151136\n",
            "train loss:  0.004081701394170523\n",
            "train loss:  0.004409452900290489\n",
            "train loss:  0.0006366181187331676\n",
            "train loss:  0.0012838656548410654\n",
            "train loss:  0.0009242671076208353\n",
            "train loss:  0.003658376168459654\n",
            "train loss:  0.00961481686681509\n",
            "train loss:  0.11074144393205643\n",
            "train loss:  0.003625334706157446\n",
            "train loss:  0.008605086244642735\n",
            "train loss:  0.0031378320418298244\n",
            "train loss:  0.04400837793946266\n",
            "train loss:  0.002773283515125513\n",
            "train loss:  0.023352589458227158\n",
            "train loss:  0.012407749891281128\n",
            "train loss:  0.037825021892786026\n",
            "train loss:  0.007605516351759434\n",
            "train loss:  0.0012365260627120733\n",
            "train loss:  0.03730461001396179\n",
            "train loss:  0.0031227809377014637\n",
            "train loss:  0.06039689853787422\n",
            "train loss:  0.08407936245203018\n",
            "train loss:  0.001038472168147564\n",
            "train loss:  0.0047239200212061405\n",
            "train loss:  0.03511566296219826\n",
            "train loss:  0.0010492729488760233\n",
            "train loss:  0.0005863234400749207\n",
            "train loss:  0.007387201767414808\n",
            "train loss:  0.000680097786244005\n",
            "train loss:  0.0045516216196119785\n",
            "train loss:  0.03629397600889206\n",
            "train loss:  0.007605619728565216\n",
            "train loss:  0.01894953101873398\n",
            "train loss:  0.0021833304781466722\n",
            "train loss:  0.028376711532473564\n",
            "train loss:  0.01824021153151989\n",
            "train loss:  0.009289788082242012\n",
            "train loss:  0.009862741455435753\n",
            "train loss:  0.010308940894901752\n",
            "train loss:  0.0013061630306765437\n",
            "train loss:  0.04671015962958336\n",
            "train loss:  0.010698015801608562\n",
            "train loss:  0.0018538435688242316\n",
            "train loss:  0.0012270023580640554\n",
            "train loss:  0.00017375062452629209\n",
            "train loss:  0.0005238231969997287\n",
            "val_loss:  0.10820920020341873\n",
            "Epoch:  9\n",
            "train loss:  0.020143523812294006\n",
            "train loss:  0.1822548359632492\n",
            "train loss:  0.007156328298151493\n",
            "train loss:  0.00714245717972517\n",
            "train loss:  0.012353206984698772\n",
            "train loss:  0.028956374153494835\n",
            "train loss:  0.017560379579663277\n",
            "train loss:  0.009476582519710064\n",
            "train loss:  0.0009371400810778141\n",
            "train loss:  0.00015107427316252142\n",
            "train loss:  0.05636489763855934\n",
            "train loss:  0.0015718169743195176\n",
            "train loss:  0.0021233537700027227\n",
            "train loss:  0.0017106676241382957\n",
            "train loss:  0.0037926749791949987\n",
            "train loss:  0.01074066013097763\n",
            "train loss:  0.07595399022102356\n",
            "train loss:  0.01056254655122757\n",
            "train loss:  0.01860441267490387\n",
            "train loss:  0.1147894337773323\n",
            "train loss:  0.024573490023612976\n",
            "train loss:  0.01438644528388977\n",
            "train loss:  0.025926174595952034\n",
            "train loss:  0.005378609988838434\n",
            "train loss:  0.02713412046432495\n",
            "train loss:  0.03766275942325592\n",
            "train loss:  0.0022494946606457233\n",
            "train loss:  0.05661604180932045\n",
            "train loss:  0.04946263134479523\n",
            "train loss:  0.016852866858243942\n",
            "train loss:  0.0010013130959123373\n",
            "train loss:  0.03037022240459919\n",
            "train loss:  0.004298390820622444\n",
            "train loss:  0.021019523963332176\n",
            "train loss:  0.0012245593825355172\n",
            "train loss:  0.010839148424565792\n",
            "train loss:  0.008221606723964214\n",
            "train loss:  0.021956803277134895\n",
            "train loss:  0.17493043839931488\n",
            "train loss:  0.015884600579738617\n",
            "train loss:  0.006487575825303793\n",
            "train loss:  0.0035095885396003723\n",
            "train loss:  0.00867189560085535\n",
            "train loss:  0.049959953874349594\n",
            "train loss:  0.0018881484866142273\n",
            "train loss:  0.0017562363063916564\n",
            "train loss:  0.01571042649447918\n",
            "train loss:  0.009309693239629269\n",
            "train loss:  0.00741732120513916\n",
            "train loss:  0.009918730705976486\n",
            "train loss:  0.015443255193531513\n",
            "train loss:  0.007618242409080267\n",
            "train loss:  0.03502335399389267\n",
            "train loss:  0.02876393124461174\n",
            "train loss:  0.0025793141685426235\n",
            "train loss:  0.013205180875957012\n",
            "train loss:  0.007120051421225071\n",
            "train loss:  0.03189634904265404\n",
            "train loss:  0.0525427870452404\n",
            "train loss:  0.07296057045459747\n",
            "train loss:  0.010490529239177704\n",
            "train loss:  0.008350126445293427\n",
            "train loss:  0.005865067709237337\n",
            "train loss:  0.004487447906285524\n",
            "train loss:  0.05627775564789772\n",
            "train loss:  0.006239089649170637\n",
            "train loss:  0.008971977978944778\n",
            "train loss:  0.005137331783771515\n",
            "train loss:  0.031427301466464996\n",
            "train loss:  0.0012962252367287874\n",
            "train loss:  0.0018979450687766075\n",
            "train loss:  0.021275952458381653\n",
            "train loss:  0.00558995408937335\n",
            "train loss:  0.03847210481762886\n",
            "train loss:  0.055349741131067276\n",
            "train loss:  0.009166200645267963\n",
            "train loss:  0.0638490617275238\n",
            "train loss:  0.010114372707903385\n",
            "train loss:  0.0006048661307431757\n",
            "train loss:  0.03586549311876297\n",
            "train loss:  0.006225195713341236\n",
            "train loss:  0.0007824755739420652\n",
            "train loss:  0.0014331802958622575\n",
            "train loss:  0.005662336014211178\n",
            "train loss:  0.008537674322724342\n",
            "train loss:  0.06303789466619492\n",
            "train loss:  0.034189291298389435\n",
            "train loss:  2.331563700863626e-05\n",
            "train loss:  0.061685483902692795\n",
            "train loss:  0.034868769347667694\n",
            "train loss:  0.011332517489790916\n",
            "train loss:  0.004482049494981766\n",
            "train loss:  0.001110055367462337\n",
            "train loss:  0.010856231674551964\n",
            "train loss:  0.0015274460893124342\n",
            "train loss:  0.037761490792036057\n",
            "train loss:  0.10579587519168854\n",
            "train loss:  0.0019844917114824057\n",
            "train loss:  0.0522867888212204\n",
            "train loss:  0.1003975048661232\n",
            "train loss:  0.08647509664297104\n",
            "train loss:  0.05423789843916893\n",
            "train loss:  0.026475558057427406\n",
            "train loss:  0.002211658051237464\n",
            "train loss:  0.0030922472942620516\n",
            "train loss:  0.024070262908935547\n",
            "train loss:  0.0005443333648145199\n",
            "train loss:  0.014640020206570625\n",
            "train loss:  0.10769212245941162\n",
            "train loss:  0.025219976902008057\n",
            "train loss:  0.0020542307756841183\n",
            "train loss:  0.00904539693146944\n",
            "train loss:  0.0013595906784757972\n",
            "train loss:  0.009985459968447685\n",
            "train loss:  0.014509785920381546\n",
            "train loss:  0.02206697128713131\n",
            "train loss:  0.0029878916684538126\n",
            "train loss:  0.011384768411517143\n",
            "train loss:  0.10940958559513092\n",
            "train loss:  0.005340698175132275\n",
            "train loss:  0.019894663244485855\n",
            "train loss:  0.025449976325035095\n",
            "train loss:  0.026969438418745995\n",
            "train loss:  0.0801365002989769\n",
            "train loss:  0.0018850681371986866\n",
            "train loss:  0.0054746815003454685\n",
            "train loss:  0.019185908138751984\n",
            "train loss:  0.000464507844299078\n",
            "train loss:  0.030013732612133026\n",
            "train loss:  0.01335326861590147\n",
            "train loss:  0.0037841880694031715\n",
            "train loss:  0.023650195449590683\n",
            "train loss:  0.012943893671035767\n",
            "train loss:  0.00386587786488235\n",
            "train loss:  0.00030438395333476365\n",
            "train loss:  0.05562260001897812\n",
            "train loss:  0.0008711196132935584\n",
            "train loss:  0.00781715102493763\n",
            "train loss:  0.01301947794854641\n",
            "train loss:  0.11242248117923737\n",
            "train loss:  0.0023809534031897783\n",
            "train loss:  0.01498606987297535\n",
            "train loss:  0.01139308512210846\n",
            "train loss:  0.03317387029528618\n",
            "train loss:  0.0005210415692999959\n",
            "train loss:  0.02021346241235733\n",
            "train loss:  0.011335794813930988\n",
            "train loss:  0.012013765051960945\n",
            "train loss:  0.012280409224331379\n",
            "train loss:  0.01754841022193432\n",
            "train loss:  0.0024133131373673677\n",
            "train loss:  0.0037751432973891497\n",
            "train loss:  0.008178021758794785\n",
            "train loss:  0.0012366881128400564\n",
            "train loss:  0.0030488206539303064\n",
            "train loss:  0.008990462869405746\n",
            "train loss:  0.023092180490493774\n",
            "train loss:  0.013798024505376816\n",
            "train loss:  0.05581328272819519\n",
            "train loss:  0.003882402554154396\n",
            "train loss:  0.000906059518456459\n",
            "train loss:  0.0038027558475732803\n",
            "train loss:  0.025747235864400864\n",
            "train loss:  0.0030481689609587193\n",
            "train loss:  0.007069080136716366\n",
            "train loss:  0.014651801437139511\n",
            "train loss:  0.03001282922923565\n",
            "train loss:  0.0031526857055723667\n",
            "train loss:  0.07897166162729263\n",
            "train loss:  0.000799422909040004\n",
            "train loss:  0.009377293288707733\n",
            "train loss:  0.00017708682571537793\n",
            "train loss:  0.16133157908916473\n",
            "train loss:  0.025179436430335045\n",
            "train loss:  0.06518913805484772\n",
            "train loss:  0.0028586904518306255\n",
            "train loss:  0.008590091951191425\n",
            "train loss:  0.008902441710233688\n",
            "train loss:  0.01693299226462841\n",
            "train loss:  0.007087326142936945\n",
            "train loss:  0.06659062951803207\n",
            "train loss:  0.0026763034984469414\n",
            "train loss:  0.0019694918300956488\n",
            "train loss:  0.0030747444834560156\n",
            "train loss:  0.018728628754615784\n",
            "train loss:  0.06610403209924698\n",
            "train loss:  0.029231693595647812\n",
            "train loss:  0.14580038189888\n",
            "train loss:  0.02461065910756588\n",
            "train loss:  0.0038167608436197042\n",
            "train loss:  0.004115686751902103\n",
            "train loss:  0.028706027194857597\n",
            "train loss:  0.001468112226575613\n",
            "train loss:  0.09731757640838623\n",
            "train loss:  0.011756880208849907\n",
            "train loss:  0.17391963303089142\n",
            "train loss:  0.0018905742326751351\n",
            "train loss:  0.04054344817996025\n",
            "train loss:  0.0013429021928459406\n",
            "train loss:  0.011072651483118534\n",
            "train loss:  0.005007980391383171\n",
            "train loss:  0.0001632146304473281\n",
            "train loss:  0.041617266833782196\n",
            "train loss:  0.008750936016440392\n",
            "train loss:  0.011531303636729717\n",
            "train loss:  0.0884462296962738\n",
            "train loss:  0.0027121854946017265\n",
            "train loss:  0.030608516186475754\n",
            "train loss:  0.019708190113306046\n",
            "train loss:  0.025068432092666626\n",
            "train loss:  0.008774319663643837\n",
            "train loss:  0.162433922290802\n",
            "train loss:  0.00026009391876868904\n",
            "train loss:  0.0037628754507750273\n",
            "train loss:  0.0006675873883068562\n",
            "train loss:  0.008885075338184834\n",
            "train loss:  0.1134311705827713\n",
            "train loss:  0.01453893817961216\n",
            "train loss:  0.08882491290569305\n",
            "train loss:  0.00017468395526520908\n",
            "train loss:  0.0012168973917141557\n",
            "train loss:  0.02854263223707676\n",
            "train loss:  0.00681248027831316\n",
            "train loss:  0.009929098188877106\n",
            "train loss:  0.0006519575254060328\n",
            "train loss:  0.013204812072217464\n",
            "train loss:  0.0007777773425914347\n",
            "train loss:  0.10674722492694855\n",
            "train loss:  0.011020587757229805\n",
            "train loss:  0.013266870751976967\n",
            "train loss:  0.032625921070575714\n",
            "train loss:  0.0044017075560987\n",
            "train loss:  0.10690584033727646\n",
            "train loss:  0.04147200286388397\n",
            "train loss:  0.02474275417625904\n",
            "train loss:  0.03138662874698639\n",
            "train loss:  0.002888283459469676\n",
            "train loss:  0.061444252729415894\n",
            "train loss:  0.008512320928275585\n",
            "train loss:  0.09812597185373306\n",
            "train loss:  0.014007861725986004\n",
            "train loss:  0.008221328258514404\n",
            "train loss:  0.04705241322517395\n",
            "train loss:  0.027295006439089775\n",
            "train loss:  0.0002570094948168844\n",
            "train loss:  0.14768823981285095\n",
            "train loss:  0.13050007820129395\n",
            "train loss:  0.0014161855215206742\n",
            "train loss:  0.0634048655629158\n",
            "train loss:  0.021040018647909164\n",
            "train loss:  0.025722699239850044\n",
            "train loss:  0.018292391672730446\n",
            "train loss:  0.03922424092888832\n",
            "train loss:  0.021004699170589447\n",
            "train loss:  0.07356806844472885\n",
            "train loss:  0.024062344804406166\n",
            "train loss:  0.03809502720832825\n",
            "train loss:  0.05318067967891693\n",
            "train loss:  0.0038579681422561407\n",
            "train loss:  0.031347572803497314\n",
            "train loss:  0.006751887500286102\n",
            "train loss:  0.018074076622724533\n",
            "train loss:  0.02683139219880104\n",
            "train loss:  0.0008705196669325233\n",
            "train loss:  0.015645645558834076\n",
            "train loss:  0.0011594658717513084\n",
            "train loss:  0.10443469882011414\n",
            "train loss:  0.008337447419762611\n",
            "train loss:  0.007225217297673225\n",
            "train loss:  0.033034518361091614\n",
            "train loss:  0.00258740340359509\n",
            "train loss:  0.010136371478438377\n",
            "train loss:  0.0021378344390541315\n",
            "train loss:  0.0013151322491467\n",
            "train loss:  0.0046922871842980385\n",
            "train loss:  0.010697663761675358\n",
            "train loss:  0.01716863177716732\n",
            "train loss:  0.005229962058365345\n",
            "train loss:  0.01127957459539175\n",
            "train loss:  0.00042737837065942585\n",
            "train loss:  0.001254601520486176\n",
            "train loss:  0.0017060708487406373\n",
            "train loss:  0.021486807614564896\n",
            "train loss:  0.007823293097317219\n",
            "train loss:  0.010630150325596333\n",
            "train loss:  0.00786619633436203\n",
            "train loss:  0.0008710856200195849\n",
            "train loss:  0.01734081283211708\n",
            "train loss:  0.03771394491195679\n",
            "train loss:  0.001374611398205161\n",
            "train loss:  0.004100143443793058\n",
            "train loss:  0.0007140797679312527\n",
            "train loss:  0.0807037502527237\n",
            "train loss:  0.0026933394838124514\n",
            "train loss:  0.0009447187185287476\n",
            "train loss:  0.04111828655004501\n",
            "train loss:  0.03705960139632225\n",
            "train loss:  0.011544161476194859\n",
            "train loss:  0.048426754772663116\n",
            "train loss:  0.003247486427426338\n",
            "train loss:  0.035903796553611755\n",
            "train loss:  0.03584175556898117\n",
            "train loss:  0.025433078408241272\n",
            "train loss:  0.09458881616592407\n",
            "train loss:  0.001917536254040897\n",
            "train loss:  0.0010876728920266032\n",
            "train loss:  0.0023130138870328665\n",
            "train loss:  0.012511503882706165\n",
            "train loss:  0.014542286284267902\n",
            "train loss:  0.003933550324290991\n",
            "train loss:  0.0013751646038144827\n",
            "train loss:  0.07894929498434067\n",
            "train loss:  0.00036630535032600164\n",
            "train loss:  0.001330341910943389\n",
            "train loss:  0.034115396440029144\n",
            "train loss:  0.006661055143922567\n",
            "train loss:  0.028902003541588783\n",
            "train loss:  0.001070120488293469\n",
            "train loss:  0.018315616995096207\n",
            "train loss:  0.006311847362667322\n",
            "train loss:  0.010016651824116707\n",
            "train loss:  0.02085709758102894\n",
            "train loss:  0.002066689310595393\n",
            "train loss:  0.005765365902334452\n",
            "train loss:  0.0004697300319094211\n",
            "train loss:  0.007996617816388607\n",
            "train loss:  0.016488121822476387\n",
            "train loss:  0.062499359250068665\n",
            "train loss:  0.0009535614517517388\n",
            "train loss:  0.012298133224248886\n",
            "train loss:  0.01445853989571333\n",
            "train loss:  0.0312611386179924\n",
            "train loss:  0.0019812562968581915\n",
            "train loss:  0.000706261198502034\n",
            "train loss:  0.00124489632435143\n",
            "train loss:  0.06369917094707489\n",
            "train loss:  0.006158111151307821\n",
            "train loss:  0.002998761599883437\n",
            "train loss:  0.04334697127342224\n",
            "train loss:  0.006156308110803366\n",
            "train loss:  0.00957780983299017\n",
            "train loss:  0.011083053424954414\n",
            "train loss:  0.012485572136938572\n",
            "train loss:  0.00568904634565115\n",
            "train loss:  0.0009175643208436668\n",
            "train loss:  0.0006435714312829077\n",
            "train loss:  0.014000024646520615\n",
            "train loss:  0.0027878861874341965\n",
            "train loss:  0.0007125831907615066\n",
            "train loss:  0.059631653130054474\n",
            "train loss:  0.03543150797486305\n",
            "train loss:  0.0283955205231905\n",
            "train loss:  0.000573554600123316\n",
            "train loss:  0.01985360123217106\n",
            "train loss:  0.2271796315908432\n",
            "train loss:  0.07088688015937805\n",
            "train loss:  0.0038472774904221296\n",
            "train loss:  0.0013640859397128224\n",
            "train loss:  0.0007720077992416918\n",
            "train loss:  0.07614125311374664\n",
            "train loss:  0.032995786517858505\n",
            "train loss:  0.010469222441315651\n",
            "train loss:  0.004605816677212715\n",
            "train loss:  0.001092248479835689\n",
            "train loss:  0.0013638779055327177\n",
            "train loss:  0.006565514486283064\n",
            "train loss:  0.010990705341100693\n",
            "train loss:  0.00018152329721488059\n",
            "train loss:  0.014206924475729465\n",
            "train loss:  0.005794055759906769\n",
            "train loss:  0.03682415187358856\n",
            "train loss:  0.01918061450123787\n",
            "train loss:  0.002370912814512849\n",
            "train loss:  0.032490335404872894\n",
            "train loss:  0.001490277238190174\n",
            "train loss:  0.0020238389261066914\n",
            "train loss:  0.0010363500332459807\n",
            "train loss:  0.0029492154717445374\n",
            "train loss:  0.01375976949930191\n",
            "train loss:  0.03621577471494675\n",
            "train loss:  0.027731038630008698\n",
            "train loss:  0.026874935254454613\n",
            "train loss:  0.008142175152897835\n",
            "train loss:  0.007394720800220966\n",
            "train loss:  0.0006005617906339467\n",
            "train loss:  0.030360670760273933\n",
            "train loss:  0.0352608785033226\n",
            "train loss:  0.001899793278425932\n",
            "train loss:  0.008277509361505508\n",
            "train loss:  0.0022976170293986797\n",
            "train loss:  0.013682297430932522\n",
            "train loss:  0.07044878602027893\n",
            "train loss:  0.0030703614465892315\n",
            "train loss:  0.13429571688175201\n",
            "train loss:  0.0011341359931975603\n",
            "train loss:  0.00017621720326133072\n",
            "train loss:  0.03423735499382019\n",
            "train loss:  0.006134370807558298\n",
            "train loss:  0.00508024450391531\n",
            "train loss:  0.00205726595595479\n",
            "train loss:  0.0006805455195717514\n",
            "train loss:  0.06735808402299881\n",
            "train loss:  0.012638667598366737\n",
            "train loss:  0.017341868951916695\n",
            "train loss:  0.027160534635186195\n",
            "train loss:  0.003630258608609438\n",
            "train loss:  0.001395795727148652\n",
            "train loss:  0.05807650834321976\n",
            "train loss:  0.018316101282835007\n",
            "train loss:  0.04926551878452301\n",
            "train loss:  0.0002497982932254672\n",
            "train loss:  0.0039155869744718075\n",
            "train loss:  0.045493438839912415\n",
            "train loss:  0.0023352850694209337\n",
            "train loss:  0.006720146629959345\n",
            "train loss:  0.0002269260148750618\n",
            "train loss:  0.06331168860197067\n",
            "train loss:  0.0032727981451898813\n",
            "train loss:  0.0009565244545228779\n",
            "train loss:  0.008500692434608936\n",
            "train loss:  0.001507669105194509\n",
            "train loss:  0.026190074160695076\n",
            "train loss:  0.0002460697724018246\n",
            "train loss:  0.004783805925399065\n",
            "train loss:  0.029988184571266174\n",
            "train loss:  0.034222230315208435\n",
            "train loss:  0.015143385156989098\n",
            "train loss:  0.010191851295530796\n",
            "train loss:  0.005940937902778387\n",
            "train loss:  0.000490620790515095\n",
            "train loss:  0.08704469352960587\n",
            "train loss:  0.002782694064080715\n",
            "train loss:  0.012048924341797829\n",
            "train loss:  0.005756508558988571\n",
            "train loss:  0.045788899064064026\n",
            "train loss:  0.0026301080361008644\n",
            "train loss:  0.0010477943578734994\n",
            "train loss:  0.0030228979885578156\n",
            "train loss:  0.0017974497750401497\n",
            "train loss:  0.0008165548788383603\n",
            "train loss:  0.01117624156177044\n",
            "train loss:  0.009883238933980465\n",
            "train loss:  0.009690867736935616\n",
            "train loss:  0.0004220110713504255\n",
            "train loss:  0.015846068039536476\n",
            "train loss:  0.0064553567208349705\n",
            "train loss:  0.0021736188791692257\n",
            "train loss:  0.00028240817482583225\n",
            "train loss:  0.0165950246155262\n",
            "train loss:  0.020742999389767647\n",
            "train loss:  0.0006116398726589978\n",
            "train loss:  0.0019829024095088243\n",
            "train loss:  0.020683234557509422\n",
            "train loss:  0.0019522933289408684\n",
            "train loss:  0.0018883657176047564\n",
            "train loss:  0.0046570501290261745\n",
            "train loss:  0.003497540717944503\n",
            "train loss:  0.017647739499807358\n",
            "train loss:  0.023389942944049835\n",
            "train loss:  0.0005494490033015609\n",
            "train loss:  0.005338070448487997\n",
            "train loss:  0.010042628273367882\n",
            "train loss:  0.016374362632632256\n",
            "train loss:  0.00021236686734482646\n",
            "train loss:  0.03622586280107498\n",
            "train loss:  0.011456253938376904\n",
            "train loss:  0.056250449270009995\n",
            "train loss:  0.0007267771288752556\n",
            "train loss:  0.008273166604340076\n",
            "train loss:  0.003091487567871809\n",
            "train loss:  0.0020989032927900553\n",
            "train loss:  0.0065553211607038975\n",
            "train loss:  0.004192355088889599\n",
            "train loss:  0.01393832452595234\n",
            "train loss:  0.00579212699085474\n",
            "train loss:  0.004060903564095497\n",
            "train loss:  0.11159545183181763\n",
            "train loss:  0.000329966947901994\n",
            "train loss:  0.021241914480924606\n",
            "train loss:  0.003445255570113659\n",
            "train loss:  0.0004048301780130714\n",
            "train loss:  0.035800427198410034\n",
            "train loss:  0.0004065302200615406\n",
            "train loss:  0.004318853374570608\n",
            "train loss:  0.02674553170800209\n",
            "train loss:  0.032235607504844666\n",
            "train loss:  0.012704342603683472\n",
            "train loss:  0.0024253849405795336\n",
            "train loss:  0.07332461327314377\n",
            "train loss:  0.04057519882917404\n",
            "train loss:  0.05741019919514656\n",
            "train loss:  0.0094059519469738\n",
            "train loss:  0.07038155198097229\n",
            "train loss:  0.023775532841682434\n",
            "train loss:  0.06351564079523087\n",
            "train loss:  0.0014527481980621815\n",
            "train loss:  0.12077069282531738\n",
            "train loss:  0.008308752439916134\n",
            "train loss:  0.02160719782114029\n",
            "train loss:  0.009227526374161243\n",
            "train loss:  0.011970380321145058\n",
            "train loss:  0.0015003555454313755\n",
            "train loss:  0.038804274052381516\n",
            "train loss:  0.08784030377864838\n",
            "train loss:  0.0018894628155976534\n",
            "train loss:  0.0011857581557705998\n",
            "train loss:  0.06258989125490189\n",
            "train loss:  0.004019188228994608\n",
            "train loss:  0.0007267058826982975\n",
            "train loss:  0.002280493965372443\n",
            "train loss:  0.010099845938384533\n",
            "train loss:  0.04261345788836479\n",
            "train loss:  0.0034508109092712402\n",
            "train loss:  0.0011370660504326224\n",
            "train loss:  0.035590387880802155\n",
            "train loss:  0.007490892894566059\n",
            "train loss:  0.0006501239258795977\n",
            "train loss:  0.011862127110362053\n",
            "train loss:  0.043728843331336975\n",
            "train loss:  0.007818645797669888\n",
            "train loss:  0.007357288617640734\n",
            "train loss:  0.015799419954419136\n",
            "train loss:  0.00427637156099081\n",
            "train loss:  0.12840045988559723\n",
            "train loss:  0.0075352005660533905\n",
            "train loss:  0.023962493985891342\n",
            "train loss:  0.005311279557645321\n",
            "train loss:  0.021882439032197\n",
            "train loss:  0.06661833822727203\n",
            "train loss:  0.0011269159149378538\n",
            "train loss:  0.016641099005937576\n",
            "train loss:  0.007553044706583023\n",
            "train loss:  0.006431440357118845\n",
            "train loss:  0.01743188127875328\n",
            "train loss:  0.00012480211444199085\n",
            "train loss:  0.002016963204368949\n",
            "train loss:  0.05489226058125496\n",
            "train loss:  0.022234082221984863\n",
            "train loss:  0.04952939227223396\n",
            "train loss:  0.03762083500623703\n",
            "train loss:  9.477300773141906e-05\n",
            "train loss:  0.04377114027738571\n",
            "train loss:  0.008016485720872879\n",
            "train loss:  0.056394193321466446\n",
            "train loss:  0.011363216675817966\n",
            "train loss:  0.00755207147449255\n",
            "train loss:  0.053482528775930405\n",
            "train loss:  0.0012625143863260746\n",
            "train loss:  0.05706231668591499\n",
            "train loss:  0.005956526380032301\n",
            "train loss:  0.07325070351362228\n",
            "train loss:  0.029009457677602768\n",
            "train loss:  0.06013038009405136\n",
            "train loss:  0.0012154178693890572\n",
            "train loss:  0.0029887056443840265\n",
            "train loss:  0.05726597458124161\n",
            "train loss:  0.005214171018451452\n",
            "train loss:  0.00454369094222784\n",
            "train loss:  0.0012965814676135778\n",
            "train loss:  0.0011663795448839664\n",
            "train loss:  0.01812606118619442\n",
            "train loss:  0.023752523586153984\n",
            "train loss:  0.024297093972563744\n",
            "train loss:  0.004506611730903387\n",
            "train loss:  0.02034168876707554\n",
            "train loss:  0.004175497684627771\n",
            "train loss:  0.0071207815781235695\n",
            "train loss:  0.07703157514333725\n",
            "train loss:  0.02098122239112854\n",
            "train loss:  0.006145970430225134\n",
            "train loss:  0.01801525615155697\n",
            "train loss:  0.01077481359243393\n",
            "train loss:  0.009380865842103958\n",
            "train loss:  0.042283255606889725\n",
            "train loss:  0.007851427420973778\n",
            "train loss:  0.013949379324913025\n",
            "train loss:  0.004481667652726173\n",
            "train loss:  0.00431660795584321\n",
            "train loss:  0.015909790992736816\n",
            "train loss:  0.005948110017925501\n",
            "train loss:  0.006778494454920292\n",
            "train loss:  0.0011019381927326322\n",
            "train loss:  0.018007829785346985\n",
            "train loss:  0.0022101604845374823\n",
            "train loss:  0.0009352568886242807\n",
            "train loss:  0.0008197628776542842\n",
            "train loss:  0.017768485471606255\n",
            "train loss:  0.038679372519254684\n",
            "train loss:  0.009167604148387909\n",
            "train loss:  0.00034846848575398326\n",
            "train loss:  0.001018244307488203\n",
            "train loss:  0.003755783662199974\n",
            "train loss:  0.026919957250356674\n",
            "train loss:  0.02049274556338787\n",
            "train loss:  0.009931594133377075\n",
            "train loss:  0.0021327706053853035\n",
            "train loss:  0.009010419249534607\n",
            "train loss:  0.024188823997974396\n",
            "train loss:  0.0038567332085222006\n",
            "train loss:  0.0020509562455117702\n",
            "train loss:  0.007988186553120613\n",
            "train loss:  0.017034994438290596\n",
            "train loss:  0.006941140629351139\n",
            "train loss:  0.005102646071463823\n",
            "train loss:  0.06212818995118141\n",
            "train loss:  0.014332850463688374\n",
            "train loss:  0.0052076890133321285\n",
            "train loss:  0.06708169728517532\n",
            "train loss:  0.0009486355702392757\n",
            "train loss:  0.003566756844520569\n",
            "train loss:  0.0003245075640734285\n",
            "train loss:  5.55395454284735e-05\n",
            "train loss:  0.03379233181476593\n",
            "train loss:  0.0009179658954963088\n",
            "train loss:  0.001595154288224876\n",
            "train loss:  0.033174075186252594\n",
            "train loss:  0.01722930185496807\n",
            "train loss:  0.002416834235191345\n",
            "train loss:  0.0011452905600890517\n",
            "train loss:  0.00026805358356796205\n",
            "train loss:  0.06189602240920067\n",
            "train loss:  0.0012413258664309978\n",
            "train loss:  0.03672105446457863\n",
            "train loss:  0.006354909855872393\n",
            "train loss:  0.0017130875494331121\n",
            "train loss:  0.08564922958612442\n",
            "train loss:  0.04057348892092705\n",
            "train loss:  0.04629399627447128\n",
            "train loss:  0.0004437638563103974\n",
            "train loss:  0.0026585827581584454\n",
            "train loss:  0.0009723204420879483\n",
            "train loss:  0.004467535298317671\n",
            "train loss:  0.06685815006494522\n",
            "train loss:  0.07988471537828445\n",
            "train loss:  0.004740614909678698\n",
            "train loss:  0.0005774776800535619\n",
            "train loss:  0.030268650501966476\n",
            "train loss:  0.09194114059209824\n",
            "train loss:  0.07087698578834534\n",
            "train loss:  0.0003090875979978591\n",
            "train loss:  0.05101645737886429\n",
            "train loss:  0.039858378469944\n",
            "train loss:  0.0035505194682627916\n",
            "train loss:  0.008636930026113987\n",
            "train loss:  0.00349686061963439\n",
            "train loss:  0.0007744620670564473\n",
            "train loss:  0.049030985683202744\n",
            "train loss:  0.0029738708399236202\n",
            "train loss:  0.0029365827795118093\n",
            "train loss:  0.001846445258706808\n",
            "train loss:  0.0006929493974894285\n",
            "train loss:  0.003723361063748598\n",
            "train loss:  0.00011916086077690125\n",
            "train loss:  0.03436533734202385\n",
            "train loss:  0.0009567260276526213\n",
            "train loss:  0.07143603265285492\n",
            "train loss:  0.026685908436775208\n",
            "train loss:  0.014570388942956924\n",
            "train loss:  0.0007862642523832619\n",
            "train loss:  0.0009750261669978499\n",
            "train loss:  0.0003312261251267046\n",
            "train loss:  0.0008429010631516576\n",
            "train loss:  0.06601815670728683\n",
            "train loss:  0.020419597625732422\n",
            "train loss:  0.004704671911895275\n",
            "train loss:  0.0009819914121180773\n",
            "train loss:  0.008895061910152435\n",
            "train loss:  0.0009852914372459054\n",
            "train loss:  0.014925051480531693\n",
            "train loss:  0.0006605659145861864\n",
            "train loss:  0.0009486545459367335\n",
            "train loss:  0.0019746078178286552\n",
            "train loss:  0.010670065879821777\n",
            "train loss:  0.0011487967567518353\n",
            "train loss:  0.003399076173081994\n",
            "train loss:  0.00852971337735653\n",
            "train loss:  0.0006318138912320137\n",
            "train loss:  0.028408488258719444\n",
            "train loss:  0.012081420049071312\n",
            "train loss:  0.007825647480785847\n",
            "train loss:  0.009348142892122269\n",
            "train loss:  0.06526683270931244\n",
            "train loss:  0.09346204996109009\n",
            "train loss:  0.005009815096855164\n",
            "train loss:  0.01833142712712288\n",
            "train loss:  0.0009249186259694397\n",
            "train loss:  0.00043052283581346273\n",
            "train loss:  0.1293950080871582\n",
            "train loss:  0.0004460331401787698\n",
            "train loss:  0.0005729548865929246\n",
            "train loss:  0.0006862037698738277\n",
            "train loss:  0.10364392399787903\n",
            "train loss:  0.1144481971859932\n",
            "train loss:  0.010030170902609825\n",
            "train loss:  0.0005273893475532532\n",
            "train loss:  0.005141464062035084\n",
            "train loss:  0.08522260189056396\n",
            "train loss:  0.06013348698616028\n",
            "train loss:  0.008165320381522179\n",
            "train loss:  0.0043922606855630875\n",
            "train loss:  0.023261938244104385\n",
            "train loss:  0.056954000145196915\n",
            "train loss:  0.013146315701305866\n",
            "train loss:  0.0031494004651904106\n",
            "train loss:  0.028466427698731422\n",
            "train loss:  0.004555051680654287\n",
            "train loss:  0.0011474678758531809\n",
            "train loss:  0.004378234501928091\n",
            "train loss:  0.0003903619071934372\n",
            "train loss:  0.002875403966754675\n",
            "train loss:  0.0009682578383944929\n",
            "train loss:  0.0003514010168146342\n",
            "train loss:  0.01482885517179966\n",
            "train loss:  0.007819431833922863\n",
            "train loss:  0.0010944842360913754\n",
            "train loss:  0.010161145590245724\n",
            "train loss:  0.03692338615655899\n",
            "train loss:  0.031193086877465248\n",
            "train loss:  0.052240148186683655\n",
            "train loss:  0.0036126580089330673\n",
            "train loss:  0.00018568008090369403\n",
            "train loss:  0.029605381190776825\n",
            "train loss:  0.0013709175400435925\n",
            "train loss:  0.01958821341395378\n",
            "train loss:  0.002210641512647271\n",
            "train loss:  0.00880846381187439\n",
            "train loss:  0.03338935226202011\n",
            "train loss:  0.08066555857658386\n",
            "train loss:  0.009399616159498692\n",
            "train loss:  0.00013451764243654907\n",
            "train loss:  0.02911190874874592\n",
            "train loss:  0.0005146245239302516\n",
            "train loss:  0.0032750049140304327\n",
            "train loss:  0.002351283561438322\n",
            "train loss:  0.015339101664721966\n",
            "train loss:  0.04234273359179497\n",
            "train loss:  0.0011599832214415073\n",
            "train loss:  0.012936820276081562\n",
            "train loss:  0.012446276843547821\n",
            "train loss:  0.0011434524785727262\n",
            "train loss:  3.8066544220782816e-05\n",
            "train loss:  0.030156945809721947\n",
            "train loss:  0.0024549318477511406\n",
            "train loss:  0.0017058474477380514\n",
            "train loss:  0.0004219890688546002\n",
            "train loss:  0.00016173069889191538\n",
            "train loss:  0.051917821168899536\n",
            "train loss:  0.032936736941337585\n",
            "train loss:  0.0019250919576734304\n",
            "train loss:  0.02116604521870613\n",
            "train loss:  0.004446233622729778\n",
            "train loss:  0.052141766995191574\n",
            "train loss:  0.0021732919849455357\n",
            "train loss:  0.001112542930059135\n",
            "train loss:  0.006903975270688534\n",
            "train loss:  0.16756144165992737\n",
            "train loss:  4.091993469046429e-05\n",
            "train loss:  0.038279611617326736\n",
            "train loss:  0.01533531118184328\n",
            "train loss:  0.00756246130913496\n",
            "train loss:  0.00010091725562233478\n",
            "train loss:  0.004583209287375212\n",
            "train loss:  0.05235007405281067\n",
            "train loss:  0.05031554773449898\n",
            "train loss:  0.027490505948662758\n",
            "train loss:  0.05439788103103638\n",
            "train loss:  0.0004965608823113143\n",
            "train loss:  0.000853078265208751\n",
            "train loss:  0.00046458866563625634\n",
            "train loss:  0.008128284476697445\n",
            "train loss:  0.0005349555285647511\n",
            "train loss:  0.01782269962131977\n",
            "train loss:  0.0002819627698045224\n",
            "train loss:  0.0016460454789921641\n",
            "train loss:  0.07938029617071152\n",
            "train loss:  0.0036822850815951824\n",
            "train loss:  0.007817313075065613\n",
            "train loss:  0.002274209400638938\n",
            "train loss:  0.0026149307377636433\n",
            "train loss:  0.0016649829922243953\n",
            "train loss:  0.000512859842274338\n",
            "train loss:  0.11648907512426376\n",
            "train loss:  0.03309540078043938\n",
            "train loss:  0.10562808811664581\n",
            "train loss:  0.021616650745272636\n",
            "train loss:  0.02073783241212368\n",
            "train loss:  0.03140104189515114\n",
            "train loss:  0.01446276530623436\n",
            "train loss:  0.011731070466339588\n",
            "train loss:  0.013041986152529716\n",
            "train loss:  0.001222795806825161\n",
            "train loss:  0.002559219254180789\n",
            "train loss:  0.011300570331513882\n",
            "train loss:  0.00815644022077322\n",
            "train loss:  0.01633032225072384\n",
            "train loss:  0.020892972126603127\n",
            "train loss:  0.005867682863026857\n",
            "train loss:  0.0008521085255779326\n",
            "train loss:  0.03059609793126583\n",
            "train loss:  0.016287975013256073\n",
            "train loss:  0.0014533482026308775\n",
            "train loss:  0.0033249380066990852\n",
            "train loss:  0.0422809012234211\n",
            "train loss:  0.003114270744845271\n",
            "train loss:  0.01627320609986782\n",
            "train loss:  0.03274962306022644\n",
            "train loss:  0.010884875431656837\n",
            "train loss:  0.007295423187315464\n",
            "train loss:  0.001775188371539116\n",
            "train loss:  0.07091806083917618\n",
            "train loss:  0.002428770763799548\n",
            "train loss:  5.962249997537583e-05\n",
            "train loss:  0.001004233956336975\n",
            "train loss:  0.000558829284273088\n",
            "train loss:  0.01315352413803339\n",
            "train loss:  0.017455684021115303\n",
            "train loss:  0.01932526007294655\n",
            "train loss:  0.005549537483602762\n",
            "train loss:  0.00942480843514204\n",
            "train loss:  0.016991734504699707\n",
            "train loss:  0.10416636615991592\n",
            "train loss:  0.0031795448157936335\n",
            "train loss:  0.02222568728029728\n",
            "train loss:  0.0026787177193909883\n",
            "train loss:  0.09177406132221222\n",
            "train loss:  0.016564801335334778\n",
            "train loss:  0.005906847305595875\n",
            "train loss:  0.0005925183068029583\n",
            "train loss:  0.00732103455811739\n",
            "train loss:  0.04068572446703911\n",
            "train loss:  0.004616440739482641\n",
            "train loss:  0.003351264400407672\n",
            "train loss:  0.05363382399082184\n",
            "train loss:  0.012728847563266754\n",
            "train loss:  0.005031008739024401\n",
            "train loss:  0.0056301504373550415\n",
            "train loss:  0.018459834158420563\n",
            "train loss:  0.014444522559642792\n",
            "train loss:  0.011294753290712833\n",
            "train loss:  0.008956038393080235\n",
            "train loss:  0.01839040406048298\n",
            "train loss:  0.2563023269176483\n",
            "train loss:  0.0004048708360642195\n",
            "train loss:  0.09980469942092896\n",
            "train loss:  0.000469670572783798\n",
            "train loss:  0.008021300658583641\n",
            "train loss:  0.0035520633682608604\n",
            "train loss:  0.0054259998723864555\n",
            "train loss:  0.006251470651477575\n",
            "train loss:  0.016773847863078117\n",
            "train loss:  0.00043211199226789176\n",
            "train loss:  0.1250792294740677\n",
            "train loss:  0.009029173292219639\n",
            "train loss:  0.00082349160220474\n",
            "train loss:  0.010428967885673046\n",
            "train loss:  0.00483083538711071\n",
            "train loss:  0.0005090165650472045\n",
            "train loss:  0.000726439175195992\n",
            "train loss:  0.0001297661365242675\n",
            "train loss:  0.000488682184368372\n",
            "val_loss:  0.09551672637462616\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}